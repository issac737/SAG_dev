"""
äº‹é¡¹æå–å™¨

ä¸»æ§åˆ¶å™¨ - åè°ƒå•ç¯‡æ–‡æ¡£çš„æå–æµç¨‹
"""

import asyncio
import uuid
from typing import Any, Dict, List, Optional

from sqlalchemy import select
from sqlalchemy.exc import IntegrityError, OperationalError
from sqlalchemy.orm import selectinload

from dataflow.core.ai.factory import get_embedding_client
from dataflow.core.ai.base import BaseLLMClient
from dataflow.core.prompt.manager import PromptManager
from dataflow.core.storage.elasticsearch import get_es_client
from dataflow.core.storage.repositories.entity_repository import EntityVectorRepository
from dataflow.core.storage.repositories.event_repository import EventVectorRepository
from dataflow.db import (
    SourceChunk,
    Entity,
    EntityType as DBEntityType,
    EventEntity,
    SourceEvent,
    get_session_factory,
)
from dataflow.exceptions import ExtractError
from dataflow.modules.extract.config import ExtractConfig
from dataflow.modules.extract.parser import EntityValueParser
from dataflow.modules.extract.processor import EventProcessor
from dataflow.utils import estimate_tokens, get_logger

logger = get_logger("extract.extractor")


# ä¸éœ€è¦ç´¢å¼•åˆ° ES çš„å®ä½“ç±»å‹åˆ—è¡¨
# åŸå› ï¼šæ—¶é—´ç±»å‹çš„å±æ€§é€šå¸¸å…·æœ‰ä¸€è‡´æ€§ï¼Œä¸éœ€è¦ç”¨äºç›¸ä¼¼åº¦è®¡ç®—å’Œå…³è”æ£€ç´¢
# å¦‚ start_timeã€end_timeã€created_time ç­‰æ—¶é—´å­—æ®µä¼šå½±å“ç›¸ä¼¼åº¦åŒ¹é…ï¼Œä½†å®é™…ä¸åº”è¯¥ä½œä¸ºå…³è”ä¾æ®
ENTITY_TYPES_SKIP_ES_INDEX = [
    "time",
    "start_time",
    "end_time",
    "created_time",
    "updated_time",
    "tags",  # å…³é”®è¯æ ‡ç­¾ä¸éœ€è¦å‘é‡ç´¢å¼•
]


def should_index_entity_to_es(entity_type: str) -> bool:
    """
    åˆ¤æ–­å®ä½“ç±»å‹æ˜¯å¦åº”è¯¥ç´¢å¼•åˆ° ES

    Args:
        entity_type: å®ä½“ç±»å‹æ ‡è¯†ç¬¦

    Returns:
        True è¡¨ç¤ºåº”è¯¥ç´¢å¼•ï¼ŒFalse è¡¨ç¤ºè·³è¿‡
    """
    return entity_type not in ENTITY_TYPES_SKIP_ES_INDEX


class EventExtractor:
    """äº‹é¡¹æå–å™¨ï¼ˆä¸»æ§åˆ¶å™¨ï¼‰"""

    def __init__(
        self,
        prompt_manager: PromptManager,
        model_config: Optional[Dict] = None,
    ):
        """
        åˆå§‹åŒ–äº‹é¡¹æå–å™¨

        Args:
            prompt_manager: æç¤ºè¯ç®¡ç†å™¨
            model_config: LLMé…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
                - å¦‚æœä¼ å…¥ï¼šä½¿ç”¨è¯¥é…ç½®
                - å¦‚æœä¸ä¼ ï¼šè‡ªåŠ¨ä»é…ç½®ç®¡ç†å™¨è·å– 'extract' åœºæ™¯é…ç½®
        """
        self.prompt_manager = prompt_manager
        self.model_config = model_config
        self._llm_client = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.session_factory = get_session_factory()
        self.logger = get_logger("extract.extractor")
        
        # ESç›¸å…³ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self.es_client = None
        self.event_repo = None
        self.entity_repo = None
    
    async def _get_llm_client(self) -> BaseLLMClient:
        """è·å–LLMå®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._llm_client is None:
            from dataflow.core.ai.factory import create_llm_client
            
            self._llm_client = await create_llm_client(
                scenario='extract',
                model_config=self.model_config
            )
        
        return self._llm_client
    

    async def extract(self, config: ExtractConfig) -> List[SourceEvent]:
        """
        æå–äº‹é¡¹ï¼ˆç»Ÿä¸€å…¥å£ - æ–°æ¶æ„ï¼‰
        
        å·¥ä½œæµç¨‹ï¼š
        1. åŠ è½½æ‰€æœ‰chunks
        2. æŒ‰max_concurrencyå¹¶å‘å¤„ç†ï¼ˆSemaphoreæ§åˆ¶ï¼‰
        3. æ¯ä¸ªchunkç”±ä¸€ä¸ªExtractorAgentå¤„ç†
        4. åˆå¹¶æ‰€æœ‰ç»“æœ
        5. ä¿å­˜åˆ°æ•°æ®åº“ + Elasticsearch
        6. æ›´æ–°æºçŠ¶æ€ä¸ºå·²å®Œæˆ

        Args:
            config: æå–é…ç½®

        Returns:
            æ‰€æœ‰chunksæå–çš„äº‹é¡¹åˆ—è¡¨

        Example:
            config = ExtractConfig(
                source_config_id="source-uuid",
                chunk_ids=["chunk-1", "chunk-2", "chunk-3"],
                max_concurrency=3
            )
            events = await extractor.extract(config)
        """
        self.logger.info(
            f"å¼€å§‹æ‰¹é‡æå–: chunks={len(config.chunk_ids)}, "
            f"å¹¶å‘æ•°={config.max_concurrency}"
        )

        try:
            # 1. åŠ è½½æ‰€æœ‰chunks
            chunks = await self._load_chunks(config.chunk_ids)

            if not chunks:
                self.logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„chunks")
                return []

            # 2. å¹¶å‘å¤„ç†chunksï¼ˆæ¯ä¸ªchunkä¸€ä¸ªAgentï¼‰
            all_events = await self._process_chunks_with_agents(chunks, config)
            
            self.logger.info(
                f"æ‰¹é‡æå–å®Œæˆ: chunks={len(chunks)}, events={len(all_events)}"
            )
            
            # 3. æŒ‰åŸæ–‡é¡ºåºé‡æ–°æ’åºå¹¶åˆ†é…å…¨å±€ rank
            if all_events:
                # åˆ›å»º chunk_id -> chunk.rank çš„æ˜ å°„
                chunk_rank_map = {chunk.id: chunk.rank for chunk in chunks}
                
                # æ’åºè§„åˆ™ï¼š
                # 1. å…ˆæŒ‰ chunk.rankï¼ˆä¿è¯ chunk ä¹‹é—´çš„é¡ºåºï¼‰
                # 2. å†æŒ‰äº‹é¡¹çš„æ—¶é—´ï¼ˆä¼šè¯ï¼‰æˆ– chunk å†… rankï¼ˆæ–‡æ¡£ï¼‰
                def sort_key(event):
                    chunk_order = chunk_rank_map.get(event.chunk_id, 9999)
                    
                    # ä¼šè¯ç±»å‹ï¼šæŒ‰æ—¶é—´æ’åº
                    if event.source_type == "CHAT" and event.start_time:
                        event_order = event.start_time
                    # æ–‡æ¡£ç±»å‹ï¼šæŒ‰ chunk å†… rank æ’åº
                    else:
                        event_order = event.rank or 0
                    
                    return (chunk_order, event_order)
                
                all_events.sort(key=sort_key)
                
                # é‡æ–°åˆ†é…å…¨å±€è¿ç»­ rank
                for i, event in enumerate(all_events):
                    event.rank = i
                
                self.logger.info(
                    f"äº‹é¡¹å·²æŒ‰åŸæ–‡é¡ºåºæ’åº: chunks={len(chunks)}, events={len(all_events)}"
                )
            
            # 4. ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆåŒ…æ‹¬ESï¼‰
            if all_events:
                await self._save_events(all_events, config)
                
                # 5. é‡æ–°ä»æ•°æ®åº“åŠ è½½äº‹é¡¹ï¼ˆå¸¦å®Œæ•´å…³ç³»æ•°æ®ï¼‰
                # è§£å†³è·¨ session é—®é¢˜ï¼šä¿å­˜åé‡æ–°æŸ¥è¯¢ï¼Œç¡®ä¿æ‰€æœ‰å…³ç³»æ­£ç¡®åŠ è½½
                event_ids = [e.id for e in all_events]
                all_events = await self._reload_events_with_relations(event_ids)
            else:
                self.logger.warning("æ²¡æœ‰æå–åˆ°ä»»ä½•äº‹é¡¹ï¼Œè·³è¿‡ä¿å­˜")
            
            # 6. æ›´æ–°æºçŠ¶æ€ä¸ºå·²å®Œæˆ
            if chunks:
                await self._update_source_status(chunks, status="COMPLETED")
            
            return all_events
            
        except Exception as e:
            # æå–å¤±è´¥æ—¶ï¼Œæ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            self.logger.error(f"æå–å¤±è´¥: {e}", exc_info=True)
            try:
                chunks = await self._load_chunks(config.chunk_ids)
                if chunks:
                    await self._update_source_status(chunks, status="FAILED", error=str(e))
            except Exception as update_error:
                self.logger.error(f"æ›´æ–°å¤±è´¥çŠ¶æ€æ—¶å‡ºé”™: {update_error}")
            
            raise ExtractError(f"æå–å¤±è´¥: {e}") from e
    
    async def _load_chunks(self, chunk_ids: List[str]) -> List[SourceChunk]:
        """æ‰¹é‡åŠ è½½chunksï¼ˆæŒ‰rankæ’åºï¼‰"""
        async with self.session_factory() as session:
            result = await session.execute(
                select(SourceChunk)
                .where(SourceChunk.id.in_(chunk_ids))
                .order_by(SourceChunk.rank)  # ğŸ†• æŒ‰ rank æ’åº
            )
            chunks = list(result.scalars().all())
            
            if len(chunks) != len(chunk_ids):
                missing = set(chunk_ids) - {c.id for c in chunks}
                self.logger.warning(f"éƒ¨åˆ†chunkä¸å­˜åœ¨: {missing}")
            
            return chunks
    
    async def _process_chunks_with_agents(
        self,
        chunks: List[SourceChunk],
        config: ExtractConfig
    ) -> List[SourceEvent]:
        """
        å¹¶å‘å¤„ç†chunksï¼ˆæ¯ä¸ªchunkä¸€ä¸ªAgentï¼‰
        
        ä½¿ç”¨asyncio.Semaphoreæ§åˆ¶å¹¶å‘æ•°é‡ï¼š
        - åŒæ—¶æœ€å¤šæœ‰max_concurrencyä¸ªAgentåœ¨è¿è¡Œ
        - è¶…å‡ºçš„chunkä¼šè‡ªåŠ¨æ’é˜Ÿç­‰å¾…
        - ä¸€ä¸ªchunkå®Œæˆåï¼Œç«‹å³å¯åŠ¨ä¸‹ä¸€ä¸ª
        
        Args:
            chunks: chunkåˆ—è¡¨
            config: æå–é…ç½®
        
        Returns:
            åˆå¹¶åçš„æ‰€æœ‰äº‹é¡¹
        """
        semaphore = asyncio.Semaphore(config.max_concurrency)
        
        # è¿›åº¦è·Ÿè¸ª
        completed = 0
        success_count = 0
        failed_count = 0
        total = len(chunks)
        lock = asyncio.Lock()
        
        async def process_single_chunk(
            chunk: SourceChunk,
            index: int
        ) -> List[SourceEvent]:
            """å¤„ç†å•ä¸ªchunkï¼ˆå¸¦å¹¶å‘æ§åˆ¶å’Œè¿›åº¦ç»Ÿè®¡ï¼‰"""
            nonlocal completed, success_count, failed_count
            
            async with semaphore:  # ğŸ”’ è·å–å¹¶å‘æ§½ä½ï¼ˆæ²¡æœ‰å°±ç­‰å¾…ï¼‰
                try:
                    self.logger.info(
                        f"[{index+1}/{total}] å¼€å§‹å¤„ç†: chunk_id={chunk.id}, "
                        f"type={chunk.source_type}"
                    )
                    
                    # è°ƒç”¨chunkçº§æå–ï¼ˆä½¿ç”¨ExtractorAgentï¼‰
                    events = await self.extract_from_chunk(chunk, config)
                    
                    # æ›´æ–°è¿›åº¦
                    async with lock:
                        completed += 1
                        success_count += 1
                        progress = completed * 100 // total
                        
                    self.logger.info(
                        f"âœ… [{index+1}/{total}] å®Œæˆ ({progress}%): "
                        f"chunk_id={chunk.id}, events={len(events)}"
                    )
                    
                    return events
                    
                except Exception as e:
                    # æ›´æ–°å¤±è´¥ç»Ÿè®¡
                    async with lock:
                        completed += 1
                        failed_count += 1
                        progress = completed * 100 // total
                        
                    self.logger.error(
                        f"âŒ [{index+1}/{total}] å¤±è´¥ ({progress}%): "
                        f"chunk_id={chunk.id}, error={e}",
                        exc_info=True
                    )
                    return []  # å¤±è´¥è¿”å›ç©ºï¼Œä¸ä¸­æ–­å…¶ä»–chunk
                # ğŸ”“ ç¦»å¼€æ—¶è‡ªåŠ¨é‡Šæ”¾æ§½ä½
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰chunk
        self.logger.info(
            f"ğŸš€ å¯åŠ¨å¹¶å‘æå–: total={total}, concurrency={config.max_concurrency}"
        )
        
        tasks = [
            process_single_chunk(chunk, i)
            for i, chunk in enumerate(chunks)
        ]
        results = await asyncio.gather(*tasks, return_exceptions=False)
        
        # åˆå¹¶ç»“æœ
        all_events = []
        for events in results:
            all_events.extend(events)
        
        # æœ€ç»ˆç»Ÿè®¡
        self.logger.info(
            f"ğŸ“Š æ‰¹é‡æå–ç»Ÿè®¡: æ€»æ•°={total}, æˆåŠŸ={success_count}, "
            f"å¤±è´¥={failed_count}, äº‹é¡¹={len(all_events)}"
        )
        
        return all_events

    async def _load_sections(self, config: ExtractConfig) -> List[SourceChunk]:
        """
        åŠ è½½æ¥æºç‰‡æ®µ

        Args:
            config: æå–é…ç½®

        Returns:
            æ¥æºç‰‡æ®µåˆ—è¡¨
        """
        self.logger.debug(f"åŠ è½½æ–‡ç« ç‰‡æ®µï¼šarticle_id={config.article_id}")

        async with self.session_factory() as session:
            result = await session.execute(
                select(SourceChunk)
                .where(SourceChunk.source_type == 'article')
                .where(SourceChunk.source_id == config.article_id)
                .order_by(SourceChunk.rank)
            )
            sections = result.scalars().all()

        self.logger.info(f"åŠ è½½äº† {len(sections)} ä¸ªæ¥æºç‰‡æ®µ")

        return list(sections)

    async def _ensure_entity_types(self, config: ExtractConfig) -> None:
        """
        ç¡®ä¿å®ä½“ç±»å‹å·²åˆå§‹åŒ–åˆ°æ•°æ®åº“

        Args:
            config: æå–é…ç½®
        """
        if not config.custom_entity_types:
            return

        self.logger.debug(f"åˆå§‹åŒ–è‡ªå®šä¹‰å®ä½“ç±»å‹ï¼š{len(config.custom_entity_types)} ä¸ª")

        async with self.session_factory() as session:
            for custom_type in config.custom_entity_types:
                result = await session.execute(
                    select(DBEntityType)
                    .where(DBEntityType.source_config_id == config.source_config_id)
                    .where(DBEntityType.type == custom_type.type)
                )
                existing = result.scalar_one_or_none()

                if not existing:
                    entity_type = DBEntityType(
                        id=str(uuid.uuid4()),
                        source_config_id=config.source_config_id,
                        type=custom_type.type,
                        name=custom_type.name,
                        description=custom_type.description,
                        weight=custom_type.weight,
                        is_active=True,
                        is_default=False,
                    )
                    session.add(entity_type)
                    self.logger.debug(f"åˆ›å»ºè‡ªå®šä¹‰å®ä½“ç±»å‹ï¼š{custom_type.type}")

            await session.commit()

    def _create_batches(
        self, sections: List[SourceChunk], config: ExtractConfig
    ) -> List[List[SourceChunk]]:
        """
        åˆ›å»ºæ‰¹æ¬¡ï¼ˆæ™ºèƒ½åˆ†æ‰¹ï¼ŒåŸºäºtokené™åˆ¶ï¼‰

        Args:
            sections: æ¥æºç‰‡æ®µåˆ—è¡¨
            config: æå–é…ç½®

        Returns:
            æ‰¹æ¬¡åˆ—è¡¨ï¼Œæ¯ä¸ªæ‰¹æ¬¡æ˜¯ç‰‡æ®µåˆ—è¡¨
        """
        batches = []
        current_batch = []
        current_tokens = 0

        for section in sections:
            section_text = f"{section.heading}\n{section.content}"
            section_tokens = estimate_tokens(section_text)

            will_exceed_token_limit = (
                current_tokens + section_tokens > config.max_tokens
            )
            will_exceed_section_limit = len(current_batch) >= config.max_sections

            if current_batch and (will_exceed_token_limit or will_exceed_section_limit):
                batches.append(current_batch)
                current_batch = []
                current_tokens = 0

            current_batch.append(section)
            current_tokens += section_tokens

        if current_batch:
            batches.append(current_batch)

        self.logger.info(
            f"åˆ›å»ºäº† {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œ"
            f"å¹³å‡æ¯æ‰¹ {len(sections) / len(batches):.1f} ä¸ªç‰‡æ®µ"
        )

        return batches

    async def _process_batches_parallel(
        self,
        sections: List[SourceChunk],
        processor: EventProcessor,
        config: ExtractConfig,
    ) -> List[SourceEvent]:
        """
        å¹¶å‘å¤„ç†æ‰€æœ‰æ‰¹æ¬¡ï¼ˆä¸¤é˜¶æ®µå¤„ç†ï¼‰

        Args:
            sections: æ¥æºç‰‡æ®µåˆ—è¡¨
            processor: äº‹é¡¹å¤„ç†å™¨
            config: æå–é…ç½®

        Returns:
            æ‰€æœ‰æ‰¹æ¬¡æå–çš„äº‹é¡¹åˆ—è¡¨ï¼ˆåŒ…å«å®ä½“å…³è”ï¼‰
        """
        batches = self._create_batches(sections, config)

        if not batches:
            self.logger.warning("æ²¡æœ‰å¯å¤„ç†çš„æ‰¹æ¬¡")
            return []

        semaphore = asyncio.Semaphore(config.max_concurrency)
        
        # è¿›åº¦ç»Ÿè®¡
        completed_count = 0
        success_count = 0
        failed_count = 0
        progress_lock = asyncio.Lock()

        async def process_single_batch_extraction(
            batch_index: int, batch: List[SourceChunk]
        ) -> List[SourceEvent]:
            nonlocal completed_count, success_count, failed_count
            
            async with semaphore:
                try:
                    self.logger.debug(
                        f"æ‰¹æ¬¡ {batch_index + 1}/{len(batches)}: "
                        f"å¼€å§‹å¤„ç†ï¼ˆé˜¶æ®µ1ï¼‰ï¼Œç‰‡æ®µæ•°={len(batch)}"
                    )
                    # é˜¶æ®µ1ï¼šæå–äº‹é¡¹ï¼ˆä¸å«å®ä½“å…³è”ï¼‰
                    events = await processor.extract_events_without_entities(batch, batch_index)
                    
                    # æ›´æ–°è¿›åº¦ç»Ÿè®¡
                    async with progress_lock:
                        completed_count += 1
                        success_count += 1
                        progress_pct = completed_count * 100 // len(batches)
                        success_rate = success_count * 100 // completed_count
                        
                    self.logger.info(
                            f"ğŸ“Š è¿›åº¦: {completed_count}/{len(batches)} ({progress_pct}%) | "
                            f"æˆåŠŸç‡: {success_rate}% ({success_count}âœ“/{failed_count}âœ—) | "
                            f"æ‰¹æ¬¡ {batch_index + 1}: æå–äº† {len(events)} ä¸ªäº‹é¡¹"
                    )
                    
                    return events
                except Exception as e:
                    # æ›´æ–°å¤±è´¥ç»Ÿè®¡
                    async with progress_lock:
                        completed_count += 1
                        failed_count += 1
                        progress_pct = completed_count * 100 // len(batches)
                        success_rate = success_count * 100 // completed_count if completed_count > 0 else 0
                        
                        self.logger.error(
                            f"âŒ è¿›åº¦: {completed_count}/{len(batches)} ({progress_pct}%) | "
                            f"æˆåŠŸç‡: {success_rate}% ({success_count}âœ“/{failed_count}âœ—) | "
                            f"æ‰¹æ¬¡ {batch_index + 1}: å¤„ç†å¤±è´¥: {e}"
                        )
                    
                    self.logger.error(
                        f"æ‰¹æ¬¡ {batch_index + 1} è¯¦ç»†é”™è¯¯",
                        exc_info=True,
                        extra={
                            "batch_index": batch_index,
                            "batch_size": len(batch),
                            "error_type": type(e).__name__,
                        },
                    )
                    # è¿”å›ç©ºåˆ—è¡¨ï¼Œä½†é”™è¯¯å·²è¢«è®°å½•
                    return []

        # é˜¶æ®µ1ï¼šå¹¶å‘æå–æ‰€æœ‰æ‰¹æ¬¡çš„äº‹é¡¹ï¼ˆä¸å«å®ä½“ï¼‰
        self.logger.info(
            f"ğŸš€ å¼€å§‹é˜¶æ®µ1ï¼šå¹¶å‘å¤„ç† {len(batches)} ä¸ªæ‰¹æ¬¡ï¼ˆäº‹é¡¹æå–ï¼‰ - "
            f"æœ€å¤§å¹¶å‘æ•°={config.max_concurrency}, LLMæ¨¡å‹={processor.llm_client.client.config.model}"
        )

        tasks = [process_single_batch_extraction(i, batch) for i, batch in enumerate(batches)]
        batch_results = await asyncio.gather(*tasks, return_exceptions=False)

        all_events = []
        for batch_events in batch_results:
            all_events.extend(batch_events)

        # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
        final_success_rate = success_count * 100 // len(batches) if len(batches) > 0 else 0

        self.logger.info(
            f"âœ… é˜¶æ®µ1å®Œæˆ | "
            f"æ€»æ‰¹æ¬¡: {len(batches)} | "
            f"æˆåŠŸ: {success_count}âœ“ | "
            f"å¤±è´¥: {failed_count}âœ— | "
            f"æˆåŠŸç‡: {final_success_rate}% | "
            f"æå–äº‹é¡¹: {len(all_events)} ä¸ª",
            extra={
                "total_batches": len(batches),
                "success_count": success_count,
                "failed_count": failed_count,
                "success_rate": final_success_rate,
                "total_sections": len(sections),
                "total_events": len(all_events),
            },
        )

        # é˜¶æ®µ1.5ï¼šå®ä½“äº¤å‰è¡¥å……ï¼ˆå…œåº•ç­–ç•¥ï¼‰
        if all_events and len(all_events) > 1:
            self.logger.info("å¼€å§‹é˜¶æ®µ1.5ï¼šå®ä½“äº¤å‰è¡¥å……")
            all_events = processor._cross_fill_entities(all_events)
        
        # é˜¶æ®µ2ï¼šç»Ÿä¸€å¤„ç†æ‰€æœ‰äº‹é¡¹çš„å®ä½“å…³è”
        if all_events:
            self.logger.info("å¼€å§‹é˜¶æ®µ2ï¼šç»Ÿä¸€å¤„ç†å®ä½“å…³è”")
            all_events = await processor.process_entity_associations(all_events)
            self.logger.info(f"é˜¶æ®µ2å®Œæˆï¼Œå·²å¤„ç† {len(all_events)} ä¸ªäº‹é¡¹çš„å®ä½“å…³è”")

        return all_events

    async def _extract_events(
        self, sections: List[SourceChunk], config: ExtractConfig
    ) -> List[SourceEvent]:
        """
        æå–äº‹é¡¹

        Args:
            sections: æ¥æºç‰‡æ®µåˆ—è¡¨
            config: æå–é…ç½®

        Returns:
            æå–çš„äº‹é¡¹åˆ—è¡¨
        """
        # è·å–LLMå®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰
        llm_client = await self._get_llm_client()
        
        processor = EventProcessor(
            llm_client=llm_client,
            prompt_manager=self.prompt_manager,
            config=config,
        )

        # åˆå§‹åŒ–å¤„ç†å™¨ï¼ˆåŠ è½½å®ä½“ç±»å‹é…ç½®ï¼‰
        await processor.initialize()

        if config.parallel:
            # å¹¶è¡Œå¤„ç†ç‰‡æ®µï¼ˆæ™ºèƒ½åˆ†æ‰¹ï¼‰- å†…éƒ¨å·²åŒ…å«å®ä½“å…³è”å¤„ç†
            events = await self._process_batches_parallel(sections, processor, config)
        else:
            # é¡ºåºå¤„ç†ç‰‡æ®µï¼ˆä¸å«å®ä½“ï¼‰
            events = []
            for i, section in enumerate(sections):
                batch_events = await processor.extract_from_sections([section], i)
                events.extend(batch_events)
            
            # ç»Ÿä¸€å¤„ç†å®ä½“å…³è”ï¼ˆä¸å¹¶è¡Œæ¨¡å¼ä¿æŒä¸€è‡´ï¼‰
            if events:
                # å®ä½“äº¤å‰è¡¥å……
                if len(events) > 1:
                    events = processor._cross_fill_entities(events)
                # å¤„ç†å®ä½“å…³è”
                events = await processor.process_entity_associations(events)

        # ç»Ÿä¸€åˆ†é…å…¨å±€ rankï¼ˆç¡®ä¿åŒä¸€æ–‡ç« å†…çš„äº‹é¡¹æŒ‰é¡ºåºæ’åˆ—ï¼‰
        for i, event in enumerate(events):
            event.rank = i

        self.logger.info(f"æå–äº† {len(events)} ä¸ªäº‹é¡¹")

        return events

    async def _save_events(self, events: List[SourceEvent], config: ExtractConfig) -> None:
        """
        ä¿å­˜äº‹é¡¹åˆ°æ•°æ®åº“ï¼ˆåŒ…æ‹¬å®ä½“å…³è”ï¼‰+ Elasticsearch

        Args:
            events: äº‹é¡¹åˆ—è¡¨
            config: æå–é…ç½®
        """
        self.logger.info(f"ä¿å­˜ {len(events)} ä¸ªäº‹é¡¹åˆ°æ•°æ®åº“")

        # 1. ä¿å­˜åˆ° MySQL
        event_ids = []
        async with self.session_factory() as session:
            for event in events:
                event_ids.append(event.id)  # è®°å½• event ID
                # æ·»åŠ äº‹é¡¹
                session.add(event)
                
                # æ·»åŠ äº‹é¡¹çš„æ‰€æœ‰å®ä½“å…³è”
                if hasattr(event, 'event_associations') and event.event_associations:
                    for assoc in event.event_associations:
                        session.add(assoc)

            await session.commit()

        self.logger.info("äº‹é¡¹ä¿å­˜åˆ°MySQLå®Œæˆ")

        # 2. åŒæ­¥åˆ° Elasticsearchï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.auto_vector:
            try:
                # é‡æ–°ä»æ•°æ®åº“æŸ¥è¯¢äº‹é¡¹ï¼ˆå¸¦å…³ç³»æ•°æ®ï¼‰ï¼Œé¿å… detached instance é—®é¢˜
                fresh_events = await self._load_events_by_ids(event_ids)
                await self._sync_to_elasticsearch(fresh_events, config)
            except Exception as e:
                self.logger.error(f"åŒæ­¥åˆ°ESå¤±è´¥: {e}", exc_info=True)
                # ä¸ä¸­æ–­æµç¨‹ï¼Œç»§ç»­æ‰§è¡Œ

    async def _load_events_by_ids(self, event_ids: List[str]) -> List[SourceEvent]:
        """
        ä»æ•°æ®åº“åŠ è½½äº‹é¡¹åˆ—è¡¨ï¼ˆé¢„åŠ è½½å…³ç³»æ•°æ®ï¼‰
        
        Args:
            event_ids: äº‹é¡¹IDåˆ—è¡¨
            
        Returns:
            äº‹é¡¹åˆ—è¡¨
        """
        if not event_ids:
            return []
        
        async with self.session_factory() as session:
            result = await session.execute(
                select(SourceEvent)
                .where(SourceEvent.id.in_(event_ids))
                .options(selectinload(SourceEvent.event_associations))
            )
            events = result.scalars().all()
            
            # ç¡®ä¿æ•°æ®åœ¨ session å¤–å¯è®¿é—®
            # è®¾ç½® expire_on_commit=False æˆ–åœ¨è¿™é‡Œè§¦å‘åŠ è½½
            session.expire_on_commit = False
            
            # è§¦å‘åŠ è½½å…³ç³»æ•°æ®å’Œæ‰€æœ‰å­—æ®µ
            for event in events:
                # è§¦å‘æ‰€æœ‰å­—æ®µåŠ è½½ï¼ŒåŒ…æ‹¬ created_time ç­‰
                _ = event.created_time
                _ = event.updated_time
                # è§¦å‘å…³ç³»æ•°æ®åŠ è½½
                if hasattr(event, 'event_associations'):
                    _ = len(event.event_associations)
            
            return list(events)

    async def _sync_to_elasticsearch(
        self, events: List[SourceEvent], config: ExtractConfig
    ) -> None:
        """
        å°†äº‹é¡¹å’Œå®ä½“åŒæ­¥åˆ°Elasticsearch

        Args:
            events: äº‹é¡¹åˆ—è¡¨
            config: æå–é…ç½®
        """
        self.logger.info(f"å¼€å§‹åŒæ­¥ {len(events)} ä¸ªäº‹é¡¹åˆ°Elasticsearch")
        
        # åˆå§‹åŒ– ES å®¢æˆ·ç«¯ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        if self.es_client is None:
            self.es_client = get_es_client()
            # Repository éœ€è¦ AsyncElasticsearch å¯¹è±¡ï¼Œè€Œä¸æ˜¯ ElasticsearchClient åŒ…è£…å¯¹è±¡
            self.event_repo = EventVectorRepository(self.es_client.client)
            self.entity_repo = EntityVectorRepository(self.es_client.client)
        
        # æ£€æŸ¥ ES è¿æ¥
        if not await self.es_client.ping():
            raise Exception("Elasticsearch è¿æ¥å¤±è´¥")
        
        # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„å®ä½“
        unique_entities = {}  # key: entity_id, value: Entityå¯¹è±¡
        for event in events:
            if hasattr(event, 'event_associations') and event.event_associations:
                for assoc in event.event_associations:
                    entity_id = assoc.entity_id
                    if entity_id not in unique_entities:
                        entity = await self._load_entity_by_id(entity_id)
                        if entity:
                            unique_entities[entity_id] = entity
        
        # 1. å…ˆåŒæ­¥å®ä½“ï¼ˆå¯é…ç½®ï¼‰
        if unique_entities and config.enable_entity_vector_sync:
            await self._sync_entities_to_es(list(unique_entities.values()), config)
        elif unique_entities:
            self.logger.info(f"è·³è¿‡ {len(unique_entities)} ä¸ªå®ä½“çš„å‘é‡åŒæ­¥ (enable_entity_vector_sync=False)")
        
        # 2. å†åŒæ­¥äº‹é¡¹
        await self._sync_events_to_es(events, config)
        
        entity_sync_status = f"{len(unique_entities)} ä¸ªå®ä½“" if config.enable_entity_vector_sync else "å®ä½“åŒæ­¥å·²ç¦ç”¨"
        self.logger.info(
            f"ESåŒæ­¥å®Œæˆ: {len(events)} ä¸ªäº‹é¡¹, {entity_sync_status}"
        )


    async def _load_entity_by_id(self, entity_id: str):
        """
        ä»æ•°æ®åº“åŠ è½½å®ä½“

        Args:
            entity_id: å®ä½“ID

        Returns:
            å®ä½“å¯¹è±¡æˆ–None
        """
        async with self.session_factory() as session:
            result = await session.execute(
                select(Entity).where(Entity.id == entity_id)
            )
            return result.scalar_one_or_none()

    async def _batch_sync_entities_to_es(
        self,
        entities: List[Entity],
        config: ExtractConfig
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡åŒæ­¥å®ä½“åˆ°ESï¼ˆä¸¤é˜¶æ®µå¤„ç†ï¼‰

        Args:
            entities: å®ä½“åˆ—è¡¨
            config: æå–é…ç½®

        Returns:
            ç»Ÿè®¡ä¿¡æ¯: {
                "total": int,
                "indexed": int,
                "embedding_failed": int,
                "es_failed": int,
                "time": str
            }
        """
        import time
        from dataflow.core.ai.factory import get_embedding_client
        from dataflow.core.storage import get_es_client

        start_time = time.perf_counter()

        if not entities:
            return {"total": 0, "indexed": 0, "embedding_failed": 0, "es_failed": 0, "time": "0.00s"}

        # è·å–ä¾èµ–
        embedding_client = await get_embedding_client(scenario='general')
        es_client = get_es_client()

        documents = []
        embedding_failed = 0

        # === é˜¶æ®µ1: æ‰¹é‡ç”Ÿæˆå‘é‡ ===
        for i in range(0, len(entities), config.embedding_batch_size):
            batch_entities = entities[i:i+config.embedding_batch_size]

            try:
                # æ‰¹é‡ç”Ÿæˆå‘é‡
                texts = [entity.name for entity in batch_entities]
                vectors = await embedding_client.batch_generate(texts)

                # æ„å»ºæ–‡æ¡£
                for entity, vector in zip(batch_entities, vectors):
                    doc = {
                        "id": entity.id,
                        "entity_id": entity.id,
                        "source_config_id": entity.source_config_id,
                        "type": entity.type,
                        "name": entity.name,
                        "vector": vector,
                        "normalized_name": entity.normalized_name or "",
                        "description": entity.description or "",
                        "created_time": entity.created_time.isoformat() if entity.created_time else None,
                    }
                    documents.append(doc)

            except Exception as e:
                self.logger.warning(f"æ‰¹é‡ç”Ÿæˆå‘é‡å¤±è´¥ï¼Œé™çº§é‡è¯•: {e}")
                # é™çº§: é€ä¸ªé‡è¯•
                for entity in batch_entities:
                    try:
                        vector = await embedding_client.generate(entity.name)
                        doc = {
                            "id": entity.id,
                            "entity_id": entity.id,
                            "source_config_id": entity.source_config_id,
                            "type": entity.type,
                            "name": entity.name,
                            "vector": vector,
                            "normalized_name": entity.normalized_name or "",
                            "description": entity.description or "",
                            "created_time": entity.created_time.isoformat() if entity.created_time else None,
                        }
                        documents.append(doc)
                    except Exception as retry_e:
                        self.logger.error(f"å•æ¡ç”Ÿæˆå‘é‡å¤±è´¥ {entity.id}: {retry_e}")
                        embedding_failed += 1

        # === é˜¶æ®µ2: æ‰¹é‡ç´¢å¼•åˆ°ES ===
        indexed = 0
        es_failed = 0

        for i in range(0, len(documents), config.es_bulk_index_size):
            batch = documents[i:i+config.es_bulk_index_size]

            try:
                result = await es_client.bulk_index(
                    index=self.entity_repo.INDEX_NAME,
                    documents=batch,
                    return_details=True,
                    routing=config.source_config_id
                )

                indexed += result["success_count"]

                # å¤„ç†å¤±è´¥é¡¹: é€ä¸ªé‡è¯•
                if result["error_count"] > 0:
                    failed_ids = {err["id"] for err in result["errors"]}
                    for doc in batch:
                        if doc["id"] in failed_ids:
                            try:
                                await es_client.index_document(
                                    index=self.entity_repo.INDEX_NAME,
                                    document=doc,
                                    doc_id=doc["id"],
                                    routing=config.source_config_id
                                )
                                indexed += 1
                            except Exception as retry_e:
                                self.logger.error(f"é‡è¯•ç´¢å¼•å¤±è´¥ {doc['id']}: {retry_e}")
                                es_failed += 1

            except Exception as e:
                self.logger.error(f"æ‰¹é‡ç´¢å¼•å¤±è´¥ï¼Œé™çº§é‡è¯•: {e}")
                # é™çº§: æ•´æ‰¹é€ä¸ªé‡è¯•
                for doc in batch:
                    try:
                        await es_client.index_document(
                            index=self.entity_repo.INDEX_NAME,
                            document=doc,
                            doc_id=doc["id"],
                            routing=config.source_config_id
                        )
                        indexed += 1
                    except Exception as retry_e:
                        self.logger.error(f"é™çº§ç´¢å¼•å¤±è´¥ {doc['id']}: {retry_e}")
                        es_failed += 1

        total_time = time.perf_counter() - start_time

        stats = {
            "total": len(entities),
            "indexed": indexed,
            "embedding_failed": embedding_failed,
            "es_failed": es_failed,
            "time": f"{total_time:.2f}s"
        }

        if es_failed > 0 or embedding_failed > 0:
            self.logger.warning(f"å®ä½“åŒæ­¥éƒ¨åˆ†å¤±è´¥: {stats}")
        else:
            self.logger.debug(f"å®ä½“åŒæ­¥æˆåŠŸ: {indexed}/{len(entities)} æ¡, è€—æ—¶{total_time:.2f}s")

        return stats

    async def _sync_entities_to_es(
        self, entities: List[Entity], config: ExtractConfig
    ) -> None:
        """
        å°†å®ä½“åŒæ­¥åˆ°Elasticsearchï¼ˆè·¯ç”±æ–¹æ³•ï¼‰

        æ ¹æ® config.enable_batch_indexing é€‰æ‹©:
        - True: æ‰¹é‡å¤„ç† (æ–°å®ç°)
        - False: ä¸²è¡Œå¤„ç† (ä¿æŒåŸæœ‰é€»è¾‘)

        Args:
            entities: å®ä½“åˆ—è¡¨
            config: æå–é…ç½®
        """
        if not entities:
            return

        # è¿‡æ»¤æ‰ä¸éœ€è¦ç´¢å¼•åˆ°ESçš„å®ä½“ç±»å‹ï¼ˆå¦‚æ—¶é—´ç±»å‹ï¼‰
        original_count = len(entities)
        entities = [e for e in entities if should_index_entity_to_es(e.type)]

        if len(entities) < original_count:
            skipped_count = original_count - len(entities)
            self.logger.info(
                f"è¿‡æ»¤æ‰ {skipped_count} ä¸ªä¸éœ€è¦ç´¢å¼•åˆ°ESçš„å®ä½“ "
                f"(æ€»æ•°: {original_count}, ç´¢å¼•: {len(entities)})"
            )

        if not entities:
            self.logger.debug("æ‰€æœ‰å®ä½“éƒ½è¢«è¿‡æ»¤ï¼Œè·³è¿‡ESåŒæ­¥")
            return

        # æ‰¹é‡å¤„ç†è·¯å¾„
        if config.enable_batch_indexing:
            stats = await self._batch_sync_entities_to_es(entities, config)
            # æ—¥å¿—å·²åœ¨ _batch_sync_entities_to_es ä¸­è¾“å‡º
            return

        # ä¸²è¡Œå¤„ç†è·¯å¾„ (ä¿æŒåŸæœ‰é€»è¾‘)
        self.logger.debug(f"åŒæ­¥ {len(entities)} ä¸ªå®ä½“åˆ°ES (ä¸²è¡Œæ¨¡å¼)")

        success_count = 0
        error_count = 0

        embedding_client = await get_embedding_client(scenario='general')

        for entity in entities:
            try:
                # ç”Ÿæˆå®ä½“åç§°çš„å‘é‡ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„embeddingæœåŠ¡ï¼‰
                vector = await embedding_client.generate(entity.name)

                # ä½¿ç”¨ Repository çš„ index_entity æ–¹æ³•ç´¢å¼•
                await self.entity_repo.index_entity(
                    entity_id=entity.id,
                    source_config_id=entity.source_config_id,
                    entity_type=entity.type,
                    name=entity.name,
                    vector=vector,
                    normalized_name=entity.normalized_name or "",
                    description=entity.description or "",
                    created_time=entity.created_time.isoformat() if entity.created_time else None,
                )
                success_count += 1

            except Exception as e:
                self.logger.error(f"å®ä½“ç´¢å¼•å¤±è´¥ {entity.id}: {e}")
                error_count += 1

        if error_count > 0:
            self.logger.warning(
                f"å®ä½“åŒæ­¥éƒ¨åˆ†å¤±è´¥: æˆåŠŸ{success_count}, å¤±è´¥{error_count}"
            )
        else:
            self.logger.debug(f"å®ä½“åŒæ­¥æˆåŠŸ: {success_count} ä¸ª")

    async def _batch_sync_events_to_es(
        self,
        events: List[SourceEvent],
        config: ExtractConfig
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡åŒæ­¥äº‹é¡¹åˆ°ESï¼ˆä¸¤é˜¶æ®µå¤„ç†ï¼‰

        Args:
            events: äº‹é¡¹åˆ—è¡¨
            config: æå–é…ç½®

        Returns:
            ç»Ÿè®¡ä¿¡æ¯: {
                "total": int,
                "indexed": int,
                "embedding_failed": int,
                "es_failed": int,
                "time": str
            }
        """
        import time
        from dataflow.core.ai.factory import get_embedding_client
        from dataflow.core.storage import get_es_client

        start_time = time.perf_counter()

        if not events:
            return {"total": 0, "indexed": 0, "embedding_failed": 0, "es_failed": 0, "time": "0.00s"}

        # è·å–ä¾èµ–
        embedding_client = await get_embedding_client(scenario='general')
        es_client = get_es_client()

        documents = []
        embedding_failed = 0

        # === é˜¶æ®µ1: æ‰¹é‡ç”Ÿæˆå‘é‡ ===
        for i in range(0, len(events), config.embedding_batch_size):
            batch_events = events[i:i+config.embedding_batch_size]

            try:
                # å‡†å¤‡æ–‡æœ¬ (æ¯ä¸ªäº‹é¡¹éœ€è¦2ä¸ªå‘é‡)
                title_texts = [event.title for event in batch_events]
                content_texts = [
                    f"{event.title}\n\n{event.content[:500]}"
                    for event in batch_events
                ]

                # æ‰¹é‡ç”Ÿæˆå‘é‡
                title_vectors = await embedding_client.batch_generate(title_texts)
                content_vectors = await embedding_client.batch_generate(content_texts)

                # æ„å»ºæ–‡æ¡£
                for event, title_vec, content_vec in zip(batch_events, title_vectors, content_vectors):
                    # æå–å…³è”å®ä½“ID
                    entity_ids = []
                    if hasattr(event, 'event_associations') and event.event_associations:
                        entity_ids = [assoc.entity_id for assoc in event.event_associations]

                    # å‡†å¤‡é¢å¤–å­—æ®µ
                    extra_fields = {}
                    if event.extra_data and "tags" in event.extra_data:
                        extra_fields["tags"] = event.extra_data["tags"]
                    if event.category:
                        extra_fields["category"] = event.category

                    doc = {
                        "id": event.id,
                        "event_id": event.id,
                        "source_config_id": event.source_config_id,
                        "source_type": event.source_type,
                        "source_id": event.source_id,
                        "title": event.title,
                        "summary": event.summary or "",
                        "content": event.content,
                        "title_vector": title_vec,
                        "content_vector": content_vec,
                        "entity_ids": entity_ids,
                        "start_time": event.start_time.isoformat() if event.start_time else None,
                        "end_time": event.end_time.isoformat() if event.end_time else None,
                        "created_time": event.created_time.isoformat() if event.created_time else None,
                        **extra_fields,
                    }
                    documents.append(doc)

            except Exception as e:
                self.logger.warning(f"æ‰¹é‡ç”Ÿæˆå‘é‡å¤±è´¥ï¼Œé™çº§é‡è¯•: {e}")
                # é™çº§: é€ä¸ªé‡è¯•
                for event in batch_events:
                    try:
                        title_vec = await embedding_client.generate(event.title)
                        content_for_vec = f"{event.title}\n\n{event.content[:500]}"
                        content_vec = await embedding_client.generate(content_for_vec)

                        # æå–å…³è”å®ä½“ID
                        entity_ids = []
                        if hasattr(event, 'event_associations') and event.event_associations:
                            entity_ids = [assoc.entity_id for assoc in event.event_associations]

                        # å‡†å¤‡é¢å¤–å­—æ®µ
                        extra_fields = {}
                        if event.extra_data and "tags" in event.extra_data:
                            extra_fields["tags"] = event.extra_data["tags"]
                        if event.category:
                            extra_fields["category"] = event.category

                        doc = {
                            "id": event.id,
                            "event_id": event.id,
                            "source_config_id": event.source_config_id,
                            "source_type": event.source_type,
                            "source_id": event.source_id,
                            "title": event.title,
                            "summary": event.summary or "",
                            "content": event.content,
                            "title_vector": title_vec,
                            "content_vector": content_vec,
                            "entity_ids": entity_ids,
                            "start_time": event.start_time.isoformat() if event.start_time else None,
                            "end_time": event.end_time.isoformat() if event.end_time else None,
                            "created_time": event.created_time.isoformat() if event.created_time else None,
                            **extra_fields,
                        }
                        documents.append(doc)
                    except Exception as retry_e:
                        self.logger.error(f"å•æ¡ç”Ÿæˆå‘é‡å¤±è´¥ {event.id}: {retry_e}")
                        embedding_failed += 1

        # === é˜¶æ®µ2: æ‰¹é‡ç´¢å¼•åˆ°ES ===
        indexed = 0
        es_failed = 0

        for i in range(0, len(documents), config.es_bulk_index_size):
            batch = documents[i:i+config.es_bulk_index_size]

            try:
                result = await es_client.bulk_index(
                    index=self.event_repo.INDEX_NAME,
                    documents=batch,
                    return_details=True,
                    routing=config.source_config_id
                )

                indexed += result["success_count"]

                # å¤„ç†å¤±è´¥é¡¹: é€ä¸ªé‡è¯•
                if result["error_count"] > 0:
                    failed_ids = {err["id"] for err in result["errors"]}
                    for doc in batch:
                        if doc["id"] in failed_ids:
                            try:
                                await es_client.index_document(
                                    index=self.event_repo.INDEX_NAME,
                                    document=doc,
                                    doc_id=doc["id"],
                                    routing=config.source_config_id
                                )
                                indexed += 1
                            except Exception as retry_e:
                                self.logger.error(f"é‡è¯•ç´¢å¼•å¤±è´¥ {doc['id']}: {retry_e}")
                                es_failed += 1

            except Exception as e:
                self.logger.error(f"æ‰¹é‡ç´¢å¼•å¤±è´¥ï¼Œé™çº§é‡è¯•: {e}")
                # é™çº§: æ•´æ‰¹é€ä¸ªé‡è¯•
                for doc in batch:
                    try:
                        await es_client.index_document(
                            index=self.event_repo.INDEX_NAME,
                            document=doc,
                            doc_id=doc["id"],
                            routing=config.source_config_id
                        )
                        indexed += 1
                    except Exception as retry_e:
                        self.logger.error(f"é™çº§ç´¢å¼•å¤±è´¥ {doc['id']}: {retry_e}")
                        es_failed += 1

        total_time = time.perf_counter() - start_time

        stats = {
            "total": len(events),
            "indexed": indexed,
            "embedding_failed": embedding_failed,
            "es_failed": es_failed,
            "time": f"{total_time:.2f}s"
        }

        if es_failed > 0 or embedding_failed > 0:
            self.logger.warning(f"äº‹é¡¹åŒæ­¥éƒ¨åˆ†å¤±è´¥: {stats}")
        else:
            self.logger.debug(f"äº‹é¡¹åŒæ­¥æˆåŠŸ: {indexed}/{len(events)} æ¡, è€—æ—¶{total_time:.2f}s")

        return stats

    async def _sync_events_to_es(
        self, events: List[SourceEvent], config: ExtractConfig
    ) -> None:
        """
        å°†äº‹é¡¹åŒæ­¥åˆ°Elasticsearchï¼ˆè·¯ç”±æ–¹æ³•ï¼‰

        æ ¹æ® config.enable_batch_indexing é€‰æ‹©:
        - True: æ‰¹é‡å¤„ç† (æ–°å®ç°)
        - False: ä¸²è¡Œå¤„ç† (ä¿æŒåŸæœ‰é€»è¾‘)

        Args:
            events: äº‹é¡¹åˆ—è¡¨
            config: æå–é…ç½®
        """
        if not events:
            return

        # æ‰¹é‡å¤„ç†è·¯å¾„
        if config.enable_batch_indexing:
            stats = await self._batch_sync_events_to_es(events, config)
            # æ—¥å¿—å·²åœ¨ _batch_sync_events_to_es ä¸­è¾“å‡º
            return

        # ä¸²è¡Œå¤„ç†è·¯å¾„ (ä¿æŒåŸæœ‰é€»è¾‘)
        self.logger.debug(f"åŒæ­¥ {len(events)} ä¸ªäº‹é¡¹åˆ°ES (ä¸²è¡Œæ¨¡å¼)")

        success_count = 0
        error_count = 0

        embedding_client = await get_embedding_client(scenario='general')

        for event in events:
            try:
                # ç”Ÿæˆæ ‡é¢˜å‘é‡ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„embeddingæœåŠ¡ï¼‰
                title_vector = await embedding_client.generate(event.title)

                # ç”Ÿæˆå†…å®¹å‘é‡ï¼ˆä½¿ç”¨æ ‡é¢˜+éƒ¨åˆ†å†…å®¹ï¼‰
                content_for_vector = f"{event.title}\n\n{event.content[:500]}"
                content_vector = await embedding_client.generate(content_for_vector)

                # æå–å…³è”çš„å®ä½“IDåˆ—è¡¨
                entity_ids = []
                if hasattr(event, 'event_associations') and event.event_associations:
                    entity_ids = [assoc.entity_id for assoc in event.event_associations]

                # å‡†å¤‡é¢å¤–å­—æ®µ
                extra_fields = {}
                if event.extra_data:
                    # categoryä¸å†ä»extra_dataè¯»å–
                    if "tags" in event.extra_data:
                        extra_fields["tags"] = event.extra_data["tags"]

                # categoryä»ç‹¬ç«‹å­—æ®µè¯»å–
                if event.category:
                    extra_fields["category"] = event.category

                # ä½¿ç”¨ Repository çš„ index_event æ–¹æ³•ç´¢å¼•
                await self.event_repo.index_event(
                    event_id=event.id,
                    source_config_id=event.source_config_id,
                    source_type=event.source_type,
                    source_id=event.source_id,
                    title=event.title,
                    summary=event.summary or "",
                    content=event.content,
                    title_vector=title_vector,
                    content_vector=content_vector,
                    entity_ids=entity_ids,
                    start_time=event.start_time.isoformat() if event.start_time else None,
                    end_time=event.end_time.isoformat() if event.end_time else None,
                    created_time=event.created_time.isoformat() if event.created_time else None,
                    **extra_fields,
                )
                success_count += 1

            except Exception as e:
                self.logger.error(f"äº‹é¡¹ç´¢å¼•å¤±è´¥ {event.id}: {e}")
                error_count += 1

        if error_count > 0:
            self.logger.warning(
                f"äº‹é¡¹åŒæ­¥éƒ¨åˆ†å¤±è´¥: æˆåŠŸ{success_count}, å¤±è´¥{error_count}"
            )
        else:
            self.logger.debug(f"äº‹é¡¹åŒæ­¥æˆåŠŸ: {success_count} ä¸ª")

    async def _reload_events_with_entities(self, article_id: str) -> List[SourceEvent]:
        """
        é‡æ–°åŠ è½½äº‹é¡¹ï¼Œé¢„åŠ è½½å®ä½“å…³ç³»
        
        Args:
            article_id: æ–‡ç« ID
            
        Returns:
            åŒ…å«å®Œæ•´å®ä½“å…³ç³»çš„äº‹é¡¹åˆ—è¡¨
        """
        async with self.session_factory() as session:
            result = await session.execute(
                select(SourceEvent)
                .where(SourceEvent.article_id == article_id)
                .order_by(SourceEvent.rank)
                .options(
                    selectinload(SourceEvent.event_associations).selectinload(EventEntity.entity)
                )
            )
            events = list(result.scalars().all())
        
        self.logger.debug(f"é‡æ–°åŠ è½½äº† {len(events)} ä¸ªäº‹é¡¹ï¼ˆåŒ…å«å®ä½“å…³ç³»ï¼‰")
        return events

    async def extract_from_chunk(
        self,
        chunk: SourceChunk,
        config: ExtractConfig
    ) -> List[SourceEvent]:
        """
        ä»chunkæå–äº‹é¡¹
        
        Args:
            chunk: æ¥æºç‰‡æ®µå¯¹è±¡
            config: æå–é…ç½®
        
        Returns:
            æå–çš„äº‹é¡¹åˆ—è¡¨
        """
        try:
            self.logger.info(f"å¼€å§‹ä»chunkæå–: chunk_id={chunk.id}, type={chunk.source_type}")
            
            # 1. åŠ è½½å†…å®¹å’Œå…ƒæ•°æ®
            content_items, raw_metadata = await self._load_chunk_content(chunk)
            if not content_items:
                self.logger.warning(f"Chunk {chunk.id} æ— å†…å®¹")
                return []
            
            # 2. åˆ›å»º EventProcessor
            llm_client = await self._get_llm_client()
            processor = EventProcessor(
                llm_client=llm_client,
                prompt_manager=self.prompt_manager,
                config=config,
            )
            await processor.initialize()
            
            # 3. æ„å»ºå…ƒæ•°æ®
            metadata = {
                "document_title": raw_metadata.get("title", ""),
                "chunk_title": chunk.heading or f"ç‰‡æ®µ{chunk.rank + 1}",
                "previous_context": self._format_previous_context(raw_metadata.get("previous_chunk")),
            }
            
            # 4. è°ƒç”¨æå–
            events = await processor.extract(
                items=content_items,
                metadata=metadata,
                source_type=chunk.source_type
            )
            
            # 5. å…³è” chunk_id å¹¶å¤„ç†å®ä½“
            for event in events:
                event.chunk_id = chunk.id
            
            # 5.5. å®ä½“äº¤å‰è¡¥å……
            if len(events) > 1:
                events = processor._cross_fill_entities(events)
            
            # 6. å¤„ç†å®ä½“å…³è”
            events = await processor.process_entity_associations(events)
            
            self.logger.info(f"Chunkæå–å®Œæˆ: chunk_id={chunk.id}, events={len(events)}")
            return events
            
        except Exception as e:
            self.logger.error(f"Chunkæå–å¤±è´¥: {e}", exc_info=True)
            raise ExtractError(f"Chunkæå–å¤±è´¥: {e}") from e
    
    def _format_previous_context(self, previous_chunk) -> str:
        """æ ¼å¼åŒ–å‰æ–‡ä¸Šä¸‹æ–‡"""
        if not previous_chunk:
            return ""
        
        title = previous_chunk.get("heading") or previous_chunk.get("title") or "å‰æ–‡"
        content = previous_chunk.get("content", "")
        
        if len(content) > 300:
            content = content[:300] + "..."
        
        return f"**{title}**\n{content}"
    
    async def _load_chunk_content(self, chunk: SourceChunk):
        """åŠ è½½chunkçš„å†…å®¹å’Œå…ƒæ•°æ®"""
        if chunk.source_type == 'ARTICLE':
            return await self._load_article_content(chunk)
        elif chunk.source_type == 'CHAT':
            return await self._load_conversation_content(chunk)
        else:
            raise ExtractError(f"ä¸æ”¯æŒçš„ç±»å‹: {chunk.source_type}")
    
    async def _load_article_content(self, chunk: SourceChunk):
        """åŠ è½½æ–‡ç« ç‰‡æ®µ + ä¸Šæ–‡chunkå†…å®¹ä½œä¸ºèƒŒæ™¯"""
        from dataflow.db import Article, ArticleSection, SourceChunk as SC
        
        async with self.session_factory() as session:
            # 1. åŠ è½½æ–‡ç« ï¼ˆæºèƒŒæ™¯ï¼‰
            article = await session.get(Article, chunk.source_id)
            if not article:
                raise ExtractError(f"æ–‡ç« ä¸å­˜åœ¨: {chunk.source_id}")
            
            # 2. åŠ è½½å½“å‰chunkçš„sectionsï¼ˆå¾…å¤„ç†å†…å®¹ï¼‰
            section_ids = chunk.references if chunk.references else []
            
            if section_ids:
                sections_result = await session.execute(
                    select(ArticleSection)
                    .where(ArticleSection.id.in_(section_ids))
                    .order_by(ArticleSection.rank)
                )
            else:
                sections_result = await session.execute(
                    select(ArticleSection)
                    .where(ArticleSection.article_id == chunk.source_id)
                    .order_by(ArticleSection.rank)
                )
            
            sections = list(sections_result.scalars().all())
            
            # 3. åŠ è½½ä¸Šä¸€ä¸ªchunkçš„å†…å®¹ï¼ˆä¸Šæ–‡èƒŒæ™¯ï¼‰
            previous_chunk = None
            if chunk.rank > 0:
                prev_result = await session.execute(
                    select(SC)
                    .where(SC.source_id == chunk.source_id)
                    .where(SC.source_type == 'ARTICLE')
                    .where(SC.rank == chunk.rank - 1)
                )
                previous_chunk = prev_result.scalar_one_or_none()
            
            return sections, {
                # Article è¡¨å­—æ®µï¼ˆæºèƒŒæ™¯ï¼‰
                "title": article.title,
                "summary": article.summary,
                "category": article.category,
                "tags": article.tags,
                
                # å½“å‰ Chunk ä¿¡æ¯
                "chunk_rank": chunk.rank,
                "chunk_heading": chunk.heading,
                
                # ä¸Šæ–‡ Chunkï¼ˆæä¾›ä¸Šä¸‹æ–‡ï¼‰
                "previous_chunk": {
                    "heading": previous_chunk.heading,
                    "content": previous_chunk.content[:800] if len(previous_chunk.content or "") > 800 else previous_chunk.content
                } if previous_chunk and previous_chunk.content else None
            }
    
    async def _load_conversation_content(self, chunk: SourceChunk):
        """åŠ è½½å¯¹è¯æ¶ˆæ¯ + ä¸Šæ–‡chunkå†…å®¹ä½œä¸ºèƒŒæ™¯"""
        from dataflow.db import ChatConversation, ChatMessage, SourceChunk as SC
        
        async with self.session_factory() as session:
            # 1. åŠ è½½ä¼šè¯ï¼ˆæºèƒŒæ™¯ï¼‰
            conversation = await session.get(ChatConversation, chunk.source_id)
            if not conversation:
                raise ExtractError(f"ä¼šè¯ä¸å­˜åœ¨: {chunk.source_id}")
            
            # 2. åŠ è½½å½“å‰chunkçš„messagesï¼ˆå¾…å¤„ç†å†…å®¹ï¼‰
            message_ids = chunk.references if chunk.references else []
            
            if message_ids:
                messages_result = await session.execute(
                    select(ChatMessage)
                    .where(ChatMessage.id.in_(message_ids))
                    .order_by(ChatMessage.timestamp)
                )
            else:
                messages_result = await session.execute(
                    select(ChatMessage)
                    .where(ChatMessage.conversation_id == chunk.source_id)
                    .order_by(ChatMessage.timestamp)
                )
            
            messages = list(messages_result.scalars().all())
            
            # ä»å½“å‰messagesæå–å‚ä¸è€…å’Œæ—¶é—´ï¼ˆæ— éœ€é¢å¤–æŸ¥è¯¢ï¼‰
            participants = []
            seen_names = set()
            for msg in messages:
                if msg.sender_name and msg.sender_name not in seen_names:
                    participants.append({
                        "name": msg.sender_name,
                        "role": msg.sender_role
                    })
                    seen_names.add(msg.sender_name)
            
            time_range = ""
            if messages:
                start = messages[0].timestamp.strftime('%H:%M')
                end = messages[-1].timestamp.strftime('%H:%M')
                time_range = f"{start} ~ {end}"
            
            # 3. åŠ è½½ä¸Šä¸€ä¸ªchunkçš„å†…å®¹ï¼ˆä¸Šæ–‡èƒŒæ™¯ï¼‰
            previous_chunk = None
            if chunk.rank > 0:
                prev_result = await session.execute(
                    select(SC)
                    .where(SC.source_id == chunk.source_id)
                    .where(SC.source_type == 'CHAT')
                    .where(SC.rank == chunk.rank - 1)
                )
                previous_chunk = prev_result.scalar_one_or_none()
            
            return messages, {
                # ChatConversation è¡¨å­—æ®µï¼ˆæºèƒŒæ™¯ï¼‰
                "title": conversation.title,
                "messages_count": conversation.messages_count,
                
                # extra_data å­—æ®µ
                "platform": conversation.extra_data.get("platform") if conversation.extra_data else None,
                "scenario": conversation.extra_data.get("scenario") if conversation.extra_data else None,
                
                # ä»å½“å‰messagesæå–
                "participants": participants,
                "time_range": time_range,
                
                # å½“å‰ Chunk ä¿¡æ¯
                "chunk_rank": chunk.rank,
                "chunk_heading": chunk.heading,
                
                # ä¸Šæ–‡ Chunkï¼ˆæä¾›ä¸Šä¸‹æ–‡ï¼‰
                "previous_chunk": {
                    "heading": previous_chunk.heading,
                    "content": previous_chunk.content[:800] if len(previous_chunk.content or "") > 800 else previous_chunk.content
                } if previous_chunk and previous_chunk.content else None
            }
    
    async def _load_entity_types_for_chunk(self, config: ExtractConfig) -> List[Dict]:
        """
        åŠ è½½å®ä½“ç±»å‹å®šä¹‰ï¼ˆå¿…å®šè¿”å›éç©ºåˆ—è¡¨ï¼‰
        
        ä¼˜å…ˆçº§ï¼š
        1. é»˜è®¤å…¨å±€ç±»å‹ï¼ˆis_default=Trueï¼‰
        2. sourceçº§åˆ«è‡ªå®šä¹‰ç±»å‹
        3. configä¸­çš„è¿è¡Œæ—¶ç±»å‹
        """
        entity_types = []
        
        async with self.session_factory() as session:
            # åŠ è½½é»˜è®¤ç±»å‹
            default_result = await session.execute(
                select(DBEntityType)
                .where(DBEntityType.is_default == True)
                .where(DBEntityType.is_active == True)
            )
            default_types = default_result.scalars().all()
            
            entity_types.extend([{
                "type": et.type,
                "name": et.name,
                "description": et.description or "",
                "weight": float(et.weight),
                "value_constraints": et.value_constraints,  # ğŸ†• JSON é…ç½®
            } for et in default_types])
            
            # åŠ è½½sourceçº§åˆ«ç±»å‹
            if config.source_config_id:
                custom_result = await session.execute(
                    select(DBEntityType)
                    .where(DBEntityType.source_config_id == config.source_config_id)
                    .where(DBEntityType.is_active == True)
                )
                custom_types = custom_result.scalars().all()
                
                entity_types.extend([{
                    "type": et.type,
                    "name": et.name,
                    "description": et.description or "",
                    "weight": float(et.weight),
                    "value_constraints": et.value_constraints,  # ğŸ†• JSON é…ç½®
                } for et in custom_types])
        
        # è¿è¡Œæ—¶ç±»å‹ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
        if config.custom_entity_types:
            entity_types.extend([{
                "type": et.type,
                "name": et.name,
                "description": et.description,
                "weight": et.weight,
                "value_constraints": getattr(et, 'value_constraints', None),  # ğŸ†•
            } for et in config.custom_entity_types])
        
        return entity_types
    
    async def _build_events_from_result(
        self,
        result: Dict,
        chunk: SourceChunk,
        config: ExtractConfig
    ) -> List[SourceEvent]:
        """è½¬æ¢Agentç»“æœä¸ºSourceEvent"""
        events = []

        # ğŸ†• å®ä½“ç¼“å­˜ï¼šé¿å…åŒä¸€æ‰¹å¤„ç†ä¸­é‡å¤åˆ›å»ºç›¸åŒå®ä½“
        entity_cache = {}  # {(type, normalized_name): Entity}
        
        # ğŸ†• è¿‡æ»¤ç»Ÿè®¡
        filter_stats = {'total': 0, 'llm_invalid': 0, 'rule_filtered': 0, 'valid': 0}
        filter_details = []

        for idx, evt in enumerate(result.get('events', [])):
            filter_stats['total'] += 1
            title_short = evt.get('title', '')[:20]
            
            # ========== ç¬¬1å±‚ï¼šLLM æ ‡è®°è¿‡æ»¤ ==========
            if not evt.get('is_valid', True):
                filter_stats['llm_invalid'] += 1
                filter_details.append(f"  âŒ LLMæ— æ•ˆ: {title_short}")
                continue
            
            # ========== ç¬¬2å±‚ï¼šä»£ç è§„åˆ™å…œåº• ==========
            is_low, reason = self._is_low_quality(evt)
            if is_low:
                filter_stats['rule_filtered'] += 1
                filter_details.append(f"  âš ï¸ {reason}: {title_short}")
                continue
            
            filter_stats['valid'] += 1
            
            # å¤„ç† references æ ¼å¼ï¼ˆå…¼å®¹æ—§æ ¼å¼å’Œæ–°æ ¼å¼ï¼‰
            raw_references = evt.get('references', [])
            if raw_references and isinstance(raw_references[0], dict):
                # æ—§æ ¼å¼: [{"type": "section", "id": "xxx"}] -> æå– ID
                references = [ref['id'] for ref in raw_references if isinstance(ref, dict) and 'id' in ref]
            else:
                # æ–°æ ¼å¼: ["xxx", "yyy"] æˆ–ç©ºåˆ—è¡¨
                references = raw_references
            
            # ğŸ†• æ ¹æ®æ¥æºç±»å‹è®¾ç½®æ—¶é—´
            from datetime import datetime
            from dataflow.db import ChatMessage
            from sqlalchemy import select as sql_select
            
            start_time = None
            end_time = None
            
            if chunk.source_type == "ARTICLE":
                # æ–‡æ¡£ç±»å‹ï¼šä½¿ç”¨å½“å‰æ—¶é—´
                current_time = datetime.now()
                start_time = current_time
                end_time = current_time
                
            elif chunk.source_type == "CHAT":
                # ä¼šè¯ç±»å‹ï¼šä»äº‹é¡¹å¼•ç”¨çš„æ¶ˆæ¯ä¸­è·å–æ—¶é—´èŒƒå›´
                # âœ… ä½¿ç”¨äº‹é¡¹è‡ªå·±çš„ referencesï¼ˆä» LLM æå–ç»“æœæ¥çš„ï¼‰
                if references and isinstance(references, list):
                    async with self.session_factory() as session:
                        result_msgs = await session.execute(
                            sql_select(ChatMessage)
                            .where(ChatMessage.id.in_(references))
                            .order_by(ChatMessage.timestamp)
                        )
                        messages = list(result_msgs.scalars().all())
                        
                        if messages:
                            start_time = messages[0].timestamp
                            end_time = messages[-1].timestamp
            
            # åˆ›å»ºäº‹é¡¹
            event = SourceEvent(
                id=str(uuid.uuid4()),
                source_config_id=config.source_config_id,
                source_type=chunk.source_type,
                source_id=chunk.source_id,
                article_id=chunk.source_id if chunk.source_type == 'ARTICLE' else None,
                conversation_id=chunk.source_id if chunk.source_type == 'CHAT' else None,
                chunk_id=chunk.id,
                title=evt['title'],
                summary=evt['summary'],
                content=evt['content'],
                category=evt.get('category', ''),
                # ä¸šåŠ¡å­—æ®µï¼ˆå…¼å®¹ä¸»ç³»ç»Ÿï¼‰- typeä¸source_typeä¿æŒä¸€è‡´
                type=chunk.source_type,
                priority="UNKNOWN",  # é»˜è®¤å€¼
                status="UNKNOWN",  # é»˜è®¤å€¼
                rank=idx,
                start_time=start_time,  # ğŸ†•
                end_time=end_time,      # ğŸ†•
                references=references
            )

            # å…³è”å®ä½“ï¼ˆä½¿ç”¨å»é‡ä¸æè¿°åˆå¹¶é€»è¾‘ï¼‰
            # æ³¨æ„ï¼šæ—¶é—´ç±»å‹å®ä½“ä¸æ³¨å…¥ï¼Œå› ä¸ºå®ƒä»¬ä¸éœ€è¦ç”¨äºç›¸ä¼¼åº¦è®¡ç®—å’Œå…³è”æ£€ç´¢
            event.event_associations = []

            # è·å–å®ä½“åˆ—è¡¨
            entities_list = list(evt.get('entities', []))  # æ‹·è´ï¼Œé¿å…ä¿®æ”¹åŸæ•°æ®
            
            # ğŸ†• æ··åˆæ¨¡å¼å…œåº•ï¼šLLM + åˆ†è¯å™¨åŒé€šé“
            existing_names = {
                e.get('name', '').strip().lower() 
                for e in entities_list 
                if e.get('name')
            }
            
            # 1. LLMå…œåº•ï¼šè¡¥å……å®ä½“ï¼ˆå¸¦æ­£ç¡®åˆ†ç±»ï¼‰
            entity_types = await self._load_entity_types_for_chunk(config)
            fallback_entities = await self._fallback_extract_entities_with_llm(
                references, chunk.source_type, existing_names, entity_types
            )
            entities_list.extend(fallback_entities)
            
            # 2. åˆ†è¯å™¨è¡¥å……ï¼šä¸“æœ‰åè¯ä½œä¸º tagsï¼ˆå…œåº•çš„å…œåº•ï¼‰
            # æ›´æ–° existing_names åŒ…å« LLM å…œåº•çš„ç»“æœ
            for e in fallback_entities:
                existing_names.add(e.get('name', '').strip().lower())
            
            tags_entities = await self._fallback_extract_tags(
                references, chunk.source_type, existing_names
            )
            entities_list.extend(tags_entities)

            # ä½¿ç”¨å­—å…¸è·Ÿè¸ªæ¯ä¸ªå®ä½“IDåŠå…¶æè¿°ï¼ˆé˜²æ­¢é‡å¤å…³è”ï¼‰
            entity_map = {}  # {entity_id: {"name": str, "descriptions": [str]}}

            # ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰å®ä½“åŠå…¶æè¿°
            for entity_data in entities_list:
                # ğŸ†• å…ˆæ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤åˆ›å»ºç›¸åŒå®ä½“
                cache_key = (entity_data.get('type', ''), entity_data.get('name', '').strip().lower())

                if cache_key in entity_cache:
                    entity = entity_cache[cache_key]
                else:
                    entity = await self._get_or_create_entity(entity_data, config)
                    if entity:
                        entity_cache[cache_key] = entity

                # è·³è¿‡æ— æ•ˆå®ä½“ï¼ˆtypeä¸å­˜åœ¨æ—¶è¿”å›Noneï¼‰
                if entity is None:
                    continue

                # æ£€æŸ¥æ˜¯å¦å·²æ·»åŠ è¿‡è¿™ä¸ªå®ä½“
                if entity.id not in entity_map:
                    entity_map[entity.id] = {
                        "name": entity.name,
                        "descriptions": []
                    }

                # æ”¶é›†æè¿°ï¼ˆå»é‡ï¼‰
                description = entity_data.get('description', '').strip()
                if description and description not in entity_map[entity.id]["descriptions"]:
                    entity_map[entity.id]["descriptions"].append(description)

            # ç¬¬äºŒéï¼šä¸ºæ¯ä¸ªå”¯ä¸€çš„ entity_id åˆ›å»ºä¸€ä¸ªå…³è”ï¼ˆåˆå¹¶æè¿°ï¼‰
            for entity_id, info in entity_map.items():
                # åˆå¹¶æè¿°
                if info["descriptions"]:
                    final_description = "ã€".join(info["descriptions"])
                else:
                    final_description = ""

                assoc = EventEntity(
                    id=str(uuid.uuid4()),
                    event_id=event.id,
                    entity_id=entity_id,
                    description=final_description  # åˆå¹¶åçš„è§’è‰²æè¿°
                )
                # âœ… ä¸è®¾ç½® entity å…³ç³»ï¼Œé¿å…è·¨ session å†²çª
                # assoc.entity = entity  # ç§»é™¤ï¼Œä¼šåœ¨ä¿å­˜åé‡æ–°åŠ è½½

                event.event_associations.append(assoc)

                # æ—¥å¿—ï¼šå¦‚æœåˆå¹¶äº†å¤šä¸ªæè¿°
                if len(info["descriptions"]) > 1:
                    self.logger.debug(
                        f"âœ… åˆå¹¶å®ä½“æè¿°: {info['name']} ({len(info['descriptions'])}ä¸ª) -> {final_description}"
                    )
            
            events.append(event)
        
        # ğŸ†• è¿‡æ»¤æ—¥å¿—ï¼ˆå§‹ç»ˆè¾“å‡ºç»Ÿè®¡ï¼‰
        filtered_total = filter_stats['llm_invalid'] + filter_stats['rule_filtered']
        
        msg = (
            f"ğŸ“Š äº‹é¡¹è´¨é‡: chunk={chunk.id[:8]}..., "
            f"æ€»è®¡{filter_stats['total']}ä¸ª, æœ‰æ•ˆ{filter_stats['valid']}ä¸ª"
            + (f", è¿‡æ»¤{filtered_total}ä¸ª (LLM{filter_stats['llm_invalid']}+è§„åˆ™{filter_stats['rule_filtered']})"
               if filtered_total > 0 else "")
        )
        self.logger.info(msg)
        
        # è¿‡æ»¤è¯¦æƒ…ï¼ˆæœ‰è¿‡æ»¤æ—¶è¾“å‡ºï¼‰
        if filter_details:
            for detail in filter_details:
                self.logger.info(detail)
        
        return events
    
    def _is_low_quality(self, evt: Dict) -> tuple:
        """
        ä»£ç å±‚è´¨é‡å…œåº•ï¼ˆåªè¿‡æ»¤æ˜æ˜¾åƒåœ¾ï¼Œå®å¯æ”¾è¿‡ä¸å¯è¯¯ä¼¤ï¼‰
        
        Returns:
            (æ˜¯å¦ä½è´¨é‡, åŸå› )
        """
        import re
        
        content = evt.get('content', '').strip()
        title = evt.get('title', '').strip()
        entities = evt.get('entities', [])
        
        # è§„åˆ™1ï¼šç©ºå†…å®¹
        if not content or not title:
            return True, "å†…å®¹æˆ–æ ‡é¢˜ä¸ºç©º"
        
        # è§„åˆ™2ï¼šçº¯é“¾æ¥ï¼ˆæ— å®è´¨æ–‡å­—ï¼‰
        text_only = re.sub(r'https?://\S+', '', content).strip()
        if len(text_only) < 5:
            return True, "çº¯é“¾æ¥"
        
        # è§„åˆ™3ï¼šç»„åˆå¹¿å‘Šç‰¹å¾ï¼ˆéœ€â‰¥2ä¸ªä¿¡å·ï¼‰
        ad_signals = 0
        ad_keywords = ['æ‰«ç ', 'åŠ ç¾¤', 'ä¼˜æƒ åˆ¸', 'ç‚¹å‡»é¢†å–', 'é™æ—¶å…è´¹']
        if any(kw in content for kw in ad_keywords):
            ad_signals += 1
        if re.search(r'(å¾®ä¿¡|QQ)[:ï¼š]?\s*\w+', content):
            ad_signals += 1
        if not entities and len(content) < 20:
            ad_signals += 1
        if ad_signals >= 2:
            return True, "ç–‘ä¼¼å¹¿å‘Š"
        
        # è§„åˆ™4ï¼šä¹±ç ï¼ˆæ§åˆ¶å­—ç¬¦ï¼‰
        if re.search(r'[\x00-\x1f\x7f-\x9f]{3,}', content):
            return True, "ä¹±ç "
        
        return False, ""
    
    def _build_value_raw(self, name: str, value: str, unit: str, value_type: str) -> str:
        """
        æ„å»ºå®Œæ•´çš„ value_rawï¼ˆäººç±»å¯è¯»çš„å®Œæ•´è¡¨è¿°ï¼‰
        
        è§„åˆ™ï¼š
        1. text ç±»å‹æˆ–æ—  value â†’ ç›´æ¥è¿”å› name
        2. datetimeï¼šæ£€æŸ¥ name æ˜¯å¦å·²åŒ…å«æ—¥æœŸæ ¼å¼
        3. æ•°å€¼ç±»å‹ï¼šæ£€æŸ¥ name æ˜¯å¦å·²åŒ…å«é˜¿æ‹‰ä¼¯æ•°å­—
        """
        import re
        
        name = name or ''
        
        # 1. text ç±»å‹æˆ–æ—  value â†’ ç›´æ¥è¿”å› name
        if value_type == 'text' or not value:
            return name
        
        # 2. datetime ç±»å‹
        if value_type == 'datetime':
            # å¦‚æœ name å·²åŒ…å«æ—¥æœŸ/æ—¶é—´æ ¼å¼ â†’ ä¸æ‹¼æ¥
            if re.search(r'(\d{4}å¹´|\d{1,2}æœˆ|\d{1,2}æ—¥|\d{1,2}[:ï¼šç‚¹]|\d{4}-)', name):
                return name
            # å¦åˆ™æ‹¼æ¥
            return name + value
        
        # 3. æ£€æŸ¥ name æ˜¯å¦å·²åŒ…å«é˜¿æ‹‰ä¼¯æ•°å­—
        if re.search(r'\d', name):
            # name å·²æœ‰æ•°å­— â†’ åªæ£€æŸ¥æ˜¯å¦éœ€è¦è¡¥ unit
            if unit and unit not in name:
                return name + unit
            return name
        
        # 4. name æ˜¯çº¯è¯­ä¹‰åç§° â†’ æ‹¼æ¥ value + unit
        result = name + value
        if unit and unit not in result:
            result += unit
        
        return result
    
    def _parse_entity_value(
        self,
        entity_data: Dict,
        entity_type
    ) -> Dict[str, Any]:
        """
        è§£æå®ä½“å€¼ç±»å‹ï¼ˆLLMä¼˜å…ˆ + ä»£ç å…œåº•ï¼‰
        
        è§„åˆ™ï¼šLLM æœ‰æ•ˆ â†’ ç”¨ LLMï¼Œå¦åˆ™ â†’ ä»£ç å…œåº•
        """
        from decimal import Decimal
        from datetime import datetime as dt
        
        name = entity_data['name']
        llm_type = entity_data.get('value_type')
        llm_value = entity_data.get('value')
        llm_unit = entity_data.get('unit')
        
        valid_types = ('text', 'int', 'float', 'datetime', 'bool')
        
        # ========== LLM æœ‰æ•ˆ â†’ ä½¿ç”¨ ==========
        if llm_type and llm_type in valid_types:
            # æ„å»ºå®Œæ•´çš„ value_raw
            value_raw = self._build_value_raw(name, llm_value, llm_unit, llm_type)
            
            fields = {
                "value_type": llm_type,
                "value_raw": value_raw,
                "int_value": None,
                "float_value": None,
                "datetime_value": None,
                "bool_value": None,
                "enum_value": None,
                "value_unit": llm_unit,
                "value_confidence": Decimal("0.90")
            }
            
            try:
                if llm_type == "int" and llm_value:
                    fields["int_value"] = int(float(llm_value.replace(',', '')))
                elif llm_type == "float" and llm_value:
                    fields["float_value"] = Decimal(llm_value.replace(',', ''))
                elif llm_type == "datetime" and llm_value:
                    for fmt in ["%Y-%m-%d %H:%M:%S", "%Y-%m-%d", "%Y%m%d", "%Y/%m/%d", "%Yå¹´%mæœˆ%dæ—¥"]:
                        try:
                            fields["datetime_value"] = dt.strptime(llm_value, fmt)
                            break
                        except ValueError:
                            continue
                elif llm_type == "bool" and llm_value:
                    fields["bool_value"] = llm_value.lower() in ("true", "æ˜¯", "1")
            except Exception:
                pass
            
            fields["_source"] = "llm"
            return fields
        
        # ========== ä»£ç å…œåº• ==========
        value_constraints = getattr(entity_type, 'value_constraints', None)
        result = EntityValueParser().parse_to_typed_fields(
            name,
            entity_type=entity_data['type'],
            value_constraints=value_constraints
        )
        result["_source"] = "code"
        return result
    
    async def _get_or_create_entity(
        self,
        entity_data: Dict,
        config: ExtractConfig
    ) -> Optional[Entity]:
        """
        æŸ¥æ‰¾æˆ–åˆ›å»ºå®ä½“ï¼ˆå»é‡ï¼‰
        
        æ”¯æŒé«˜å¹¶å‘åœºæ™¯ï¼Œå¤„ç†ä»¥ä¸‹å¼‚å¸¸ï¼š
        - IntegrityError: å”¯ä¸€çº¦æŸå†²çªï¼Œé‡æ–°æŸ¥è¯¢å·²å­˜åœ¨çš„å®ä½“
        - OperationalError (1205): é”ç­‰å¾…è¶…æ—¶ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•
        
        Returns:
            Entityå¯¹è±¡ï¼Œå¦‚æœå®ä½“ç±»å‹æ— æ•ˆæˆ–åç§°æ— æ•ˆåˆ™è¿”å›None
        """
        # è¿‡æ»¤å•å­—ç¬¦å®ä½“ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å· ã€Œã€ç­‰ï¼‰
        entity_name = entity_data.get('name', '').strip()
        if len(entity_name) <= 1:
            return None
        
        normalized_name = entity_name.lower()
        
        # ğŸ†• é‡è¯•é…ç½®ï¼šå¤„ç†é”ç­‰å¾…è¶…æ—¶ (MySQL error 1205)
        max_retries = 3
        base_delay = 0.1  # åŸºç¡€å»¶è¿Ÿ 100ms
        
        for attempt in range(max_retries):
            try:
                return await self._get_or_create_entity_inner(
                    entity_data, config, normalized_name
                )
            except OperationalError as e:
                # æ£€æŸ¥æ˜¯å¦ä¸ºé”ç­‰å¾…è¶…æ—¶ (MySQL error 1205) æˆ–æ­»é” (MySQL error 1213)
                error_str = str(e)
                is_lock_timeout = "1205" in error_str or "Lock wait timeout" in error_str
                is_deadlock = "1213" in error_str or "Deadlock" in error_str
                
                if is_lock_timeout or is_deadlock:
                    error_type = "æ­»é”" if is_deadlock else "é”ç­‰å¾…è¶…æ—¶"
                    if attempt < max_retries - 1:
                        delay = base_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿: 0.1s, 0.2s, 0.4s
                        self.logger.warning(
                            f"ğŸ”„ å®ä½“{error_type}ï¼Œ{delay:.1f}s åé‡è¯• ({attempt + 1}/{max_retries}): "
                            f"{entity_data['name']}"
                        )
                        await asyncio.sleep(delay)
                        continue
                    else:
                        self.logger.error(
                            f"âŒ å®ä½“{error_type}ï¼Œé‡è¯•{max_retries}æ¬¡åä»å¤±è´¥: {entity_data['name']}"
                        )
                # éé”è¶…æ—¶/æ­»é”é”™è¯¯æˆ–é‡è¯•æ¬¡æ•°ç”¨å°½ï¼ŒæŠ›å‡ºå¼‚å¸¸
                raise
        
        # ä¸åº”è¯¥åˆ°è¿™é‡Œ
        return None
    
    async def _get_or_create_entity_inner(
        self,
        entity_data: Dict,
        config: ExtractConfig,
        normalized_name: str
    ) -> Optional[Entity]:
        """
        å®é™…æ‰§è¡Œå®ä½“æŸ¥æ‰¾æˆ–åˆ›å»ºçš„å†…éƒ¨æ–¹æ³•
        
        åˆ†ç¦»å‡ºæ¥ä»¥æ”¯æŒå¤–å±‚é‡è¯•é€»è¾‘
        """
        async with self.session_factory() as session:
            # æŸ¥æ‰¾å·²å­˜åœ¨
            existing_result = await session.execute(
                select(Entity)
                .where(Entity.source_config_id == config.source_config_id)
                .where(Entity.type == entity_data['type'])
                .where(Entity.normalized_name == normalized_name)
            )
            entity = existing_result.scalar_one_or_none()
            
            if entity:
                return entity
            
            # åˆ›å»ºæ–°å®ä½“ - è·å–entity_type_id
            entity_type_result = await session.execute(
                select(DBEntityType)
                .where(DBEntityType.type == entity_data['type'])
                .where(
                    (DBEntityType.source_config_id == config.source_config_id) |
                    (DBEntityType.is_default == True)
                )
                .where(DBEntityType.is_active == True)
            )
            entity_type = entity_type_result.scalar_one_or_none()
            
            if not entity_type:
                # âœ… è®°å½•è­¦å‘Šä½†ä¸æŠ›å¼‚å¸¸ï¼Œè¿”å› None
                self.logger.warning(
                    f"è·³è¿‡æ— æ•ˆå®ä½“ç±»å‹: type={entity_data['type']}, "
                    f"name={entity_data.get('name', 'N/A')}"
                )
                return None
            
            entity = Entity(
                id=str(uuid.uuid4()),
                source_config_id=config.source_config_id,
                entity_type_id=entity_type.id,
                type=entity_data['type'],
                name=entity_data['name'],
                normalized_name=normalized_name,
                description=None  # è§’è‰²æè¿°ä¿å­˜åœ¨ EventEntity.descriptionï¼Œä¸ä¿å­˜åœ¨ Entity.description
            )

            # ğŸ†• è§£æç±»å‹åŒ–å€¼ï¼ˆLLMä¼˜å…ˆ + ä»£ç å…œåº•ï¼‰
            typed_fields = self._parse_entity_value(entity_data, entity_type)
            if typed_fields:
                source = typed_fields.pop("_source", "code")
                entity.value_type = typed_fields.get("value_type")
                entity.value_raw = typed_fields.get("value_raw")
                entity.int_value = typed_fields.get("int_value")
                entity.float_value = typed_fields.get("float_value")
                entity.datetime_value = typed_fields.get("datetime_value")
                entity.bool_value = typed_fields.get("bool_value")
                entity.enum_value = typed_fields.get("enum_value")
                entity.value_unit = typed_fields.get("value_unit")
                entity.value_confidence = typed_fields.get("value_confidence")
                
                # åªè®°å½•étextç±»å‹ï¼ˆæœ‰æ„ä¹‰çš„ç±»å‹è§£æï¼‰
                if entity.value_type and entity.value_type != "text":
                    unit_str = f", unit={entity.value_unit}" if entity.value_unit else ""
                    self.logger.info(
                        f"ğŸ“Œ å®ä½“å€¼: {entity_data['name'][:15]} â†’ {entity.value_type}({source}){unit_str}"
                    )

            try:
                session.add(entity)
                await session.commit()
                await session.refresh(entity)
                return entity

            except IntegrityError as e:
                # å¹¶å‘å†²çªï¼šå¦ä¸€ä¸ªè¿›ç¨‹/åç¨‹å·²åˆ›å»ºç›¸åŒå®ä½“
                await session.rollback()
                self.logger.debug(
                    f"ğŸ”„ å®ä½“å¹¶å‘å†²çªï¼Œé‡æ–°æŸ¥è¯¢: {entity_data['name']}, error={e}"
                )

                # é‡æ–°æŸ¥è¯¢å·²å­˜åœ¨çš„å®ä½“
                retry_result = await session.execute(
                    select(Entity)
                    .where(Entity.source_config_id == config.source_config_id)
                    .where(Entity.type == entity_data['type'])
                    .where(Entity.normalized_name == normalized_name)
                )
                existing_entity = retry_result.scalar_one_or_none()

                if existing_entity:
                    return existing_entity

                # ç†è®ºä¸Šä¸åº”è¯¥åˆ°è¿™é‡Œ
                self.logger.error(f"âŒ å®ä½“å†²çªåé‡æ–°æŸ¥è¯¢å¤±è´¥: {entity_data['name']}")
                raise
    
    async def _reload_events_with_relations(self, event_ids: List[str]) -> List[SourceEvent]:
        """
        é‡æ–°ä»æ•°æ®åº“åŠ è½½äº‹é¡¹åˆ—è¡¨ï¼ˆé¢„åŠ è½½å…³ç³»æ•°æ®ï¼‰
        
        è§£å†³è·¨ session é—®é¢˜ï¼šä¿å­˜åé‡æ–°æŸ¥è¯¢ï¼Œç¡®ä¿æ‰€æœ‰å…³ç³»æ­£ç¡®åŠ è½½
        
        Args:
            event_ids: äº‹é¡¹IDåˆ—è¡¨
            
        Returns:
            åŒ…å«å®Œæ•´å…³ç³»çš„äº‹é¡¹åˆ—è¡¨
        """
        if not event_ids:
            return []
        
        async with self.session_factory() as session:
            result = await session.execute(
                select(SourceEvent)
                .where(SourceEvent.id.in_(event_ids))
                .options(
                    selectinload(SourceEvent.event_associations).selectinload(EventEntity.entity)
                )
            )
            events = list(result.scalars().all())
            
            # è®¾ç½® expire_on_commit=Falseï¼Œç¡®ä¿æ•°æ®åœ¨ session å¤–å¯è®¿é—®
            session.expire_on_commit = False
            
            # è§¦å‘å…³ç³»æ•°æ®åŠ è½½ï¼ˆç¡®ä¿æ‰€æœ‰å­—æ®µåœ¨ session å¤–å¯è®¿é—®ï¼‰
            for event in events:
                # è§¦å‘äº‹é¡¹å­—æ®µåŠ è½½
                _ = event.title
                _ = event.created_time
                
                # è§¦å‘å…³è”å’Œå®ä½“åŠ è½½
                if hasattr(event, 'event_associations'):
                    for assoc in event.event_associations:
                        _ = assoc.id
                        if assoc.entity:
                            _ = assoc.entity.name
                            _ = assoc.entity.type
            
            self.logger.debug(f"é‡æ–°åŠ è½½äº† {len(events)} ä¸ªäº‹é¡¹ï¼ˆåŒ…å«å®Œæ•´å…³ç³»ï¼‰")
            return events
    
    async def _fallback_extract_tags(
        self,
        references: List[str],
        source_type: str,
        existing_entity_names: set
    ) -> List[Dict]:
        """
        åˆ†è¯å™¨å…œåº•ï¼šä»åŸæ–‡æå– tags å®ä½“
        
        Args:
            references: åŸæ–‡å¼•ç”¨IDåˆ—è¡¨
            source_type: ARTICLE æˆ– CHAT
            existing_entity_names: å·²æå–çš„å®ä½“åç§°é›†åˆï¼ˆå°å†™ï¼‰
        
        Returns:
            è¡¥å……çš„ tags å®ä½“åˆ—è¡¨
        """
        if not references:
            return []
        
        try:
            from dataflow.db import ArticleSection, ChatMessage
            from dataflow.core.ai.tokensize import get_keyword_extractor, POS
            
            # 1. åŠ è½½åŸæ–‡
            async with self.session_factory() as session:
                if source_type == "ARTICLE":
                    result = await session.execute(
                        select(ArticleSection.content).where(ArticleSection.id.in_(references))
                    )
                else:
                    result = await session.execute(
                        select(ChatMessage.content).where(ChatMessage.id.in_(references))
                    )
                contents = [row[0] for row in result.all() if row[0]]
            
            if not contents:
                return []
            
            original_text = " ".join(contents)
            
            # 2. åˆ†è¯æå– - é™åˆ¶ä¸ºä¸“æœ‰åè¯ï¼ˆäººåã€åœ°åã€æœºæ„åã€äº§å“åï¼‰
            proper_noun_pos = {POS.PERSON, POS.PLACE, POS.ORG, POS.PRODUCT}
            keywords = get_keyword_extractor().extract(
                original_text, 
                top_k=20,
                pos=proper_noun_pos
            )
            
            if not keywords:
                return []
            
            # 3. å»é‡ï¼ˆåªæ£€æŸ¥å®Œå…¨åŒ¹é…ï¼Œé¿å…è¯¯åˆ¤ï¼‰
            tags = []
            skipped = []
            
            for kw in keywords:
                kw_lower = kw.lower()
                is_duplicate = kw_lower in existing_entity_names
                
                if is_duplicate:
                    skipped.append(kw)
                else:
                    tags.append({
                        "type": "tags",
                        "name": kw,
                        "description": "å…³é”®è¯",
                        "value_type": "text"  # æ˜ç¡®æŒ‡å®šï¼Œé¿å…ä»£ç è¯¯åˆ¤
                    })
            
            # 4. æ—¥å¿—
            if tags or skipped:
                self.logger.info(
                    f"ğŸ“Œ Tagså…œåº•: åˆ†è¯={len(keywords)}, æ–°å¢={len(tags)}, å»é‡={len(skipped)}"
                )
            if tags:
                self.logger.info(f"   âœ… æ–°å¢: {[t['name'] for t in tags]}")
            
            return tags
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Tagså…œåº•å¤±è´¥: {e}")
            return []

    async def _fallback_extract_entities_with_llm(
        self,
        references: List[str],
        source_type: str,
        existing_entity_names: set,
        entity_types: List[Dict]
    ) -> List[Dict]:
        """
        LLMå…œåº•ï¼šä»åŸæ–‡æå–å®ä½“å¹¶æ­£ç¡®åˆ†ç±»ï¼ˆæ¯”åˆ†è¯å™¨æ›´ç²¾å‡†ï¼‰
        
        Args:
            references: åŸæ–‡å¼•ç”¨IDåˆ—è¡¨
            source_type: ARTICLE æˆ– CHAT
            existing_entity_names: å·²æå–çš„å®ä½“åç§°é›†åˆï¼ˆå°å†™ï¼‰
            entity_types: å®ä½“ç±»å‹å®šä¹‰åˆ—è¡¨
        
        Returns:
            è¡¥å……çš„å®ä½“åˆ—è¡¨ï¼ˆå¸¦æ­£ç¡®åˆ†ç±»ï¼‰
        """
        if not references:
            return []
        
        try:
            from dataflow.db import ArticleSection, ChatMessage
            from dataflow.core.ai.factory import create_llm_client
            
            # 1. åŠ è½½åŸæ–‡
            async with self.session_factory() as session:
                if source_type == "ARTICLE":
                    result = await session.execute(
                        select(ArticleSection.content).where(ArticleSection.id.in_(references))
                    )
                else:
                    result = await session.execute(
                        select(ChatMessage.content).where(ChatMessage.id.in_(references))
                    )
                contents = [row[0] for row in result.all() if row[0]]
            
            if not contents:
                return []
            
            original_text = " ".join(contents)
            
            # 2. æ„å»ºå®ä½“ç±»å‹æè¿°ï¼ˆä¸é¦–æ¬¡æå–ä¸€è‡´ï¼‰
            type_descriptions = []
            valid_types = []
            for et in entity_types:
                if et.get('type') != 'tags':  # æ’é™¤ tags ç±»å‹
                    desc = et.get('description', et['name'])
                    type_descriptions.append(f"- **{et['type']}** ({et['name']}): {desc}")
                    valid_types.append(et['type'])
            
            # æ—¥å¿—ï¼šæ˜¾ç¤ºå¯ç”¨ç±»å‹
            self.logger.debug(f"ğŸ¤– LLMå…œåº•å¯ç”¨ç±»å‹: {valid_types}")
            
            # 3. æ„å»º LLM æç¤ºè¯
            # å°†å·²æå–çš„å®ä½“åˆ—è¡¨å…¨éƒ¨ä¼ ç»™ LLMï¼ˆæœ€å¤š50ä¸ªï¼‰
            existing_list = sorted(list(existing_entity_names))[:50]
            
            prompt = f"""You are an entity extraction expert. Your goal is to ensure no important clues are missed.

## Purpose
Entities are **answers or clues leading to answers**.
When users ask questions, these entities help find the relevant documents.
The initial extraction may have missed some critical clues. Your job is to fill the gaps.

## Available Entity Types:
{chr(10).join(type_descriptions)}

## Text:
{original_text[:3000]}

## Already Extracted (DO NOT repeat):
{existing_list}

## What to Extract

**Critical clues often missed:**
- âœ… Proper nouns: Names that could be answers (people, places, organizations)
- âœ… Abbreviations & nicknames: Alternative ways users might ask
- âœ… Specific identifiers: Team names, product names, work titles
- âœ… Unique terms: Specialized vocabulary that distinguishes this document

**Skip generic terms (not useful as clues):**
- âŒ Category words: "university", "team", "airport" (without specific name)
- âŒ Common adjectives: "famous", "important", "retired"

**Ask yourself**: 
"If someone asks a question where THIS is the answer, would we find this document?"
- "Who coached VCU in 2011?" â†’ "Shaka Smart" âœ… (must extract)
- "What team?" â†’ "basketball team" âŒ (too generic)

## Output (JSON):
{{"entities": [{{"name": "Entity Name", "type": "type_id"}}]}}
"""
            
            # 4. è°ƒç”¨ LLM
            llm_client = await create_llm_client(scenario='extract')
            from dataflow.core.ai.models import LLMMessage, LLMRole
            
            result = await llm_client.chat_with_schema(
                [LLMMessage(role=LLMRole.USER, content=prompt)],
                response_schema={
                    "type": "object",
                    "properties": {
                        "entities": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "name": {"type": "string"},
                                    "type": {"type": "string"}
                                },
                                "required": ["name", "type"]
                            }
                        }
                    },
                    "required": ["entities"]
                },
                temperature=0.1
            )
            
            if not result or "entities" not in result:
                return []
            
            # 5. è¿‡æ»¤å’Œå»é‡
            entities = []
            skipped_duplicates = []
            llm_returned = result.get("entities", [])
            
            for e in llm_returned:
                name = e.get("name", "").strip()
                etype = e.get("type", "").strip()
                
                if not name or not etype:
                    continue
                
                # ç±»å‹æ ¡éªŒ
                if etype not in valid_types:
                    etype = "tags"  # æ— æ•ˆç±»å‹é™çº§ä¸º tags
                
                # å»é‡ï¼šåªæ£€æŸ¥å®Œå…¨åŒ¹é…ï¼ˆé¿å… "VCU Rams" è¢« "Rams" è¯¯åˆ¤ä¸ºé‡å¤ï¼‰
                name_lower = name.lower()
                is_duplicate = name_lower in existing_entity_names
                
                if is_duplicate:
                    skipped_duplicates.append(name)
                    continue
                
                entities.append({
                    "type": etype,
                    "name": name,
                    "description": "",
                    "value_type": "text"
                })
                existing_entity_names.add(name_lower)
            
            # è¯¦ç»†æ—¥å¿—
            self.logger.info(
                f"ğŸ¤– LLMå…œåº•: è¿”å›={len(llm_returned)}, å»é‡={len(skipped_duplicates)}, æ–°å¢={len(entities)}"
            )
            if skipped_duplicates:
                self.logger.debug(f"   å»é‡è·³è¿‡: {skipped_duplicates[:5]}")
            if entities:
                entity_preview = ', '.join(
                    f"{e['type']}:{e['name'][:15]}" for e in entities[:5]
                )
                self.logger.info(f"   âœ… æ–°å¢: {entity_preview}{'...' if len(entities) > 5 else ''}")
            
            return entities
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ LLMå…œåº•å¤±è´¥: {e}")
            # è¿”å›ç©ºï¼Œç”±ä¸»æµç¨‹çš„åˆ†è¯å™¨è¡¥å……å¤„ç†
            return []

    async def _update_source_status(
        self,
        chunks: List[SourceChunk],
        status: str,
        error: Optional[str] = None
    ) -> None:
        """
        æ›´æ–°æºçŠ¶æ€ï¼ˆArticle æˆ– ChatConversationï¼‰
        
        Args:
            chunks: chunkåˆ—è¡¨ï¼ˆç”¨äºç¡®å®šæºç±»å‹å’ŒIDï¼‰
            status: çŠ¶æ€å€¼ï¼ˆPENDING/PROCESSING/COMPLETED/FAILEDï¼‰
            error: é”™è¯¯ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œå¤±è´¥æ—¶æä¾›ï¼‰
        """
        if not chunks:
            return
        
        from dataflow.db import Article, ChatConversation
        
        # ç¡®å®šæºç±»å‹å’ŒIDï¼ˆåŒä¸€æ‰¹chunksåº”è¯¥æ¥è‡ªåŒä¸€ä¸ªæºï¼‰
        source_type = chunks[0].source_type
        source_id = chunks[0].source_id
        
        # éªŒè¯æ‰€æœ‰chunksæ¥è‡ªåŒä¸€ä¸ªæº
        if not all(c.source_type == source_type and c.source_id == source_id for c in chunks):
            self.logger.warning("Chunksæ¥è‡ªä¸åŒçš„æºï¼Œæ— æ³•ç»Ÿä¸€æ›´æ–°çŠ¶æ€")
            return
        
        async with self.session_factory() as session:
            try:
                if source_type == "ARTICLE":
                    result = await session.execute(
                        select(Article).where(Article.id == source_id)
                    )
                    source = result.scalar_one_or_none()
                    
                    if source:
                        source.status = status
                        if error:
                            source.error = error
                        await session.commit()
                        self.logger.info(f"âœ… å·²æ›´æ–°æ–‡ç« çŠ¶æ€: {source_id} -> {status}")
                    else:
                        self.logger.warning(f"æ–‡ç« ä¸å­˜åœ¨: {source_id}")
                
                elif source_type == "CHAT":
                    result = await session.execute(
                        select(ChatConversation).where(ChatConversation.id == source_id)
                    )
                    source = result.scalar_one_or_none()
                    
                    if source:
                        # ChatConversation æ²¡æœ‰ status å­—æ®µï¼Œè®°å½•åˆ° extra_data
                        if source.extra_data is None:
                            source.extra_data = {}
                        source.extra_data["extract_status"] = status
                        if error:
                            source.extra_data["extract_error"] = error
                        await session.commit()
                        self.logger.info(f"âœ… å·²æ›´æ–°ä¼šè¯æå–çŠ¶æ€: {source_id} -> {status}")
                    else:
                        self.logger.warning(f"ä¼šè¯ä¸å­˜åœ¨: {source_id}")
                
                else:
                    self.logger.warning(f"ä¸æ”¯æŒçš„æºç±»å‹: {source_type}")
            
            except Exception as e:
                self.logger.error(f"æ›´æ–°æºçŠ¶æ€å¤±è´¥: {e}", exc_info=True)
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…å½±å“ä¸»æµç¨‹
    