"""
äº‹é¡¹å¤„ç†å™¨

è´Ÿè´£ä»æ–‡ç« ç‰‡æ®µä¸­æå–äº‹é¡¹å’Œå®ä½“çš„æ ¸å¿ƒé€»è¾‘
"""

import uuid
from typing import Any, Dict, List, Optional

from sqlalchemy import or_, select

from sqlalchemy.orm import selectinload

from dataflow.core.ai.base import BaseLLMClient
from dataflow.core.ai.models import LLMMessage, LLMRole
from dataflow.core.prompt.manager import PromptManager
from dataflow.core.storage.elasticsearch import get_es_client
from dataflow.core.storage.repositories.event_repository import EventVectorRepository
from dataflow.db import get_session_factory
from dataflow.db.models import (
    SourceChunk,
    Entity,
    EntityType as DBEntityType,
    EventEntity,
    SourceEvent,
)
from dataflow.exceptions import ExtractError
from dataflow.modules.extract.config import ExtractConfig
from dataflow.modules.extract.parser import EntityValueParser
from dataflow.utils import get_logger

logger = get_logger("extract.processor")


class EventProcessor:
    """äº‹é¡¹å¤„ç†å™¨ï¼ˆæ ¸å¿ƒæå–é€»è¾‘ï¼‰"""

    def __init__(
        self,
        llm_client: BaseLLMClient,
        prompt_manager: PromptManager,
        config: ExtractConfig,
    ):
        """
        åˆå§‹åŒ–äº‹é¡¹å¤„ç†å™¨

        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            prompt_manager: æç¤ºè¯ç®¡ç†å™¨
            config: æå–é…ç½®
        """
        self.llm_client = llm_client
        self.prompt_manager = prompt_manager
        self.config = config
        self.session_factory = get_session_factory()
        
        # å†å²äº‹é¡¹å¬å›ç›¸å…³ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._es_client = None
        self._event_repo = None
        self._embedding_client = None
        self.entity_types: List[DBEntityType] = []
        self.logger = get_logger("extract.processor")
        self.parser = EntityValueParser()

    async def extract(
        self,
        items: List,
        metadata: Dict,
        source_type: str,
    ) -> List[SourceEvent]:
        """
        ä»å†…å®¹åˆ—è¡¨æå–äº‹é¡¹ï¼ˆchunkçº§æå–å…¥å£ï¼‰
        
        Args:
            items: ArticleSection æˆ– ChatMessage åˆ—è¡¨
            metadata: {document_title, chunk_title, previous_context}
            source_type: "ARTICLE" æˆ– "CHAT"
        
        Returns:
            SourceEvent åˆ—è¡¨
        """
        import json
        
        if not items:
            self.logger.warning("items ä¸ºç©ºï¼Œè·³è¿‡æå–")
            return []
        
        try:
            is_article = source_type == "ARTICLE"
            
            # 1. æ„å»º SYSTEM æç¤ºè¯ï¼ˆåŒ…å«èƒŒæ™¯ã€å®ä½“ç±»å‹ã€å€™é€‰å…³é”®è¯ã€è¾“å‡ºschemaï¼‰
            system_prompt = await self._build_system_prompt(items, metadata, is_article)
            self.logger.debug(f"SYSTEM: {system_prompt[:500]}...")
            
            # 2. æ„å»º USER è¾“å…¥ï¼ˆçº¯æ•°æ®ï¼‰
            user_input = self._build_user_input(items, is_article)
            self.logger.debug(f"USER: {json.dumps(user_input, ensure_ascii=False)[:300]}...")
            
            # 3. æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [
                LLMMessage(role=LLMRole.SYSTEM, content=system_prompt),
                LLMMessage(role=LLMRole.USER, content=json.dumps(user_input, ensure_ascii=False))
            ]
            self.logger.info(f"å¼€å§‹æå–: items={len(items)}, type={source_type}")
            
            # 4. è°ƒç”¨ LLMï¼ˆä¼ å…¥ schema ç”¨äºæ ¡éªŒï¼‰
            schema = self._build_extraction_schema()
            result = await self.llm_client.chat_with_schema(
                messages,
                response_schema=schema,
                temperature=0.3
            )
            
            # 5. è§£æç»“æœ
            events = self._parse_result(result, items, source_type)
            
            self.logger.info(f"æå–å®Œæˆ: {len(events)} ä¸ªäº‹é¡¹")
            return events
            
        except Exception as e:
            self.logger.error(f"æå–å¤±è´¥: {e}", exc_info=True)
            raise ExtractError(f"æå–å¤±è´¥: {e}") from e
    
    def _parse_result(self, result: Dict, items: List, source_type: str) -> List[SourceEvent]:
        """è§£æç»“æœä¸º SourceEvent"""
        from datetime import datetime
        
        events = []
        id_map = {item.id: item for item in items}
        
        # è·å– source_idï¼ˆä»ç¬¬ä¸€ä¸ª item è·å–ï¼‰
        first_item = items[0] if items else None
        source_id = None
        article_id = None
        conversation_id = None
        
        if first_item:
            if source_type == "ARTICLE":
                # ArticleSection æœ‰ article_id å±æ€§
                article_id = getattr(first_item, "article_id", None)
                source_id = article_id
            elif source_type == "CHAT":
                # ChatMessage æœ‰ conversation_id å±æ€§
                conversation_id = getattr(first_item, "conversation_id", None)
                source_id = conversation_id
        
        for event_data in result.get("items", []):
            references = event_data.get("references", [])
            valid_refs = [ref for ref in references if ref in id_map]
            
            if not valid_refs and references:
                self.logger.warning(f"äº‹é¡¹å¼•ç”¨æ— æ•ˆ: {references}")
                continue
            
            # è½¬æ¢å®ä½“æ ¼å¼ï¼šä»åˆ—è¡¨ -> å­—å…¸
            # LLMè¿”å›: [{"type": "person", "name": "å¼ ä¸‰", "description": "CEO"}, ...]
            # æœŸæœ›æ ¼å¼: {"person": [{"name": "å¼ ä¸‰", "description": "CEO"}], ...}
            raw_entities_list = event_data.get("entities", [])
            raw_entities_dict = {}
            if isinstance(raw_entities_list, list):
                for entity in raw_entities_list:
                    if isinstance(entity, dict):
                        entity_type = entity.get("type", "")
                        if entity_type:
                            if entity_type not in raw_entities_dict:
                                raw_entities_dict[entity_type] = []
                            raw_entities_dict[entity_type].append({
                                "name": entity.get("name", ""),
                                "description": entity.get("description", "")
                            })
            elif isinstance(raw_entities_list, dict):
                # å·²ç»æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                raw_entities_dict = raw_entities_list
            
            # æ ¹æ®æ¥æºç±»å‹è®¾ç½®æ—¶é—´
            start_time = None
            end_time = None
            
            if source_type == "ARTICLE":
                # æ–‡æ¡£ç±»å‹ï¼šä½¿ç”¨å½“å‰æ—¶é—´
                current_time = datetime.now()
                start_time = current_time
                end_time = current_time
            elif source_type == "CHAT":
                # ä¼šè¯ç±»å‹ï¼šä»å¼•ç”¨çš„æ¶ˆæ¯ä¸­è·å–æ—¶é—´èŒƒå›´
                ref_items = [id_map[ref] for ref in valid_refs if ref in id_map]
                if ref_items:
                    timestamps = [getattr(item, 'timestamp', None) for item in ref_items]
                    timestamps = [t for t in timestamps if t is not None]
                    if timestamps:
                        start_time = min(timestamps)
                        end_time = max(timestamps)
            
            event = SourceEvent(
                id=str(uuid.uuid4()),
                source_config_id=self.config.source_config_id,
                source_type=source_type,
                source_id=source_id or "",
                article_id=article_id,
                conversation_id=conversation_id,
                title=event_data.get("title", ""),
                summary=event_data.get("summary", ""),
                content=event_data.get("content", ""),
                category=event_data.get("category", ""),
                type=source_type,  # ä¸šåŠ¡å­—æ®µï¼Œä¸ source_type ä¿æŒä¸€è‡´
                references=valid_refs or references,  # ç›´æ¥è®¾ç½®åˆ° references å­—æ®µ
                start_time=start_time,
                end_time=end_time,
                extra_data={
                    "raw_entities": raw_entities_dict,
                    "is_valid": event_data.get("is_valid", True)
                }
            )
            events.append(event)
        
        return events

    async def extract_from_sections(
        self, sections: List[SourceChunk], batch_index: int
    ) -> List[SourceEvent]:
        """
        ä»æ¥æºç‰‡æ®µæå–äº‹é¡¹ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰

        è¿™æ˜¯æœ€åº•å±‚çš„æå–é€»è¾‘ï¼Œå•æ¬¡LLMè°ƒç”¨

        Args:
            sections: æ¥æºç‰‡æ®µåˆ—è¡¨
            batch_index: æ‰¹æ¬¡ç´¢å¼•ï¼ˆç”¨äºæ—¥å¿—ï¼‰

        Returns:
            æå–çš„äº‹é¡¹åˆ—è¡¨

        Raises:
            ExtractError: æå–å¤±è´¥
        """
        import json
        
        # è¾“å…¥éªŒè¯
        if not sections:
            self.logger.warning(f"æ‰¹æ¬¡ {batch_index}: sections åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡æå–")
            return []

        try:
            # 1. æ„å»º SYSTEM æç¤ºè¯
            metadata = {"document_title": "", "chunk_title": f"æ‰¹æ¬¡{batch_index}", "previous_context": ""}
            system_prompt = await self._build_system_prompt(sections, metadata, is_article=True)

            # 2. æ„å»º USER è¾“å…¥
            user_input = self._build_user_input(sections, is_article=True)

            # 3. æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [
                LLMMessage(role=LLMRole.SYSTEM, content=system_prompt),
                LLMMessage(role=LLMRole.USER, content=json.dumps(user_input, ensure_ascii=False))
            ]

            # 4. æ„å»ºJSON Schema
            schema = self._build_extraction_schema()

            # 5. è°ƒç”¨LLM
            result = await self.llm_client.chat_with_schema(
                messages, response_schema=schema, temperature=0.3
            )

            # 6. è§£æç»“æœ -> SourceEvent å¯¹è±¡ï¼ˆä¸å«å®ä½“ï¼Œç»Ÿä¸€ç”± process_entity_associations å¤„ç†ï¼‰
            events = await self._parse_extraction_result_without_entities(result, sections)

            self.logger.info(
                f"æ‰¹æ¬¡ {batch_index}: æå–äº† {len(events)} ä¸ªäº‹é¡¹ï¼ˆä¸å«å®ä½“ï¼‰",
                extra={"batch_index": batch_index, "event_count": len(events)},
            )

            return events

        except Exception as e:
            self.logger.error(f"æ‰¹æ¬¡ {batch_index} æå–å¤±è´¥: {e}", exc_info=True)
            raise ExtractError(f"æ‰¹æ¬¡ {batch_index} æå–å¤±è´¥: {e}") from e

    async def extract_events_without_entities(
        self, sections: List[SourceChunk], batch_index: int
    ) -> List[SourceEvent]:
        """
        é˜¶æ®µ1ï¼šæå–äº‹é¡¹ï¼ˆä¸å«å®ä½“å…³è”ï¼‰

        Args:
            sections: æ¥æºç‰‡æ®µåˆ—è¡¨
            batch_index: æ‰¹æ¬¡ç´¢å¼•

        Returns:
            ä¸å«å®ä½“å…³è”çš„äº‹é¡¹åˆ—è¡¨
        """
        import json
        
        try:
            # 1. æ„å»º SYSTEM æç¤ºè¯
            metadata = {"document_title": "", "chunk_title": f"æ‰¹æ¬¡{batch_index}", "previous_context": ""}
            system_prompt = await self._build_system_prompt(sections, metadata, is_article=True)

            # 2. æ„å»º USER è¾“å…¥
            user_input = self._build_user_input(sections, is_article=True)

            # 3. æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [
                LLMMessage(role=LLMRole.SYSTEM, content=system_prompt),
                LLMMessage(role=LLMRole.USER, content=json.dumps(user_input, ensure_ascii=False))
            ]

            # 4. æ„å»ºJSON Schema
            schema = self._build_extraction_schema()

            self.logger.info(
                f"ğŸ“¦ æ‰¹æ¬¡ {batch_index}: å¼€å§‹æå–äº‹é¡¹ - ç‰‡æ®µæ•°={len(sections)}, "
                f"LLMæ¨¡å‹={self.llm_client.client.config.model}"
            )

            # 5. è°ƒç”¨LLM
            result = await self.llm_client.chat_with_schema(
                messages, response_schema=schema, temperature=0.3
            )

            # 6. è§£æç»“æœï¼ˆä¸å¤„ç†å®ä½“å…³è”ï¼‰
            events = await self._parse_extraction_result_without_entities(result, sections)

            self.logger.info(
                f"æ‰¹æ¬¡ {batch_index}: æå–äº† {len(events)} ä¸ªäº‹é¡¹ï¼ˆä¸å«å®ä½“ï¼‰",
                extra={"batch_index": batch_index, "event_count": len(events)},
            )

            return events

        except Exception as e:
            self.logger.error(
                f"âŒ æ‰¹æ¬¡ {batch_index} æå–å¤±è´¥ - æ¨¡å‹: {self.llm_client.client.config.model}, "
                f"ç‰‡æ®µæ•°: {len(sections)}, é”™è¯¯: {e}",
                exc_info=True
            )
            raise ExtractError(f"æ‰¹æ¬¡ {batch_index} æå–å¤±è´¥: {e}") from e

    async def process_entity_associations(
        self, events: List[SourceEvent], session=None  # noqa: ARG002
    ) -> List[SourceEvent]:
        """
        é˜¶æ®µ2ï¼šç»Ÿä¸€å¤„ç†æ‰€æœ‰äº‹é¡¹çš„å®ä½“å…³è”ï¼ˆä¸¤é˜¶æ®µå¤„ç†ä¼˜åŒ–ç‰ˆï¼‰
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„å®ä½“ï¼ˆå‡å°‘ SELECT æ¬¡æ•°ï¼‰
        2. é€ä¸ªåˆ›å»ºæ–°å®ä½“ï¼ˆç‹¬ç«‹äº‹åŠ¡ï¼Œå†²çªéš”ç¦»ï¼‰
        3. å»ºç«‹å…³è”
        
        æ³¨æ„ï¼šsession å‚æ•°ä¿ç•™æ˜¯ä¸ºäº†å‘åå…¼å®¹ï¼Œä½†å½“å‰å®ç°ä½¿ç”¨ç‹¬ç«‹äº‹åŠ¡åˆ›å»ºå®ä½“

        Args:
            events: æ‰€æœ‰äº‹é¡¹åˆ—è¡¨ï¼ˆä¸å«å®ä½“å…³è”ï¼‰
            session: æ•°æ®åº“ sessionï¼ˆä¿ç•™å‚æ•°ï¼Œå½“å‰æœªä½¿ç”¨ï¼‰

        Returns:
            åŒ…å«å®ä½“å…³è”çš„äº‹é¡¹åˆ—è¡¨
        """
        try:
            self.logger.info(f"å¼€å§‹ç»Ÿä¸€å¤„ç† {len(events)} ä¸ªäº‹é¡¹çš„å®ä½“å…³è”")

            # ========== é˜¶æ®µ1ï¼šæ”¶é›†æ‰€æœ‰éœ€è¦çš„å®ä½“ ==========
            # è‡ªåŠ¨æ³¨å…¥æ—¶é—´å®ä½“ï¼ˆåŸºäº start_time/end_time å­—æ®µï¼‰
            for event in events:
                self._inject_time_entities_for_event(event)

            # æ”¶é›†æ‰€æœ‰å®ä½“æ•°æ®ï¼š{entity_type: {name: description}}
            all_entities_data = {}

            # 1ï¸âƒ£ æ”¶é›† LLM æå–çš„å®ä½“
            for event in events:
                entities_data = event.extra_data.get("raw_entities", {})
                for entity_type, entity_names in entities_data.items():
                    if entity_type not in all_entities_data:
                        all_entities_data[entity_type] = {}

                    for entity_data in entity_names:
                        if isinstance(entity_data, dict):
                            name = entity_data.get("name")
                            description = entity_data.get("description", "")
                        else:
                            name = entity_data
                            description = ""

                        if name:
                            if name not in all_entities_data[entity_type]:
                                all_entities_data[entity_type][name] = description
                            elif description and not all_entities_data[entity_type][name]:
                                all_entities_data[entity_type][name] = description

            # 2ï¸âƒ£ æ·»åŠ é…ç½®çš„é»˜è®¤å€¼å®ä½“åˆ°æ”¶é›†æ± 
            for entity_type_config in self.entity_types:
                constraints = entity_type_config.value_constraints or {}
                default_value = constraints.get('default')
                if default_value:
                    entity_type = entity_type_config.type
                    if entity_type not in all_entities_data:
                        all_entities_data[entity_type] = {}
                    if default_value not in all_entities_data[entity_type]:
                        all_entities_data[entity_type][default_value] = "ç³»ç»Ÿé»˜è®¤å€¼"
                        self.logger.debug(f"ğŸ“Œ æ·»åŠ é»˜è®¤å€¼å®ä½“: {entity_type}={default_value}")

            # ========== é˜¶æ®µ2ï¼šæ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„å®ä½“ ==========
            entity_id_map = await self._batch_query_existing_entities(all_entities_data)

            # ========== é˜¶æ®µ3ï¼šé€ä¸ªåˆ›å»ºæ–°å®ä½“ï¼ˆç‹¬ç«‹äº‹åŠ¡ï¼‰ ==========
            entities_to_create = []
            for entity_type, entities_dict in all_entities_data.items():
                entity_type_obj = self._get_entity_type_by_type(entity_type)
                if not entity_type_obj:
                    continue
                
                for name, description in entities_dict.items():
                    normalized_name = self._normalize_entity_name(name)
                    key = (entity_type, normalized_name)
                    
                    if key not in entity_id_map:
                        entities_to_create.append((entity_type, name, description, entity_type_obj))

            if entities_to_create:
                self.logger.info(f"éœ€è¦åˆ›å»º {len(entities_to_create)} ä¸ªæ–°å®ä½“")
                
                for entity_type, name, description, entity_type_obj in entities_to_create:
                    try:
                        entity_id = await self._create_entity_with_retry(
                            entity_type, name, entity_type_obj
                        )
                        normalized_name = self._normalize_entity_name(name)
                        entity_id_map[(entity_type, normalized_name)] = (entity_id, description)
                    except Exception as e:
                        self.logger.error(f"âŒ åˆ›å»ºå®ä½“å¤±è´¥: {name}, error={e}")
                        # ç»§ç»­å¤„ç†å…¶ä»–å®ä½“ï¼Œä¸ä¸­æ–­

            # ========== é˜¶æ®µ4ï¼šå»ºç«‹å®ä½“å…³è” ==========
            # é¢„å…ˆæ”¶é›†æ‰€æœ‰å¼ºåˆ¶æ¨¡å¼çš„é»˜è®¤å€¼
            forced_defaults = {}
            for entity_type_config in self.entity_types:
                constraints = entity_type_config.value_constraints or {}
                default_value = constraints.get('default')
                override_mode = constraints.get('override', False)
                if default_value and override_mode:
                    forced_defaults[entity_type_config.type] = default_value

            # ä¸ºæ‰€æœ‰äº‹é¡¹å»ºç«‹å®ä½“å…³è”
            for event in events:
                entities_data = event.extra_data.get("raw_entities", {})
                event_associations = []
                entity_map = {}  # ç”¨äºå»é‡å’Œåˆå¹¶æè¿°

                # å»ºç«‹ LLM æå–çš„å®ä½“å…³è”
                for entity_type, entity_names in entities_data.items():
                    entity_type_obj = self._get_entity_type_by_type(entity_type)
                    if not entity_type_obj:
                        continue

                    for entity_data in entity_names:
                        if isinstance(entity_data, dict):
                            name = entity_data.get("name")
                            description = entity_data.get("description", "")
                        else:
                            name = entity_data
                            description = ""

                        if not name:
                            continue

                        normalized_name = self._normalize_entity_name(name)
                        key = (entity_type, normalized_name)
                        
                        if key in entity_id_map:
                            entity_id, cached_description = entity_id_map[key]
                            
                            if entity_id not in entity_map:
                                entity_map[entity_id] = {
                                    "name": name,
                                    "type": entity_type,
                                    "descriptions": [],
                                    "weight": float(entity_type_obj.weight),
                                    "is_forced_default": False
                                }
                            
                            if description and description not in entity_map[entity_id]["descriptions"]:
                                entity_map[entity_id]["descriptions"].append(description)
                            if cached_description and cached_description not in entity_map[entity_id]["descriptions"]:
                                entity_map[entity_id]["descriptions"].append(cached_description)
                            
                            if entity_type in forced_defaults and name == forced_defaults[entity_type]:
                                entity_map[entity_id]["is_forced_default"] = True

                # åº”ç”¨é»˜è®¤å€¼å®ä½“å…³è”é€»è¾‘
                extracted_by_type = {}
                for entity_type, entity_names in entities_data.items():
                    names = [e.get('name') if isinstance(e, dict) else e for e in entity_names]
                    extracted_by_type[entity_type] = [n for n in names if n]

                for entity_type_config in self.entity_types:
                    constraints = entity_type_config.value_constraints or {}
                    default_value = constraints.get('default')
                    override_mode = constraints.get('override', False)

                    if not default_value:
                        continue

                    entity_type = entity_type_config.type
                    entity_names_of_type = extracted_by_type.get(entity_type, [])
                    has_default = default_value in entity_names_of_type

                    should_add_default = (
                        (override_mode and not has_default) or
                        (not override_mode and len(entity_names_of_type) == 0)
                    )

                    if should_add_default:
                        normalized_name = self._normalize_entity_name(default_value)
                        key = (entity_type, normalized_name)
                        if key in entity_id_map:
                            entity_id, _ = entity_id_map[key]
                            
                            if entity_id not in entity_map:
                                mode_desc = "å¼ºåˆ¶è¿½åŠ " if override_mode else "è‡ªåŠ¨è¡¥å……"
                                entity_map[entity_id] = {
                                    "name": default_value,
                                    "type": entity_type,
                                    "descriptions": [f"ç³»ç»Ÿé»˜è®¤å€¼ï¼ˆ{mode_desc}ï¼‰"],
                                    "weight": float(entity_type_config.weight),
                                    "is_forced_default": False,
                                    "is_default": True,
                                    "mode": mode_desc
                                }

                # ä¸ºæ¯ä¸ªå”¯ä¸€çš„ entity_id åˆ›å»ºå…³è”
                for entity_id, info in entity_map.items():
                    if info.get("is_forced_default"):
                        final_description = "ç³»ç»Ÿé»˜è®¤å€¼ï¼ˆå¼ºåˆ¶å†™å…¥ï¼‰"
                    elif info.get("is_default"):
                        final_description = info["descriptions"][0] if info["descriptions"] else None
                    elif info["descriptions"]:
                        final_description = "ã€".join(info["descriptions"])
                    else:
                        final_description = None
                    
                    extra_data = {"confidence": event.extra_data.get("quality_score", 0.8)}
                    if info.get("is_forced_default"):
                        extra_data["is_forced_default"] = True
                    if info.get("is_default"):
                        extra_data["is_default"] = True
                        extra_data["mode"] = info.get("mode")
                    if len(info["descriptions"]) > 1:
                        extra_data["description_count"] = len(info["descriptions"])
                    
                    assoc = EventEntity(
                        id=str(uuid.uuid4()),
                        event_id=event.id,
                        entity_id=entity_id,
                        weight=info["weight"],
                        description=final_description,
                        extra_data=extra_data,
                    )
                    event_associations.append(assoc)

                event.event_associations = event_associations

                # æ¸…ç†ä¸´æ—¶æ•°æ®
                if "raw_entities" in event.extra_data:
                    del event.extra_data["raw_entities"]

            self.logger.info(f"âœ… å®Œæˆ {len(events)} ä¸ªäº‹é¡¹çš„å®ä½“å…³è”å¤„ç†")
            return events

        except Exception as e:
            self.logger.error(f"å®ä½“å…³è”å¤„ç†å¤±è´¥: {e}", exc_info=True)
            raise ExtractError(f"å®ä½“å…³è”å¤„ç†å¤±è´¥: {e}") from e

    async def _parse_extraction_result_without_entities(
        self, result: Dict[str, Any], sections: List[SourceChunk]
    ) -> List[SourceEvent]:
        """
        è§£æLLMæå–ç»“æœä¸ºSourceEventå¯¹è±¡ï¼ˆä¸å¤„ç†å®ä½“å…³è”ï¼‰

        Args:
            result: LLMè¿”å›çš„JSONç»“æœ
            sections: åŸå§‹ç‰‡æ®µåˆ—è¡¨ï¼ˆç”¨äºç”Ÿæˆå¼•ç”¨ï¼‰

        Returns:
            ä¸å«å®ä½“å…³è”çš„SourceEventå¯¹è±¡åˆ—è¡¨
        """
        events = []
        for event_data in result.get("items", []):
            # è§£æ LLM æ ‡æ³¨çš„å¼•ç”¨ï¼ˆç‰‡æ®µç¼–å·ï¼Œä»1å¼€å§‹ï¼‰
            referenced_indices = event_data.get("references", [])
            # å°†ç‰‡æ®µç¼–å·è½¬æ¢ä¸ºå®é™…çš„ section_id
            referenced_section_ids = []
            invalid_indices = []
            for idx in referenced_indices:
                if isinstance(idx, int) and 1 <= idx <= len(sections):  # éªŒè¯ç´¢å¼•æœ‰æ•ˆæ€§
                    section = sections[idx - 1]  # ç¼–å·ä»1å¼€å§‹ï¼Œç´¢å¼•ä»0å¼€å§‹
                    referenced_section_ids.append(section.id)
                else:
                    # è®°å½•æ— æ•ˆç´¢å¼•
                    invalid_indices.append(idx)

            # è®°å½•è­¦å‘Šï¼ˆå¦‚æœæœ‰æ— æ•ˆç´¢å¼•ï¼‰
            if invalid_indices:
                self.logger.warning(
                    f"äº‹é¡¹ '{event_data.get('title', 'æœªçŸ¥')}' åŒ…å«æ— æ•ˆçš„ç‰‡æ®µå¼•ç”¨ç´¢å¼•: {invalid_indices}",
                    extra={
                        "event_title": event_data.get("title"),
                        "invalid_indices": invalid_indices,
                        "total_sections": len(sections),
                    },
                )

            # ğŸ†• ==================== å®ä½“è½¬æ¢ã€å»é‡ä¸åˆå¹¶é€»è¾‘ï¼ˆæºå¤´å¤„ç†ï¼‰====================
            # 1. å°† LLM è¿”å›çš„æ•°ç»„æ ¼å¼è½¬æ¢ä¸ºæŒ‰ type åˆ†ç»„çš„å­—å…¸æ ¼å¼
            entities_from_llm = event_data.get("entities", [])
            entities_raw = {}

            # å¦‚æœ LLM è¿”å›çš„æ˜¯æ•°ç»„ï¼ˆschema å®šä¹‰çš„æ ¼å¼ï¼‰
            if isinstance(entities_from_llm, list):
                for entity_item in entities_from_llm:
                    if not isinstance(entity_item, dict):
                        continue
                    
                    entity_type = entity_item.get("type")
                    if not entity_type:
                        continue
                    
                    # æŒ‰ç±»å‹åˆ†ç»„
                    if entity_type not in entities_raw:
                        entities_raw[entity_type] = []
                    
                    entities_raw[entity_type].append({
                        "name": entity_item.get("name", ""),
                        "description": entity_item.get("description", "")  # ä¿ç•™ description
                    })
            # å…¼å®¹æ—§çš„å­—å…¸æ ¼å¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            elif isinstance(entities_from_llm, dict):
                entities_raw = entities_from_llm

            # 2. å¯¹æ¯ä¸ªç±»å‹å†…çš„å®ä½“å»é‡ï¼Œå¹¶æ™ºèƒ½åˆå¹¶ description
            entities_deduped = {}

            for entity_type, entity_list in entities_raw.items():
                if not entity_list:
                    entities_deduped[entity_type] = []
                    continue
                
                # ä½¿ç”¨å­—å…¸æ”¶é›†ï¼škey=normalized_name, value={"name": str, "descriptions": [str]}
                merged_entities = {}
                
                for entity_data in entity_list:
                    # å…¼å®¹æ ¼å¼ï¼šå­—å…¸æˆ–å­—ç¬¦ä¸²
                    if isinstance(entity_data, dict):
                        name = entity_data.get("name", "").strip()
                        description = entity_data.get("description", "").strip()
                    else:
                        name = str(entity_data).strip()
                        description = ""
                    
                    if not name:
                        continue
                    
                    # è§„èŒƒåŒ–åç§°ç”¨äºå»é‡
                    normalized_name = name.lower().strip()
                    
                    # ç¬¬ä¸€æ¬¡é‡åˆ°è¿™ä¸ªå®ä½“
                    if normalized_name not in merged_entities:
                        merged_entities[normalized_name] = {
                            "name": name,  # ä¿ç•™åŸå§‹åç§°ï¼ˆç¬¬ä¸€æ¬¡å‡ºç°çš„ï¼‰
                            "descriptions": []
                        }
                    
                    # æ”¶é›†æè¿°ï¼ˆå»é‡ã€å»ç©ºï¼‰
                    if description:
                        existing_descs = merged_entities[normalized_name]["descriptions"]
                        if description not in existing_descs:
                            existing_descs.append(description)
                
                # è½¬æ¢å›åˆ—è¡¨æ ¼å¼ï¼Œåˆå¹¶æè¿°
                deduped_list = []
                for entity_info in merged_entities.values():
                    # ç”¨ä¸­æ–‡é¡¿å·è¿æ¥å¤šä¸ªæè¿°
                    final_desc = "ã€".join(entity_info["descriptions"]) if entity_info["descriptions"] else ""
                    
                    deduped_list.append({
                        "name": entity_info["name"],
                        "description": final_desc  # åˆå¹¶åçš„æè¿°
                    })
                    
                    if len(entity_info["descriptions"]) > 1:
                        self.logger.debug(
                            f"âœ… åˆå¹¶é‡å¤å®ä½“æè¿° [{entity_type}] {entity_info['name']}: "
                            f"{len(entity_info['descriptions'])}ä¸ª -> {final_desc}"
                        )
                
                entities_deduped[entity_type] = deduped_list
            # =================================================================

            # ç¡®å®šä¸»è¦å¼•ç”¨çš„ chunkï¼ˆå–ç¬¬ä¸€ä¸ªè¢«å¼•ç”¨çš„ chunkï¼‰
            primary_chunk = None
            if referenced_section_ids:
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªè¢«å¼•ç”¨çš„ section å¯¹åº”çš„ chunk
                for section in sections:
                    if section.id == referenced_section_ids[0]:
                        primary_chunk = section
                        break
                if not primary_chunk:
                    primary_chunk = sections[0]  # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œé»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ª chunk
            else:
                primary_chunk = sections[0] if sections else None

            # ğŸ†• æ ¹æ®æ¥æºç±»å‹è®¾ç½®æ—¶é—´
            # æ³¨æ„ï¼šåœ¨ processor ä¸­ï¼Œäº‹é¡¹çš„ references ç›´æ¥ç»§æ‰¿è‡ª primary_chunk.references
            # æ‰€ä»¥ç”¨ primary_chunk.references æ¥æŸ¥è¯¢æ—¶é—´æ˜¯æ­£ç¡®çš„
            from datetime import datetime
            from dataflow.db import ChatMessage
            from sqlalchemy import select
            
            start_time = None
            end_time = None
            event_references = primary_chunk.references if primary_chunk else None
            
            if primary_chunk:
                if primary_chunk.source_type == "ARTICLE":
                    # æ–‡æ¡£ç±»å‹ï¼šä½¿ç”¨å½“å‰æ—¶é—´
                    current_time = datetime.now()
                    start_time = current_time
                    end_time = current_time
                    
                elif primary_chunk.source_type == "CHAT":
                    # ä¼šè¯ç±»å‹ï¼šä»å¼•ç”¨çš„æ¶ˆæ¯ä¸­è·å–æ—¶é—´èŒƒå›´
                    # ä½¿ç”¨ primary_chunk.referencesï¼ˆå› ä¸ºäº‹é¡¹ä¼šç»§æ‰¿è¿™ä¸ªï¼‰
                    if event_references and isinstance(event_references, list):
                        async with self.session_factory() as session:
                            result_msgs = await session.execute(
                                select(ChatMessage)
                                .where(ChatMessage.id.in_(event_references))
                                .order_by(ChatMessage.timestamp)
                            )
                            messages = list(result_msgs.scalars().all())
                            
                            if messages:
                                start_time = messages[0].timestamp  # æœ€æ—©æ—¶é—´
                                end_time = messages[-1].timestamp  # æœ€æ™šæ—¶é—´
                                self.logger.debug(
                                    f"ä¼šè¯äº‹é¡¹æ—¶é—´: {start_time} ~ {end_time} "
                                    f"(å…±{len(messages)}æ¡æ¶ˆæ¯)"
                                )

            # åˆ›å»ºäº‹é¡¹å¯¹è±¡
            source_type_value = primary_chunk.source_type if primary_chunk else "ARTICLE"
            event = SourceEvent(
                id=str(uuid.uuid4()),
                source_config_id=self.config.source_config_id,
                source_type=source_type_value,
                source_id=primary_chunk.source_id if primary_chunk else sections[0].source_id,
                article_id=sections[0].article_id if primary_chunk and primary_chunk.source_type == "ARTICLE" else None,
                conversation_id=primary_chunk.conversation_id if primary_chunk and primary_chunk.source_type == "CHAT" else None,
                title=event_data["title"],
                summary=event_data.get("summary") or "",
                content=event_data["content"],
                category=event_data.get("category") or "",  # ç‹¬ç«‹å­—æ®µï¼Œç¡®ä¿Noneè½¬ä¸ºç©ºå­—ç¬¦ä¸²
                # ä¸šåŠ¡å­—æ®µï¼ˆå…¼å®¹ä¸»ç³»ç»Ÿï¼‰- typeä¸source_typeä¿æŒä¸€è‡´
                type=source_type_value,
                priority="UNKNOWN",  # é»˜è®¤å€¼
                status="UNKNOWN",  # é»˜è®¤å€¼
                rank=None,  # ç”±ä¸Šå±‚ EventExtractor ç»Ÿä¸€åˆ†é…å…¨å±€ rank
                start_time=start_time,
                end_time=end_time,
                references=referenced_section_ids,  # âœ… ä¿®å¤ï¼šä½¿ç”¨LLMç²¾ç¡®æ ‡æ³¨çš„å¼•ç”¨
                chunk_id=primary_chunk.id if primary_chunk else None,
                extra_data={
                    "quality_score": event_data.get("quality_score", 0.8),
                    "batch_size": len(sections),
                    # ä¿å­˜å»é‡åçš„å®ä½“æ•°æ®ï¼Œç”¨äºç¬¬äºŒé˜¶æ®µå¤„ç†
                    "raw_entities": entities_deduped,
                },
            )

            events.append(event)

        return events

    async def initialize(self) -> None:
        """
        åˆå§‹åŒ–å¤„ç†å™¨ï¼ˆåŠ è½½å®ä½“ç±»å‹é…ç½®ï¼‰

        å¿…é¡»åœ¨ä½¿ç”¨å¤„ç†å™¨ä¹‹å‰è°ƒç”¨æ­¤æ–¹æ³•
        """
        await self._load_entity_types()

    async def _load_entity_types(self) -> None:
        """
        ä»æ•°æ®åº“åŠ è½½å®ä½“ç±»å‹é…ç½®

        åŠ è½½è§„åˆ™ï¼ˆæŒ‰ä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š
        1. æ–‡æ¡£çº§åˆ«ï¼ˆscope='article', article_id=å½“å‰æ–‡æ¡£ï¼‰
        2. ä¿¡æ¯æºçº§åˆ«ï¼ˆscope='source', source_config_id=å½“å‰ä¿¡æ¯æºï¼‰
        3. å…¨å±€è‡ªå®šä¹‰ï¼ˆscope='global', source_config_id IS NULL, is_default=FALSEï¼‰
        4. ç³»ç»Ÿé»˜è®¤ï¼ˆsource_config_id IS NULL, is_default=TRUEï¼‰

        æ³¨æ„ï¼šåŒä¸€ä¸ª type åªå–ä¼˜å…ˆçº§æœ€é«˜çš„é…ç½®
        """
        async with self.session_factory() as session:
            # æŸ¥è¯¢æ¡ä»¶åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
            conditions = []

            # 1. æ–‡æ¡£çº§åˆ«ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
            if self.config.article_id:
                conditions.append(
                    (DBEntityType.scope == 'article')
                    & (DBEntityType.article_id == self.config.article_id)
                    & DBEntityType.is_active
                )

            # 2. ä¿¡æ¯æºçº§åˆ«
            if self.config.source_config_id:
                conditions.append(
                    (DBEntityType.scope == 'source')
                    & (DBEntityType.source_config_id == self.config.source_config_id)
                    & DBEntityType.is_active
                )

            # 3. å…¨å±€è‡ªå®šä¹‰ç±»å‹
            conditions.append(
                (DBEntityType.scope == 'global')
                & DBEntityType.source_config_id.is_(None)
                & (DBEntityType.is_default == False)
                & DBEntityType.is_active
            )

            # 4. ç³»ç»Ÿé»˜è®¤ç±»å‹
            conditions.append(
                DBEntityType.source_config_id.is_(None)
                & DBEntityType.is_default
                & DBEntityType.is_active
                    )

            # æŸ¥è¯¢æ‰€æœ‰åŒ¹é…çš„å®ä½“ç±»å‹
            result = await session.execute(
                select(DBEntityType)
                .where(or_(*conditions))
                .order_by(DBEntityType.weight.desc())
            )
            all_entity_types = list(result.scalars().all())

            # å»é‡ï¼šåŒä¸€ä¸ª type åªä¿ç•™ä¼˜å…ˆçº§æœ€é«˜çš„
            # ä¼˜å…ˆçº§ï¼šæ–‡æ¡£ > ä¿¡æ¯æº > å…¨å±€ > é»˜è®¤
            type_priority_map = {}
            for et in all_entity_types:
                if et.type not in type_priority_map:
                    # ç¬¬ä¸€æ¬¡å‡ºç°è¯¥ç±»å‹ï¼Œè®°å½•ä¸‹æ¥
                    type_priority_map[et.type] = et
                else:
                    # è¯¥ç±»å‹å·²å­˜åœ¨ï¼Œæ¯”è¾ƒä¼˜å…ˆçº§
                    existing = type_priority_map[et.type]

                    # ç¡®å®šä¼˜å…ˆçº§å¾—åˆ†ï¼ˆæ•°å€¼è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
                    def get_priority_score(entity_type):
                        if entity_type.scope == 'article' and entity_type.article_id == self.config.article_id:
                            return 1  # æ–‡æ¡£çº§åˆ«
                        elif entity_type.scope == 'source' and entity_type.source_config_id == self.config.source_config_id:
                            return 2  # ä¿¡æ¯æºçº§åˆ«
                        elif entity_type.scope == 'global' and not entity_type.is_default:
                            return 3  # å…¨å±€è‡ªå®šä¹‰
                        elif entity_type.is_default:
                            return 4  # ç³»ç»Ÿé»˜è®¤
                        else:
                            return 5  # å…¶ä»–ï¼ˆä¸åº”è¯¥å‡ºç°ï¼‰

                    if get_priority_score(et) < get_priority_score(existing):
                        type_priority_map[et.type] = et

            self.entity_types = list(type_priority_map.values())

        self.logger.info(
            f"åŠ è½½äº† {len(self.entity_types)} ä¸ªå®ä½“ç±»å‹é…ç½®",
            extra={
                "article_id": self.config.article_id,
                "source_config_id": self.config.source_config_id,
                "entity_types": [et.type for et in self.entity_types],
            },
        )

        # ğŸ” è°ƒè¯•ï¼šè¾“å‡ºæ¯ä¸ªå®ä½“ç±»å‹çš„è¯¦ç»†ä¿¡æ¯
        # for et in self.entity_types:
        #     scope_desc = f"{et.scope}"
        #     if et.scope == 'article':
        #         scope_desc += f"(article_id={et.article_id[:8]}...)"
        #     elif et.scope == 'source':
        #         scope_desc += f"(source_config_id={et.source_config_id[:8] if et.source_config_id else 'None'}...)"
        #     elif et.is_default:
        #         scope_desc += "(default)"

        #     self.logger.info(
        #         f"ğŸ” å®ä½“ç±»å‹ [{et.type}]: "
        #         f"name={et.name}, scope={scope_desc}, "
        #         f"is_active={et.is_active}, is_default={et.is_default}, "
        #         f"value_constraints={et.value_constraints}"
        #     )

    async def _build_system_prompt(self, items: List, metadata: Dict, is_article: bool) -> str:
        """
        æ„å»º SYSTEM æç¤ºè¯ï¼ˆå¼‚æ­¥ï¼šæ”¯æŒå†å²äº‹é¡¹å¬å›ï¼‰
        
        Args:
            items: å†…å®¹åˆ—è¡¨
            metadata: å…ƒæ•°æ® {document_title, chunk_title, previous_context}
            is_article: æ˜¯å¦ä¸ºæ–‡ç« ç±»å‹
        
        Returns:
            SYSTEM æç¤ºè¯
        """
        import json
        from dataflow.core.ai.tokensize import extract_keywords
        
        # 1. æå–åŸæ–‡å†…å®¹
        raw_text = "\n".join([item.content for item in items if hasattr(item, 'content')])
        
        # 2. å¬å›ç›¸å…³å†å²äº‹é¡¹ï¼ˆåˆ†ç±»å’Œå®ä½“å‘½åå‚è€ƒï¼‰
        related_events = await self._recall_related_events(raw_text)
        
        # 3. æ„å»ºèƒŒæ™¯ä¿¡æ¯ï¼ˆåŒ…å«å†å²äº‹é¡¹å‚è€ƒï¼‰
        background = self._build_background(metadata, is_article, related_events)
        
        # 4. è·å–å®ä½“ç±»å‹è¯´æ˜
        entity_types = self._get_entity_types_description()
        
        # 5. æå–å€™é€‰å…³é”®è¯ï¼ˆåˆ†è¯å™¨é¢„æå–ï¼‰
        keywords = extract_keywords(raw_text, top_k=50) if raw_text else []
        candidate_keywords = "ã€".join(keywords) if keywords else "ï¼ˆæ— ï¼‰"
        
        # 6. æ„å»ºè¾“å‡º schema ç¤ºä¾‹ï¼ˆå•ä¸ªäº‹é¡¹çš„ç»“æ„ï¼‰
        output_schema = json.dumps({
            "title": "ç®€æ´æ ‡é¢˜",
            "summary": "ä¸€å¥è¯æ‘˜è¦",
            "content": "å®Œæ•´äº‹ä»¶å†…å®¹",
            "category": "åˆ†ç±»æ ‡ç­¾",
            "references": ["item.id"],
            "entities": [{"type": "ç±»å‹", "name": "åç§°", "description": "æè¿°"}],
            "is_valid": True
        }, ensure_ascii=False, indent=2)
        
        # 7. æ¸²æŸ“æ¨¡æ¿
        return self.prompt_manager.render(
            "event_extraction",
            background=background,
            entity_types=entity_types,
            candidate_keywords=candidate_keywords,
            output_schema=output_schema
        )
    
    def _build_background(self, metadata: Dict, is_article: bool, related_events: List[Dict] = None) -> str:
        """
        æ„å»ºèƒŒæ™¯ä¿¡æ¯
        
        Args:
            metadata: å…ƒæ•°æ®
            is_article: æ˜¯å¦ä¸ºæ–‡ç« ç±»å‹
            related_events: ç›¸å…³å†å²äº‹é¡¹åˆ—è¡¨
        
        Returns:
            èƒŒæ™¯ä¿¡æ¯æ–‡æœ¬
        """
        from datetime import datetime
        from zoneinfo import ZoneInfo
        
        tz = ZoneInfo("Asia/Shanghai")
        parts = [f"æ—¶é—´: {datetime.now(tz).strftime('%Y-%m-%d %H:%M')}"]
        
        # æ–‡æ¡£/ä¼šè¯æ ‡é¢˜
        if metadata.get("document_title"):
            label = "æ–‡æ¡£" if is_article else "ä¼šè¯"
            parts.append(f"{label}: {metadata['document_title']}")
        
        # å½“å‰ç‰‡æ®µæ ‡é¢˜
        if metadata.get("chunk_title"):
            parts.append(f"å½“å‰ç‰‡æ®µ: {metadata['chunk_title']}")
        
        # å‰æ–‡å‚è€ƒ
        if metadata.get("previous_context"):
            parts.append(f"\nå‰æ–‡å‚è€ƒ:\n{metadata['previous_context']}")
        
        # ç›¸å…³å†å²äº‹é¡¹ï¼ˆä»…ä¾›å‚è€ƒåˆ†ç±»æ ‡ç­¾å’Œå®ä½“å‘½åé£æ ¼ï¼‰
        if related_events:
            parts.append("\n### ç›¸å…³å†å²äº‹é¡¹")
            parts.append("ä»¥ä¸‹æ˜¯åŒä¿¡æ¯æºçš„å†å²äº‹é¡¹ï¼Œä¾›ä½ å‚è€ƒï¼š")
            parts.append("- å‚è€ƒå…¶ã€Œåˆ†ç±»ã€æ ‡ç­¾çš„å‘½åé£æ ¼")
            parts.append("- å‚è€ƒå…¶ã€Œå®ä½“ã€çš„æå–ç²’åº¦å’Œå‘½åæ–¹å¼")
            
            for i, event in enumerate(related_events, 1):
                title = event.get('title', '')
                category = event.get('category', '')
                entities = event.get('entities', [])
                # å®ä½“å¸¦ä¸Šç±»å‹ï¼šname(type)
                entities_str = "ã€".join([f"{e['name']}({e['type']})" for e in entities[:5]])
                
                parts.append(f"äº‹é¡¹{i}:")
                parts.append(f"  æ ‡é¢˜: {title}")
                if category:
                    parts.append(f"  åˆ†ç±»: {category}")
                if entities_str:
                    parts.append(f"  å®ä½“: {entities_str}")
        
        return "\n".join(parts)
    
    async def _ensure_recall_deps(self):
        """ç¡®ä¿å¬å›ä¾èµ–å¯ç”¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œå‚è€ƒ search æ¨¡å—ï¼‰"""
        if self._event_repo is None:
            from dataflow.modules.load.processor import DocumentProcessor
            
            # åˆå§‹åŒ– ES å®¢æˆ·ç«¯å’Œä»“åº“
            self._es_client = get_es_client()
            self._event_repo = EventVectorRepository(self._es_client)
            
            # åˆå§‹åŒ–å‘é‡ç”Ÿæˆå™¨ï¼ˆä½¿ç”¨ DocumentProcessorï¼Œä¸ search æ¨¡å—ä¸€è‡´ï¼‰
            self._embedding_client = DocumentProcessor(llm_client=self.llm_client)
    
    async def _recall_related_events(self, content: str) -> List[Dict]:
        """
        å¬å›åŒä¿¡æ¯æºçš„ç›¸å…³å†å²äº‹é¡¹ï¼ˆç”¨äºåˆ†ç±»å’Œå®ä½“å‘½åå‚è€ƒï¼‰
        
        Args:
            content: å½“å‰å¤„ç†çš„å†…å®¹æ–‡æœ¬
        
        Returns:
            å†å²äº‹é¡¹åˆ—è¡¨ [{title, category, entities: [{type, name}]}]
        """
        if not self.config.enable_related_events:
            return []
        
        try:
            # ç¡®ä¿ä¾èµ–å¯ç”¨
            await self._ensure_recall_deps()
            
            # 1. ç”Ÿæˆå½“å‰å†…å®¹çš„å‘é‡ï¼ˆä½¿ç”¨ DocumentProcessorï¼‰
            content_vector = await self._embedding_client.generate_embedding(content[:2000])
            
            # 2. ä» ES å¬å›åŒä¿¡æ¯æºçš„å†å²äº‹é¡¹
            results = await self._event_repo.search_similar_by_content(
                query_vector=content_vector,
                k=self.config.related_events_top_k,
                source_config_id=self.config.source_config_id
            )
            
            # 3. è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ
            results = [r for r in results if r.get("_score", 0) >= self.config.related_events_threshold]
            
            if not results:
                return []
            
            # 4. ä»æ•°æ®åº“åŠ è½½äº‹é¡¹è¯¦æƒ…å’Œå®ä½“
            event_ids = [r["event_id"] for r in results]
            related_events = []
            
            async with self.session_factory() as session:
                stmt = (
                    select(SourceEvent)
                    .options(selectinload(SourceEvent.event_associations).selectinload(EventEntity.entity))
                    .where(SourceEvent.id.in_(event_ids))
                )
                db_events = (await session.execute(stmt)).scalars().all()
                
                for event in db_events:
                    entities = [
                        {"type": ee.entity.type, "name": ee.entity.name}
                        for ee in event.event_associations if ee.entity
                    ]
                    related_events.append({
                        "title": event.title,
                        "category": event.category or "",
                        "entities": entities[:10]  # é™åˆ¶å®ä½“æ•°é‡
                    })
            
            self.logger.info(f"å¬å› {len(related_events)} ä¸ªç›¸å…³å†å²äº‹é¡¹ä½œä¸ºå‚è€ƒ")
            return related_events
            
        except Exception as e:
            self.logger.warning(f"å†å²äº‹é¡¹å¬å›å¤±è´¥: {e}")
            return []
    
    def _build_user_input(self, items: List, is_article: bool) -> Dict:
        """
        æ„å»º USER è¾“å…¥ï¼ˆç»Ÿä¸€ç»“æ„ï¼štype + name + description + itemsï¼‰
        
        Args:
            items: å†…å®¹åˆ—è¡¨
            is_article: æ˜¯å¦ä¸ºæ–‡ç« ç±»å‹
        
        Returns:
            {"type": "input", "name": "...", "description": "...", "items": [...]}
        """
        if is_article:
            # ArticleSection: heading + content
            items_data = [
                {
                    "id": item.id,
                    "content": f"{getattr(item, 'heading', '') or ''}\n{item.content}".strip()
                }
                for item in items
            ]
            name = "æ–‡æ¡£ç‰‡æ®µ"
            description = "æ¥æºäºæ–‡æ¡£çš„ç‰‡æ®µå†…å®¹ï¼Œæ¯ä¸ª item åŒ…å«æ ‡é¢˜å’Œæ­£æ–‡ï¼Œid ç”¨äºäº‹é¡¹å¼•ç”¨"
        else:
            # ChatMessage: [time] sender: content
            items_data = [
                {
                    "id": item.id,
                    "content": f"[{item.timestamp.strftime('%H:%M') if hasattr(item, 'timestamp') and item.timestamp else ''}] {getattr(item, 'sender_name', 'æœªçŸ¥')}: {item.content}"
                }
                for item in items
            ]
            name = "èŠå¤©æ¶ˆæ¯"
            description = "æ¥æºäºä¼šè¯çš„æ¶ˆæ¯è®°å½•ï¼Œæ¯ä¸ª item åŒ…å«æ—¶é—´ã€å‘é€è€…å’Œå†…å®¹ï¼Œid ç”¨äºäº‹é¡¹å¼•ç”¨"
        
        return {
            "type": "input",
            "name": name,
            "description": description,
            "items": items_data
        }

    def _get_entity_types_description(self) -> str:
        """è·å–å®ä½“ç±»å‹è¯´æ˜ï¼ˆç®€åŒ–ç‰ˆï¼Œåªå–ç¬¬ä¸€è¡Œæè¿°ï¼‰"""
        lines = []

        for entity_type in self.entity_types:
            # åªå–æè¿°çš„ç¬¬ä¸€è¡Œï¼Œé¿å…å¤ªé•¿
            desc = entity_type.description or ""
            first_line = desc.split('\n')[0].strip() if desc else f"æå–{entity_type.name}ç›¸å…³å®ä½“"
            lines.append(f"- **{entity_type.type}** ({entity_type.name}): {first_line}")

        return "\n".join(lines)

    def _build_extraction_schema(self) -> Dict[str, Any]:
        """
        æ„å»ºåŠ¨æ€JSON Schemaï¼ˆç»Ÿä¸€ç»“æ„ï¼štype + name + description + itemsï¼‰

        Returns:
            JSON Schemaå­—å…¸
        """
        valid_types = [et.type for et in self.entity_types]
        
        return {
            "type": "object",
            "properties": {
                "type": {"type": "string", "const": "output"},
                "name": {"type": "string"},
                "description": {"type": "string"},
                "items": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "title": {"type": "string"},
                            "summary": {"type": "string"},
                            "content": {"type": "string"},
                            "category": {"type": "string"},
                            "references": {
                                "type": "array",
                                "items": {"type": "string"},
                                "minItems": 1
                            },
                            "entities": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "type": {"type": "string", "enum": valid_types},
                                        "name": {"type": "string"},
                                        "description": {"type": "string"}
                                    },
                                    "required": ["type", "name", "description"]
                                }
                            },
                            "is_valid": {"type": "boolean"}
                        },
                        "required": ["title", "summary", "content", "category", "references", "entities", "is_valid"]
                    }
                }
            },
            "required": ["type", "name", "description", "items"]
        }

    async def _parse_extraction_result(
        self, result: Dict[str, Any], sections: List[SourceChunk]
    ) -> List[SourceEvent]:
        """
        è§£æLLMæå–ç»“æœä¸ºSourceEventå¯¹è±¡

        Args:
            result: LLMè¿”å›çš„JSONç»“æœ
            sections: åŸå§‹ç‰‡æ®µåˆ—è¡¨ï¼ˆç”¨äºç”Ÿæˆå¼•ç”¨ï¼‰

        Returns:
            SourceEventå¯¹è±¡åˆ—è¡¨
        """
        events = []

        for event_data in result.get("items", []):
            # è§£æ LLM æ ‡æ³¨çš„å¼•ç”¨ï¼ˆç‰‡æ®µç¼–å·ï¼Œä»1å¼€å§‹ï¼‰
            referenced_indices = event_data.get("references", [])

            # å°†ç‰‡æ®µç¼–å·è½¬æ¢ä¸ºå®é™…çš„ section_id
            referenced_section_ids = []
            invalid_indices = []

            for idx in referenced_indices:
                if isinstance(idx, int) and 1 <= idx <= len(sections):  # éªŒè¯ç´¢å¼•æœ‰æ•ˆæ€§
                    section = sections[idx - 1]  # ç¼–å·ä»1å¼€å§‹ï¼Œç´¢å¼•ä»0å¼€å§‹
                    referenced_section_ids.append(section.id)
                else:
                    # è®°å½•æ— æ•ˆç´¢å¼•
                    invalid_indices.append(idx)

            # è®°å½•è­¦å‘Šï¼ˆå¦‚æœæœ‰æ— æ•ˆç´¢å¼•ï¼‰
            if invalid_indices:
                self.logger.warning(
                    f"äº‹é¡¹ '{event_data.get('title', 'æœªçŸ¥')}' åŒ…å«æ— æ•ˆçš„ç‰‡æ®µå¼•ç”¨ç´¢å¼•: {invalid_indices}",
                    extra={
                        "event_title": event_data.get("title"),
                        "invalid_indices": invalid_indices,
                        "total_sections": len(sections),
                    },
                )

            # ç¡®å®šä¸»è¦å¼•ç”¨çš„ chunkï¼ˆå–ç¬¬ä¸€ä¸ªè¢«å¼•ç”¨çš„ chunkï¼‰
            primary_chunk = None
            if referenced_section_ids:
                # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªè¢«å¼•ç”¨çš„ section å¯¹åº”çš„ chunk
                for section in sections:
                    if section.id == referenced_section_ids[0]:
                        primary_chunk = section
                        break
                if not primary_chunk:
                    primary_chunk = sections[0]  # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œé»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ª chunk
            else:
                primary_chunk = sections[0] if sections else None

            # ğŸ†• æ ¹æ®æ¥æºç±»å‹è®¾ç½®æ—¶é—´
            from datetime import datetime
            from dataflow.db import ChatMessage
            from sqlalchemy import select
            
            start_time = None
            end_time = None
            event_references = primary_chunk.references if primary_chunk else None
            
            if primary_chunk:
                if primary_chunk.source_type == "ARTICLE":
                    # æ–‡æ¡£ç±»å‹ï¼šä½¿ç”¨å½“å‰æ—¶é—´
                    current_time = datetime.now()
                    start_time = current_time
                    end_time = current_time
                    
                elif primary_chunk.source_type == "CHAT":
                    # ä¼šè¯ç±»å‹ï¼šä»å¼•ç”¨çš„æ¶ˆæ¯ä¸­è·å–æ—¶é—´èŒƒå›´
                    # ä½¿ç”¨ primary_chunk.referencesï¼ˆå› ä¸ºäº‹é¡¹ä¼šç»§æ‰¿è¿™ä¸ªï¼‰
                    if event_references and isinstance(event_references, list):
                        async with self.session_factory() as session:
                            result_msgs = await session.execute(
                                select(ChatMessage)
                                .where(ChatMessage.id.in_(event_references))
                                .order_by(ChatMessage.timestamp)
                            )
                            messages = list(result_msgs.scalars().all())
                            
                            if messages:
                                start_time = messages[0].timestamp
                                end_time = messages[-1].timestamp
            
            # åˆ›å»ºäº‹é¡¹å¯¹è±¡
            # æ³¨æ„ï¼šsections åˆ—è¡¨å·²åœ¨æ–¹æ³•å¼€å§‹æ—¶éªŒè¯ä¸ºéç©º
            source_type_value = primary_chunk.source_type if primary_chunk else "ARTICLE"
            event = SourceEvent(
                id=str(uuid.uuid4()),
                source_config_id=self.config.source_config_id,
                source_type=source_type_value,  # ğŸ†•
                source_id=primary_chunk.source_id if primary_chunk else sections[0].source_id,  # ğŸ†•
                article_id=sections[0].article_id if primary_chunk and primary_chunk.source_type == "ARTICLE" else None,  # ğŸ†• ä¿®æ”¹
                conversation_id=primary_chunk.conversation_id if primary_chunk and primary_chunk.source_type == "CHAT" else None,  # ğŸ†•
                title=event_data["title"],
                summary=event_data.get("summary") or "",
                content=event_data["content"],
                category=event_data.get("category") or "",  # ç‹¬ç«‹å­—æ®µï¼Œç¡®ä¿Noneè½¬ä¸ºç©ºå­—ç¬¦ä¸²
                # ä¸šåŠ¡å­—æ®µï¼ˆå…¼å®¹ä¸»ç³»ç»Ÿï¼‰- typeä¸source_typeä¿æŒä¸€è‡´
                type=source_type_value,
                priority="UNKNOWN",  # é»˜è®¤å€¼
                status="UNKNOWN",  # é»˜è®¤å€¼
                rank=None,  # ç”±ä¸Šå±‚ EventExtractor ç»Ÿä¸€åˆ†é…å…¨å±€ rankï¼Œç¡®ä¿åŒä¸€æ–‡ç« å†…äº‹é¡¹æŒ‰é¡ºåºæ’åˆ—
                start_time=start_time,  # ğŸ†•
                end_time=end_time,  # ğŸ†•
                # ä½¿ç”¨ references å­—æ®µå­˜å‚¨ AI æ ‡æ³¨çš„å¼•ç”¨ç‰‡æ®µï¼ˆç²¾ç¡®å¼•ç”¨ï¼‰
                references=referenced_section_ids,  # âœ… ä¿®å¤ï¼šä½¿ç”¨LLMç²¾ç¡®æ ‡æ³¨çš„å¼•ç”¨
                chunk_id=primary_chunk.id if primary_chunk else None,
                extra_data={
                    "quality_score": event_data.get("quality_score", 0.8),
                    "batch_size": len(sections),
                    # categoryä¸å†å­˜å‚¨åœ¨extra_dataä¸­
                },
            )

            # è§£æå®ä½“
            entities_data = event_data.get("entities", {})
            event_associations = []

            # å¤„ç†æ¯ç§ç±»å‹çš„å®ä½“
            for entity_type, entity_names in entities_data.items():
                if not entity_names:
                    continue

                # æŸ¥æ‰¾å¯¹åº”çš„å®ä½“ç±»å‹å®šä¹‰
                entity_type_obj = self._get_entity_type_by_type(entity_type)
                if not entity_type_obj:
                    self.logger.warning(
                        f"æœªæ‰¾åˆ°å®ä½“ç±»å‹ '{entity_type}'ï¼Œè·³è¿‡è¯¥ç±»å‹çš„å®ä½“æå–",
                        extra={"entity_type": entity_type,
                               "event_title": event_data.get("title")},
                    )
                    continue

                for entity_data in entity_names:
                    # å…¼å®¹æ–°æ—§æ ¼å¼ï¼šå­—ç¬¦ä¸²æˆ–å¯¹è±¡
                    if isinstance(entity_data, dict):
                        name = entity_data.get("name")
                        description = entity_data.get("description", "")
                    else:
                        # æ—§æ ¼å¼ï¼šç›´æ¥æ˜¯å­—ç¬¦ä¸²
                        name = entity_data
                        description = ""

                    if not name:
                        continue

                    # è·å–æˆ–åˆ›å»ºå®ä½“IDï¼ˆä¸å†ä¼ é€’descriptionï¼‰
                    entity_id = await self._get_or_create_entity(
                        entity_type, name, entity_type_obj
                    )

                    # åˆ›å»ºå…³è”å¯¹è±¡ï¼ˆdescriptionä¿å­˜åˆ°ä¸­é—´è¡¨ï¼‰
                    assoc = EventEntity(
                        id=str(uuid.uuid4()),
                        event_id=event.id,
                        entity_id=entity_id,
                        weight=float(entity_type_obj.weight),
                        description=description or None,  # ä¿å­˜åˆ°ä¸­é—´è¡¨
                        extra_data={"confidence": event_data.get(
                            "quality_score", 0.8)},
                    )

                    # ç»‘å®šå…³ç³»
                    event_associations.append(assoc)

            event.event_associations = event_associations
            events.append(event)

        return events

    async def _get_or_create_entity(
        self, entity_type: str, entity_name: str, entity_type_obj: DBEntityType
    ) -> str:
        """
        è·å–æˆ–åˆ›å»ºå®ä½“çš„IDï¼ˆä½¿ç”¨æ–° sessionï¼‰

        å…ˆæŸ¥è¯¢æ•°æ®åº“æ˜¯å¦å­˜åœ¨ç›¸åŒ (source_config_id, type, normalized_name) çš„å®ä½“ï¼Œ
        å¦‚æœå­˜åœ¨åˆ™è¿”å›å…¶IDï¼Œå¦åˆ™åˆ›å»ºæ–°å®ä½“å¹¶è¿”å›æ–°IDã€‚

        Args:
            entity_type: å®ä½“ç±»å‹æ ‡è¯†ç¬¦
            entity_name: å®ä½“åŸå§‹åç§°
            entity_type_obj: å®ä½“ç±»å‹å¯¹è±¡

        Returns:
            å®ä½“ID
        """
        normalized_name = self._normalize_entity_name(entity_name)

        async with self.session_factory() as session:
            return await self._get_or_create_entity_with_session(
                session, entity_type, entity_name, normalized_name, entity_type_obj
            )

    async def _get_or_create_entity_with_session(
        self,
        session,
        entity_type: str,
        entity_name: str,
        normalized_name: str,
        entity_type_obj: DBEntityType,
    ) -> str:
        """
        è·å–æˆ–åˆ›å»ºå®ä½“çš„IDï¼ˆä½¿ç”¨å·²æœ‰ sessionï¼‰

        å…ˆæŸ¥è¯¢æ•°æ®åº“æ˜¯å¦å­˜åœ¨ç›¸åŒ (source_config_id, type, normalized_name) çš„å®ä½“ï¼Œ
        å¦‚æœå­˜åœ¨åˆ™è¿”å›å…¶IDï¼Œå¦åˆ™åˆ›å»ºæ–°å®ä½“å¹¶è¿”å›æ–°IDã€‚

        Args:
            session: æ•°æ®åº“ session
            entity_type: å®ä½“ç±»å‹æ ‡è¯†ç¬¦
            entity_name: å®ä½“åŸå§‹åç§°
            normalized_name: æ ‡å‡†åŒ–çš„å®ä½“åç§°
            entity_type_obj: å®ä½“ç±»å‹å¯¹è±¡

        Returns:
            å®ä½“ID
        """
        # æŸ¥è¯¢å·²å­˜åœ¨çš„å®ä½“
        result = await session.execute(
            select(Entity)
            .where(Entity.source_config_id == self.config.source_config_id)
            .where(Entity.type == entity_type)
            .where(Entity.normalized_name == normalized_name)
        )
        existing_entity = result.scalar_one_or_none()

        if existing_entity:
            self.logger.debug(
                f"å®ä½“å·²å­˜åœ¨ï¼š{entity_name} -> {existing_entity.name} (ID: {existing_entity.id})"
            )
            return existing_entity.id

        # åˆ›å»ºæ–°å®ä½“ï¼ˆä¸ä¿å­˜descriptionï¼‰
        new_entity = Entity(
            id=str(uuid.uuid4()),
            source_config_id=self.config.source_config_id,
            entity_type_id=entity_type_obj.id,
            type=entity_type,
            name=entity_name,
            normalized_name=normalized_name,
            description=None,  # ä¸å†ä¿å­˜descriptionåˆ°Entityè¡¨
            extra_data={},
        )

        # ğŸ†• è§£æç±»å‹åŒ–å€¼
        try:
            value_constraints = entity_type_obj.value_constraints if hasattr(
                entity_type_obj, 'value_constraints') else None
            entity_type_category = entity_type_obj.type if hasattr(
                entity_type_obj, 'type') else None
            typed_fields = self.parser.parse_to_typed_fields(
                entity_name,
                entity_type=entity_type,
                entity_type_category=entity_type_category,  # ğŸ†• ä¼ é€’å±æ€§ç±»å‹ï¼ˆtime/person/locationç­‰ï¼‰
                value_constraints=value_constraints
            )

            # å¡«å……ç±»å‹åŒ–å­—æ®µ
            if typed_fields:
                new_entity.value_type = typed_fields.get("value_type")
                new_entity.value_raw = typed_fields.get("value_raw")
                new_entity.int_value = typed_fields.get("int_value")
                new_entity.float_value = typed_fields.get("float_value")
                new_entity.datetime_value = typed_fields.get("datetime_value")
                new_entity.bool_value = typed_fields.get("bool_value")
                new_entity.enum_value = typed_fields.get("enum_value")
                new_entity.value_unit = typed_fields.get("value_unit")
                new_entity.value_confidence = typed_fields.get(
                    "value_confidence")

                self.logger.debug(
                    f"âœ… è§£æå®ä½“å€¼: {entity_name} -> {typed_fields.get('value_type')} = {typed_fields.get('int_value') or typed_fields.get('float_value') or typed_fields.get('datetime_value') or typed_fields.get('bool_value') or typed_fields.get('enum_value')}"
                )
        except Exception as e:
            # è§£æå¤±è´¥ä¸å½±å“å®ä½“åˆ›å»º
            self.logger.warning(f"âš ï¸ å®ä½“å€¼è§£æå¤±è´¥: {entity_name}, error={e}")

        # æ·»åŠ åˆ° sessionï¼ˆä½†ä¸ç«‹å³æäº¤ï¼‰
        # ğŸ†• ä¸åœ¨å†…å±‚å¤„ç†å¼‚å¸¸ï¼Œè®©å¼‚å¸¸ä¼ æ’­åˆ°å¤–å±‚ç”±é‡è¯•æœºåˆ¶ç»Ÿä¸€å¤„ç†
        # åŸå› ï¼šå†…å±‚ rollback ä¼šå›æ»šåŒä¸€ session ä¸­æ‰€æœ‰å·² flush ä½†æœª commit çš„å®ä½“
        session.add(new_entity)
        await session.flush()  # flush ä»¥è·å– IDï¼Œä½†ä¸æäº¤äº‹åŠ¡
        self.logger.debug(f"åˆ›å»ºæ–°å®ä½“ï¼š{entity_name} (ID: {new_entity.id})")
        return new_entity.id

    async def _batch_query_existing_entities(
        self, all_entities_data: Dict[str, Dict[str, str]]
    ) -> Dict[tuple, tuple]:
        """
        æ‰¹é‡æŸ¥è¯¢å·²å­˜åœ¨çš„å®ä½“ï¼ˆä¼˜åŒ–ï¼šå‡å°‘æ•°æ®åº“æŸ¥è¯¢æ¬¡æ•°ï¼‰
        
        Args:
            all_entities_data: {entity_type: {name: description}}
        
        Returns:
            {(entity_type, normalized_name): (entity_id, description)}
        """
        entity_id_map = {}
        
        # æ”¶é›†æ‰€æœ‰ (type, normalized_name, name, description) å…ƒç»„
        all_keys = []
        for entity_type, entities_dict in all_entities_data.items():
            for name, description in entities_dict.items():
                normalized = self._normalize_entity_name(name)
                all_keys.append((entity_type, normalized, name, description))
        
        if not all_keys:
            return entity_id_map
        
        # æ‰¹é‡æŸ¥è¯¢ï¼ˆåˆ†æ‰¹é¿å… SQL è¿‡é•¿ï¼‰
        batch_size = 100
        async with self.session_factory() as session:
            for i in range(0, len(all_keys), batch_size):
                batch = all_keys[i:i + batch_size]
                
                # æ„å»º OR æ¡ä»¶
                conditions = [
                    (Entity.type == etype) & (Entity.normalized_name == norm)
                    for etype, norm, _, _ in batch
                ]
                
                result = await session.execute(
                    select(Entity)
                    .where(Entity.source_config_id == self.config.source_config_id)
                    .where(or_(*conditions))
                )
                
                # å»ºç«‹å·²å­˜åœ¨å®ä½“çš„æ˜ å°„
                for entity in result.scalars().all():
                    key = (entity.type, entity.normalized_name)
                    # æ‰¾åˆ°å¯¹åº”çš„ description
                    for etype, norm, name, desc in batch:
                        if (etype, norm) == key:
                            entity_id_map[key] = (entity.id, desc)
                            break
        
        self.logger.info(f"æ‰¹é‡æŸ¥è¯¢: éœ€è¦ {len(all_keys)} ä¸ªå®ä½“ï¼Œå·²å­˜åœ¨ {len(entity_id_map)} ä¸ª")
        return entity_id_map

    async def _create_entity_with_retry(
        self,
        entity_type: str,
        entity_name: str,
        entity_type_obj: DBEntityType,
    ) -> str:
        """
        ç‹¬ç«‹äº‹åŠ¡åˆ›å»ºå®ä½“ï¼Œå†²çªæ—¶é‡æ–°æŸ¥è¯¢
        
        æ¯ä¸ªå®ä½“ä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®åº“äº‹åŠ¡ï¼Œå†²çªä¸ä¼šå½±å“å…¶ä»–å®ä½“ã€‚
        å¤„ç†çš„é”™è¯¯ç±»å‹ï¼š
        - IntegrityError: å”¯ä¸€çº¦æŸå†²çªï¼ˆå¹¶å‘åˆ›å»ºç›¸åŒå®ä½“ï¼‰
        - OperationalError: æ­»é”(1213)ã€é”ç­‰å¾…è¶…æ—¶(1205)
        
        Args:
            entity_type: å®ä½“ç±»å‹æ ‡è¯†ç¬¦
            entity_name: å®ä½“åŸå§‹åç§°
            entity_type_obj: å®ä½“ç±»å‹å¯¹è±¡
        
        Returns:
            å®ä½“ID
        """
        import asyncio
        from sqlalchemy.exc import IntegrityError, OperationalError
        
        normalized_name = self._normalize_entity_name(entity_name)
        max_retries = 3
        base_delay = 0.05  # 50ms åŸºç¡€å»¶è¿Ÿ
        
        for attempt in range(max_retries):
            async with self.session_factory() as session:
                try:
                    # å†æ¬¡æŸ¥è¯¢ï¼ˆå¯èƒ½å·²è¢«å…¶ä»–ä»»åŠ¡åˆ›å»ºï¼‰
                    result = await session.execute(
                        select(Entity)
                        .where(Entity.source_config_id == self.config.source_config_id)
                        .where(Entity.type == entity_type)
                        .where(Entity.normalized_name == normalized_name)
                    )
                    existing = result.scalar_one_or_none()
                    if existing:
                        self.logger.debug(f"ğŸ”„ å®ä½“å·²è¢«å…¶ä»–ä»»åŠ¡åˆ›å»º: {entity_name}")
                        return existing.id
                    
                    # åˆ›å»ºæ–°å®ä½“
                    new_entity = Entity(
                        id=str(uuid.uuid4()),
                        source_config_id=self.config.source_config_id,
                        entity_type_id=entity_type_obj.id,
                        type=entity_type,
                        name=entity_name,
                        normalized_name=normalized_name,
                        description=None,
                        extra_data={},
                    )
                    
                    # è§£æç±»å‹åŒ–å€¼
                    try:
                        value_constraints = getattr(entity_type_obj, 'value_constraints', None)
                        typed_fields = self.parser.parse_to_typed_fields(
                            entity_name,
                            entity_type=entity_type,
                            entity_type_category=entity_type_obj.type,
                            value_constraints=value_constraints
                        )
                        if typed_fields:
                            new_entity.value_type = typed_fields.get("value_type")
                            new_entity.value_raw = typed_fields.get("value_raw")
                            new_entity.int_value = typed_fields.get("int_value")
                            new_entity.float_value = typed_fields.get("float_value")
                            new_entity.datetime_value = typed_fields.get("datetime_value")
                            new_entity.bool_value = typed_fields.get("bool_value")
                            new_entity.enum_value = typed_fields.get("enum_value")
                            new_entity.value_unit = typed_fields.get("value_unit")
                            new_entity.value_confidence = typed_fields.get("value_confidence")
                    except Exception as e:
                        self.logger.warning(f"âš ï¸ å®ä½“å€¼è§£æå¤±è´¥: {entity_name}, error={e}")
                    
                    session.add(new_entity)
                    await session.commit()
                    self.logger.debug(f"âœ… åˆ›å»ºæ–°å®ä½“: {entity_name} (ID: {new_entity.id})")
                    return new_entity.id
                    
                except IntegrityError as exc:
                    # å¹¶å‘å†²çªï¼ˆå”¯ä¸€çº¦æŸï¼‰ï¼Œå›æ»šå¹¶é‡è¯•
                    await session.rollback()
                    self.logger.debug(f"ğŸ”„ å®ä½“åˆ›å»ºå†²çªï¼Œé‡è¯• ({attempt + 1}/{max_retries}): {entity_name}")
                    
                    if attempt == max_retries - 1:
                        # æœ€åä¸€æ¬¡é‡è¯•ï¼šæŸ¥è¯¢å·²å­˜åœ¨çš„å®ä½“
                        async with self.session_factory() as retry_session:
                            result = await retry_session.execute(
                                select(Entity)
                                .where(Entity.source_config_id == self.config.source_config_id)
                                .where(Entity.type == entity_type)
                                .where(Entity.normalized_name == normalized_name)
                            )
                            existing = result.scalar_one_or_none()
                            if existing:
                                self.logger.debug(f"âœ… å†²çªåæŸ¥è¯¢åˆ°å·²å­˜åœ¨å®ä½“: {entity_name}")
                                return existing.id
                        raise ExtractError(f"å®ä½“åˆ›å»ºå¤±è´¥ï¼ˆé‡è¯•{max_retries}æ¬¡ï¼‰: {entity_name}") from exc
                    
                    await asyncio.sleep(base_delay * (2 ** attempt))
                    
                except OperationalError as exc:
                    # æ­»é”(1213)æˆ–é”ç­‰å¾…è¶…æ—¶(1205)
                    error_str = str(exc)
                    is_deadlock = "1213" in error_str or "Deadlock" in error_str
                    is_lock_timeout = "1205" in error_str or "Lock wait timeout" in error_str
                    
                    if is_deadlock or is_lock_timeout:
                        await session.rollback()
                        error_type = "æ­»é”" if is_deadlock else "é”è¶…æ—¶"
                        self.logger.warning(
                            f"ğŸ”„ å®ä½“åˆ›å»º{error_type}ï¼Œé‡è¯• ({attempt + 1}/{max_retries}): {entity_name}"
                        )
                        
                        if attempt == max_retries - 1:
                            raise ExtractError(
                                f"å®ä½“åˆ›å»ºå¤±è´¥ï¼ˆ{error_type}ï¼Œé‡è¯•{max_retries}æ¬¡ï¼‰: {entity_name}"
                            ) from exc
                        
                        # æ­»é”ä½¿ç”¨æ›´é•¿çš„é€€é¿æ—¶é—´
                        delay = base_delay * (2 ** (attempt + 1)) if is_deadlock else base_delay * (2 ** attempt)
                        await asyncio.sleep(delay)
                    else:
                        # éæ­»é”/é”è¶…æ—¶çš„ OperationalErrorï¼Œç›´æ¥æŠ›å‡º
                        raise
        
        raise ExtractError(f"å®ä½“åˆ›å»ºå¤±è´¥: {entity_name}")

    def _normalize_entity_name(self, name: str) -> str:
        """
        æ ‡å‡†åŒ–å®ä½“åç§°

        Args:
            name: åŸå§‹åç§°ï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–å…¶ä»–ç±»å‹ï¼Œå¦‚æ•´æ•°ï¼‰

        Returns:
            æ ‡å‡†åŒ–åçš„åç§°
        """
        import re

        # å…ˆè½¬ä¸ºå­—ç¬¦ä¸²ï¼Œç¡®ä¿èƒ½å¤„ç†éå­—ç¬¦ä¸²è¾“å…¥ï¼ˆå¦‚ LLM æå–çš„æ•°å­—å®ä½“ï¼‰
        name_str = str(name)

        # å»é™¤é¦–å°¾ç©ºæ ¼å¹¶è½¬å°å†™
        normalized = name_str.strip().lower()

        # å»é™¤å¤šä½™çš„ç©ºæ ¼ï¼ˆå¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ªï¼‰
        normalized = re.sub(r"\s+", " ", normalized)

        # å»é™¤å¸¸è§çš„æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™ä¸­æ–‡æ ‡ç‚¹ï¼‰
        normalized = re.sub(r"[^\w\s\u4e00-\u9fff]", "", normalized)

        return normalized.strip()

    def _get_entity_type_by_type(self, entity_type: str) -> Optional[DBEntityType]:
        """
        æ ¹æ®ç±»å‹æ ‡è¯†ç¬¦æŸ¥æ‰¾å®ä½“ç±»å‹

        Args:
            entity_type: å®ä½“ç±»å‹æ ‡è¯†ç¬¦

        Returns:
            å®ä½“ç±»å‹å¯¹è±¡ï¼Œå¦‚æœæœªæ‰¾åˆ°è¿”å› None
        """
        for et in self.entity_types:
            if et.type == entity_type:
                return et
        return None

    def _get_entity_type_weight(self, entity_type: str) -> float:
        """
        è·å–å®ä½“ç±»å‹æƒé‡

        Args:
            entity_type: å®ä½“ç±»å‹

        Returns:
            æƒé‡å€¼
        """
        # ä»åŠ è½½çš„å®ä½“ç±»å‹ä¸­æŸ¥æ‰¾
        entity_type_obj = self._get_entity_type_by_type(entity_type)
        if entity_type_obj:
            return float(entity_type_obj.weight)

        # é»˜è®¤æƒé‡
        return 1.0

    def _cross_fill_entities(self, events: List[SourceEvent]) -> List[SourceEvent]:
        """
        å®ä½“äº¤å‰è¡¥å……ï¼ˆå…œåº•ç­–ç•¥ï¼‰
        
        æ”¶é›†æœ¬æ¬¡æå–çš„æ‰€æœ‰å®ä½“ï¼Œæ£€æŸ¥æ¯ä¸ªäº‹é¡¹çš„æ ‡é¢˜+æ­£æ–‡æ˜¯å¦åŒ…å«å…¶ä»–äº‹é¡¹çš„å®ä½“ä½†æœªæå–ï¼Œ
        å¦‚æœåŒ…å«åˆ™è‡ªåŠ¨è¡¥å……ã€‚
        
        æ³¨æ„ï¼šæƒé‡å°äº1çš„å®ä½“ç±»å‹ä¸å‚ä¸äº¤å‰ï¼Œä»¥å‡å°‘å™ªéŸ³ã€‚
        
        Args:
            events: æœ¬æ¬¡æå–çš„äº‹é¡¹åˆ—è¡¨ï¼ˆextra_data["raw_entities"] å­˜å‚¨å®ä½“ï¼‰
        
        Returns:
            è¡¥å……åçš„äº‹é¡¹åˆ—è¡¨
        """
        if len(events) <= 1:
            return events
        
        # æ‰¹é‡è·å–å®ä½“ç±»å‹æƒé‡
        type_weights = {et.type: float(et.weight) for et in self.entity_types}
        
        # æ”¶é›†æ‰€æœ‰å®ä½“ï¼Œè¿‡æ»¤ä½æƒé‡ç±»å‹
        # key: (type, name_lower), value: {type, name, description}
        all_entities = {}
        filtered_count = 0
        
        for event in events:
            raw_entities = event.extra_data.get("raw_entities", {})
            for entity_type, entity_list in raw_entities.items():
                # æƒé‡ < 1 çš„å®ä½“ç±»å‹ä¸å‚ä¸äº¤å‰
                if type_weights.get(entity_type, 1.0) < 1.0:
                    filtered_count += len(entity_list) if entity_list else 0
                    continue
                
                for entity_data in entity_list:
                    if isinstance(entity_data, dict):
                        name = entity_data.get('name', '')
                        description = entity_data.get('description', '')
                    else:
                        name = str(entity_data)
                        description = ''
                    
                    if not name:
                        continue
                    
                    key = (entity_type, name.lower())
                    if key not in all_entities:
                        all_entities[key] = {
                            'type': entity_type,
                            'name': name,
                            'description': description
                        }
        
        if not all_entities:
            if filtered_count > 0:
                self.logger.debug(f"å®ä½“äº¤å‰è¡¥å……: è¿‡æ»¤äº† {filtered_count} ä¸ªä½æƒé‡ç±»å‹å®ä½“")
            return events
        
        self.logger.info(
            f"å®ä½“äº¤å‰è¡¥å……: {len(all_entities)} ä¸ªå®ä½“å¾…æ£€æŸ¥ï¼Œè¿‡æ»¤ {filtered_count} ä¸ªä½æƒé‡ç±»å‹"
        )
        
        # äº¤å‰è¡¥å……
        total_added = 0
        for event in events:
            text = f"{event.title} {event.content}".lower()
            raw_entities = event.extra_data.get("raw_entities", {})
            
            # æ”¶é›†å½“å‰äº‹é¡¹å·²æœ‰çš„å®ä½“åç§°ï¼ˆæŒ‰ç±»å‹ï¼‰
            existing_by_type = {}  # {type: set(name_lower)}
            for entity_type, entity_list in raw_entities.items():
                existing_by_type[entity_type] = set()
                for entity_data in entity_list:
                    if isinstance(entity_data, dict):
                        name = entity_data.get('name', '').lower()
                    else:
                        name = str(entity_data).lower()
                    if name:
                        existing_by_type[entity_type].add(name)
            
            # æ£€æŸ¥å¹¶è¡¥å……ç¼ºå¤±çš„å®ä½“
            added = []
            for (entity_type, name_lower), entity in all_entities.items():
                # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«è¯¥å®ä½“åç§°
                if name_lower not in text:
                    continue
                
                # æ£€æŸ¥è¯¥ç±»å‹æ˜¯å¦å·²æœ‰è¯¥å®ä½“
                if entity_type in existing_by_type and name_lower in existing_by_type[entity_type]:
                    continue
                
                # è¡¥å……å®ä½“
                if entity_type not in raw_entities:
                    raw_entities[entity_type] = []
                
                raw_entities[entity_type].append({
                    'name': entity['name'],
                    'description': entity.get('description', '')
                })
                
                # æ›´æ–°å·²å­˜åœ¨é›†åˆ
                if entity_type not in existing_by_type:
                    existing_by_type[entity_type] = set()
                existing_by_type[entity_type].add(name_lower)
                
                added.append(f"{entity['name']}({entity_type})")
            
            if added:
                total_added += len(added)
                event.extra_data["raw_entities"] = raw_entities
                self.logger.debug(f"äº‹é¡¹ '{event.title[:30]}' è¡¥å……å®ä½“: {', '.join(added)}")
        
        if total_added > 0:
            self.logger.info(f"å®ä½“äº¤å‰è¡¥å……å®Œæˆ: å…±è¡¥å…… {total_added} ä¸ªå®ä½“")
        
        return events

    def _inject_time_entities_for_event(self, event: SourceEvent) -> None:
        """
        è‡ªåŠ¨æ³¨å…¥æ—¶é—´å®ä½“ï¼ˆåŸºäºäº‹é¡¹çš„ start_time/end_time å­—æ®µï¼‰

        è§„åˆ™ï¼š
        - å¦‚æœé…ç½®äº† start_time ç±»å‹ä¸”äº‹é¡¹æœ‰ start_time â†’ æ³¨å…¥
        - å¦‚æœé…ç½®äº† end_time ç±»å‹ä¸”äº‹é¡¹æœ‰ end_time â†’ æ³¨å…¥
        - æ²¡æœ‰é…ç½®å¯¹åº”ç±»å‹ â†’ è·³è¿‡

        Args:
            event: äº‹é¡¹å¯¹è±¡
        """
        # ç¡®ä¿æœ‰ raw_entities å­—æ®µ
        if "raw_entities" not in event.extra_data:
            event.extra_data["raw_entities"] = {}

        raw_entities = event.extra_data["raw_entities"]

        # æ£€æŸ¥æ˜¯å¦é…ç½®äº† start_time ç±»å‹
        has_start_time_type = any(et.type == "start_time" for et in self.entity_types)

        # æ£€æŸ¥æ˜¯å¦é…ç½®äº† end_time ç±»å‹
        has_end_time_type = any(et.type == "end_time" for et in self.entity_types)

        # æ³¨å…¥å¼€å§‹æ—¶é—´
        if has_start_time_type and event.start_time:
            time_str = event.start_time.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
            if "start_time" not in raw_entities:
                raw_entities["start_time"] = []
            raw_entities["start_time"].append({
                "name": time_str,
                "description": "äº‹é¡¹å¼€å§‹æ—¶é—´"
            })
            self.logger.debug(f"âœ… æ³¨å…¥å¼€å§‹æ—¶é—´: {time_str}, event_id={event.id[:8]}...")

        # æ³¨å…¥ç»“æŸæ—¶é—´
        if has_end_time_type and event.end_time:
            time_str = event.end_time.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")
            if "end_time" not in raw_entities:
                raw_entities["end_time"] = []
            raw_entities["end_time"].append({
                "name": time_str,
                "description": "äº‹é¡¹ç»“æŸæ—¶é—´"
            })
            self.logger.debug(f"âœ… æ³¨å…¥ç»“æŸæ—¶é—´: {time_str}, event_id={event.id[:8]}...")
