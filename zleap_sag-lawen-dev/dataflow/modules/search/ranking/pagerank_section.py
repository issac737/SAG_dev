"""
æœç´¢ Rerank æ¨¡å— - æ®µè½çº§å®ç°

å®ç°6æ­¥éª¤çš„æŸ¥æ‰¾æœ€é‡è¦åŸæ–‡å—çš„æ–¹æ³•ï¼š
1. keyæ‰¾contentï¼šæ ¹æ®[key-final]ä»sqlä¸­æå–åŸæ–‡å—[content-key-related]ï¼Œä»ESè·å–é¢„å­˜å‘é‡å¹¶è®¡ç®—å’Œqueryçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆè®°å½•event_idï¼‰
2. queryæ‰¾contentï¼šé€šè¿‡å‘é‡ç›¸ä¼¼åº¦ï¼ˆKNN+ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰åœ¨å‘é‡æ•°æ®åº“æ‰¾åˆ°åŸæ–‡å—[content-query-related]ï¼ˆå·²åˆ é™¤ï¼‰
3. contentåˆå¹¶+å»é‡ï¼šåˆå¹¶[content-key-related]å’Œ[content-query-related]ï¼ˆå·²åˆ é™¤ï¼Œä¸å†éœ€è¦ï¼‰
4. åˆ¶ä½œ[content-related]æƒé‡å‘é‡ï¼šä½¿ç”¨å…¬å¼ weight = 0.5*ç›¸ä¼¼åº¦ + ln(1 + Î£(keyæƒé‡ Ã— ln(1+å‡ºç°æ¬¡æ•°) / step))
5. PageRanké‡æ’åºï¼šåŸºäºæ®µè½é—´çš„å…±åŒå®ä½“æ„å»ºå…³ç³»å›¾ï¼Œä½¿ç”¨PageRankç®—æ³•é‡æ–°æ’åº
6. å–Top-Nå¹¶è¿”å›ï¼šæŒ‰PageRankå¾—åˆ†æ’åº

è¿”å›æ ¼å¼ï¼š
Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
    - sections (List[Dict]): æ®µè½å¯¹è±¡åˆ—è¡¨ï¼ˆæŒ‰PageRanké¡ºåºæ’åˆ—ï¼‰
    - clues (Dict): å¬å›çº¿ç´¢ä¿¡æ¯
        - origin_query (str): åŸå§‹æŸ¥è¯¢ï¼ˆé‡å†™å‰ï¼‰
        - final_query (str): LLMé‡å†™åçš„æŸ¥è¯¢ï¼ˆé‡å†™åï¼‰
        - query_entities (List[Dict]): æŸ¥è¯¢å¬å›çš„å®ä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼‰
        - recall_entities (List[Dict]): å¬å›çš„å®ä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼Œè¿‡æ»¤æ‰query_entitiesä¸­çš„å€¼ï¼‰

å…³é”®ç‰¹æ€§ï¼š
- å®æ—¶æƒé‡è®¡ç®—ï¼šåŸºäºkeyåœ¨æ®µè½å†…å®¹ä¸­çš„å®é™…å‡ºç°é¢‘æ¬¡è®¡ç®—æƒé‡
- å†…å®¹æ„ŸçŸ¥ï¼šæƒé‡åŸºäºkeyåœ¨æ®µè½å†…å®¹ä¸­çš„å®é™…å‡ºç°é¢‘æ¬¡
- PageRanké‡æ’åºï¼šè€ƒè™‘æ®µè½é—´çš„å…³è”å…³ç³»ï¼Œæå‡æ•´ä½“æ’åºè´¨é‡

"""

from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass
import numpy as np
import math
import re
import time
import asyncio
import logging
from collections import Counter, defaultdict


from sqlalchemy import select, and_, or_
from sqlalchemy.orm import selectinload

from dataflow.core.storage.elasticsearch import get_es_client
from dataflow.core.storage.repositories.source_chunk_repository import SourceChunkRepository
from dataflow.core.storage.repositories.event_repository import EventVectorRepository
from dataflow.db import SourceEvent, Entity, EventEntity, ArticleSection, Article, SourceChunk, get_session_factory
from dataflow.exceptions import AIError
from dataflow.modules.load.processor import DocumentProcessor
from dataflow.modules.search.config import SearchConfig, BM25Config
from dataflow.modules.search.tracker import Tracker  # ğŸ†• æ·»åŠ çº¿ç´¢è¿½è¸ªå™¨
from dataflow.utils import get_logger
from .base_pagerank import BasePageRankSearcher, ContentSearchResult

logger = get_logger("search.rerank.pagerank")


class RerankPageRankSearcher(BasePageRankSearcher):
    """Rerankæ®µè½æœç´¢å™¨ - å®ç°6æ­¥éª¤çš„æŸ¥æ‰¾æœ€é‡è¦æ®µè½çš„æ–¹æ³•"""


    async def search(
        self,
        key_final: List[Dict[str, Any]],
        config: SearchConfig
    ) -> Dict[str, Any]:
        """
        Rerank æœç´¢ä¸»æ–¹æ³•ï¼ˆæ®µè½çº§ + PageRankï¼‰

        æ•´åˆæ­¥éª¤1-6ï¼Œç»Ÿä¸€è¿›è¡Œqueryå‘é‡åŒ–ï¼Œé¿å…é‡å¤è®¡ç®—

        æ­¥éª¤æµç¨‹ï¼š
          1. keyæ‰¾contentï¼ˆå‘é‡ç›¸ä¼¼åº¦è¿‡æ»¤ï¼‰ï¼šåŸºäºå®ä½“å…³è”æ‰¾åˆ°ç›¸å…³æ®µè½
          2. queryæ‰¾contentï¼ˆå‘é‡ç›¸ä¼¼åº¦è¿‡æ»¤ï¼‰ï¼šåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦æ‰¾åˆ°ç›¸å…³æ®µè½ï¼ˆå·²åˆ é™¤ï¼‰
          3. åˆå¹¶contentå»é‡ï¼ˆä¼˜å…ˆä¿ç•™step1ç»“æœï¼‰ï¼šå®ä½“å¬å›çš„æ®µè½ä¼˜å…ˆçº§æ›´é«˜ï¼ˆå·²åˆ é™¤ï¼‰
          4. è®¡ç®—æ®µè½æƒé‡å‘é‡ï¼šç»“åˆç›¸ä¼¼åº¦å’Œå®ä½“æƒé‡è®¡ç®—æƒé‡å¹¶æ’åº
          5. PageRanké‡æ’åºï¼šåŸºäºæ®µè½é—´çš„å…±åŒå®ä½“æ„å»ºå…³ç³»å›¾ï¼Œä½¿ç”¨PageRankç®—æ³•é‡æ–°æ’åº
          6. é€‰æ‹©Top-Næ®µè½ï¼šæŒ‰PageRankå¾—åˆ†æ’åºå¹¶ä¿ç•™çº¿ç´¢

        Args:
            key_final: ä»Recallè¿”å›çš„å…³é”®å®ä½“åˆ—è¡¨
            config: Rerankæœç´¢é…ç½®

        Returns:
            Dict[str, Any]: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸ï¼š
                - sections (List[Dict]): æ®µè½å¯¹è±¡åˆ—è¡¨ï¼ˆæŒ‰PageRanké¡ºåºæ’åˆ—ï¼‰
                - clues (Dict): å¬å›çº¿ç´¢ä¿¡æ¯
                    - origin_query (str): åŸå§‹æŸ¥è¯¢ï¼ˆé‡å†™å‰ï¼‰
                    - final_query (str): LLMé‡å†™åçš„æŸ¥è¯¢ï¼ˆé‡å†™åï¼‰
                    - query_entities (List[Dict]): æŸ¥è¯¢å¬å›çš„å®ä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼‰
                    - recall_entities (List[Dict]): å¬å›çš„å®ä½“åˆ—è¡¨ï¼ˆkey_idæ”¹ä¸ºidï¼Œè¿‡æ»¤æ‰query_entitiesä¸­çš„å€¼ï¼‰
        """
        try:
            # è®°å½•æ€»ä½“å¼€å§‹æ—¶é—´
            overall_start = time.perf_counter()

            self.logger.info("=" * 80)
            self.logger.info(f"ã€æ®µè½çº§ã€‘Rerankæœç´¢å¼€å§‹")
            self.logger.info(f"Query: '{config.query}'")
            self.logger.info(f"Source IDs: {config.get_source_config_ids()}")
            self.logger.info("=" * 80)
            self.logger.info("ã€æ®µè½çº§ã€‘Rerankæœç´¢é…ç½®")
            self.logger.info(f"  - skip_pagerank: {config.rerank.skip_pagerank}")
            self.logger.info("=" * 80)
            if config.rerank.skip_pagerank:
                self.logger.warning(
                    "âš ï¸  è­¦å‘Š: skip_pagerank=Trueï¼Œ"
                    "å·²è·³è¿‡ PageRank é‡æ’åºï¼Œæ’åºè´¨é‡å¯èƒ½ä¸‹é™ä½†é€Ÿåº¦æ›´å¿«"
                )

            # ç»Ÿä¸€è¿›è¡Œqueryå‘é‡åŒ–ï¼ˆé¿å…åœ¨step1å’Œstep2ä¸­é‡å¤è®¡ç®—ï¼‰
            vector_start = time.perf_counter()
            query_vector = await self._generate_query_vector(config.query, config)
            vector_time = time.perf_counter() - vector_start
            if config.has_query_embedding:
                self.logger.info(
                    f"âœ“ ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(query_vector)}, è€—æ—¶: {vector_time:.3f}ç§’")
            else:
                self.logger.info(
                    f"âœ“ æŸ¥è¯¢å‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_vector)}, è€—æ—¶: {vector_time:.3f}ç§’")

            # ç”¨äºè®°å½•å„æ­¥éª¤è€—æ—¶
            step_times = {}

            # æ­¥éª¤1: key->content
            self.logger.info("\n" + "=" * 80)
            self.logger.info("ã€Step1ã€‘æ‰§è¡Œå®ä½“å¬å›ï¼ˆå·²ç¦ç”¨Queryå¬å›ï¼‰...")
            self.logger.info("=" * 80)
            step1_start = time.perf_counter()

            # æ‰§è¡Œ step1
            step1_results = await self._step1_keys_to_contents(
                key_final=key_final,
                query=config.query,
                source_config_ids=config.get_source_config_ids(),
                query_vector=query_vector,
                config=config
            )

            # ğŸ†• BM25 æœç´¢ä½œä¸ºç‹¬ç«‹çš„å¬å›é€šé“
            bm25_results = []
            if config.bm25_enabled:
                self.logger.info("\n" + "=" * 80)
                self.logger.info("ã€BM25ã€‘æ‰§è¡ŒBM25æœç´¢å¬å›...")
                self.logger.info("=" * 80)

                from dataflow.modules.search.bm25 import BM25Searcher
                bm25_searcher = BM25Searcher()
                bm25_config = BM25Config(
                    top_k=config.bm25_top_k,
                    title_weight=config.bm25_title_weight,
                    content_weight=config.bm25_content_weight
                )

                bm25_chunks = await bm25_searcher.search_chunks(
                    query=config.query,
                    source_config_ids=config.get_source_config_ids(),
                    config=bm25_config
                )

                # å°† BM25 ç»“æœè½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
                for idx, chunk in enumerate(bm25_chunks):
                    bm25_results.append({
                        "chunk_id": chunk["chunk_id"],
                        "chunk": chunk,  # å®Œæ•´çš„ chunk ä¿¡æ¯
                        "heading": chunk["heading"],
                        "content": chunk["content"],
                        "score": chunk["score"],  # BM25 åˆ†æ•°
                        "source": "bm25",  # æ ‡è®°æ¥æºä¸º bm25
                        "clues": [],  # BM25 å¬å›çš„æ®µè½æ²¡æœ‰å®ä½“çº¿ç´¢
                        "bm25_rank": idx + 1,  # è®°å½• BM25 æ’å
                    })

                self.logger.info(f"âœ“ BM25 å¬å›å®Œæˆ: {len(bm25_results)} ä¸ªæ®µè½")

            step1_time = time.perf_counter() - step1_start
            step_times['Step1æ‰§è¡Œ'] = step1_time

            # æ­¥éª¤4: è®¡ç®—æƒé‡å¹¶æ’åºï¼ˆä½¿ç”¨å¤šè·³è¡°å‡ + ç›®æ ‡ç»´åº¦åŠ æƒï¼‰- ä½¿ç”¨ç»Ÿä¸€æ–¹æ³•
            step4_start = time.perf_counter()

            # åˆå¹¶ Step1 å’Œ BM25 çš„å¬å›ç»“æœï¼ˆæŒ‰ chunk_id å»é‡åˆå¹¶ï¼‰
            self.logger.info("åˆå¹¶ Step1 å’Œ BM25 å¬å›ç»“æœï¼ˆæŒ‰ chunk_id å»é‡ï¼‰...")
            chunk_dict = {}

            # å¤„ç† step1_resultsï¼ˆentityå¬å›ï¼‰
            for result in step1_results:
                chunk_id = result.get("chunk_id")
                if not chunk_id:
                    continue

                if chunk_id not in chunk_dict:
                    chunk_dict[chunk_id] = {
                        "chunk_id": chunk_id,
                        "chunk": result.get("chunk"),
                        "heading": result.get("heading"),
                        "content": result.get("content"),
                        "score": result.get("score", 0.0),  # ç›¸ä¼¼åº¦åˆ†æ•°
                        "source": "entity",  # æ ‡è®°ä¸ºentityå¬å›
                        "clues": result.get("clues", []),
                        "recall_sources": ["entity"],  # è®°å½•æ‰€æœ‰å¬å›æ¥æº
                        "bm25_rank": None,  # é»˜è®¤æ²¡æœ‰bm25æ’å
                        "weight": result.get("weight", 0.0),
                    }

            # å¤„ç† bm25_resultsï¼Œåˆå¹¶æˆ–æ–°å¢
            for result in bm25_results:
                chunk_id = result.get("chunk_id")
                if not chunk_id:
                    continue

                if chunk_id in chunk_dict:
                    # å·²å­˜åœ¨ï¼Œåˆå¹¶bm25ä¿¡æ¯
                    chunk_dict[chunk_id]["recall_sources"].append("bm25")
                    chunk_dict[chunk_id]["bm25_rank"] = result.get("bm25_rank")
                    # æ›´æ–°sourceæ ‡è®°ä¸ºbothï¼Œè¡¨ç¤ºæ¥è‡ªå¤šä¸ªæº
                    if "entity" in chunk_dict[chunk_id]["recall_sources"]:
                        chunk_dict[chunk_id]["source"] = "both"
                else:
                    # æ–°å¢bm25å¬å›çš„æ®µè½
                    chunk_dict[chunk_id] = {
                        "chunk_id": chunk_id,
                        "chunk": result.get("chunk"),
                        "heading": result.get("heading"),
                        "content": result.get("content"),
                        "score": 0.0,  # BM25æ®µè½æ²¡æœ‰ç›¸ä¼¼åº¦åˆ†æ•°
                        "source": "bm25",  # æ ‡è®°ä¸ºbm25å¬å›
                        "clues": [],  # BM25å¬å›çš„æ®µè½æ²¡æœ‰å®ä½“çº¿ç´¢
                        "recall_sources": ["bm25"],
                        "bm25_rank": result.get("bm25_rank"),
                        "weight": 0.0,  # åˆå§‹æƒé‡ä¸º0
                    }

            # è½¬æ¢ä¸ºåˆ—è¡¨
            all_chunks = list(chunk_dict.values())

            merged_count = len(all_chunks)
            entity_only = sum(1 for c in all_chunks if c["source"] == "entity")
            bm25_only = sum(1 for c in all_chunks if c["source"] == "bm25")
            both_sources = sum(1 for c in all_chunks if c["source"] == "both")

            self.logger.info(
                f"âœ“ åˆå¹¶å®Œæˆ: å…± {merged_count} ä¸ªç‹¬ç‰¹æ®µè½ "
                f"(ä»…entity: {entity_only}, ä»…bm25: {bm25_only}, ä¸¤è€…éƒ½æœ‰: {both_sources})"
            )

            sorted_results = await self._step4_calculate_weights(
                items=all_chunks,
                key_final=key_final,
                config=config,
                item_type="æ®µè½",
                store_detailed_scores=False  # æ®µè½çº§ä¸éœ€è¦è¯¦ç»†åˆ†æ•°
            )
            step4_time = time.perf_counter() - step4_start
            step_times['Step4_æƒé‡è®¡ç®—'] = step4_time
            self.logger.info(
                f"âœ“ Step4 å®Œæˆ: è®¡ç®—å¹¶æ’åº {len(sorted_results)} ä¸ªæ®µè½, è€—æ—¶: {step4_time:.3f}ç§’")

            # ğŸ†• æ­¥éª¤5: æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ‰§è¡Œ PageRank
            if config and config.rerank.skip_pagerank:
                # âœ¨ è·³è¿‡ PageRankæ¨¡å¼
                self.logger.info("=" * 80)
                self.logger.info("ã€é…ç½®ã€‘skip_pagerank=Trueï¼Œè·³è¿‡PageRanké‡æ’åº")
                self.logger.info("å°†ç›´æ¥ä½¿ç”¨Step4æƒé‡æ’åºç»“æœè¿›è¡ŒTop-Nç­›é€‰")
                self.logger.info("=" * 80)

                step5_time = 0.0  # PageRankæœªæ‰§è¡Œ
                content_for_step6 = sorted_results  # ä½¿ç”¨Step4ç»“æœ

                # è®°å½•è·³è¿‡ç»Ÿè®¡
                self.logger.warning(
                    f"âš ï¸  å·²è·³è¿‡ PageRankï¼ˆé…ç½® skip_pagerank=Trueï¼‰, "
                    f"æ’åºè´¨é‡å¯èƒ½ä¸‹é™ï¼Œä½†é€Ÿåº¦æ›´å¿«"
                )
            else:
                # âœ¨ ä½¿ç”¨ PageRankæ¨¡å¼
                self.logger.info("=" * 80)
                self.logger.info("ã€é…ç½®ã€‘skip_pagerank=Falseï¼Œæ‰§è¡ŒPageRanké‡æ’åº")
                self.logger.info("=" * 80)

                # æ­¥éª¤5: PageRank é‡æ’åº
                step5_start = time.perf_counter()
                pagerank_results = await self._step5_pagerank_rerank(
                    weighted_contents=sorted_results,
                    config=config
                )
                step5_time = time.perf_counter() - step5_start
                step_times['Step5_PageRanké‡æ’åº'] = step5_time
                self.logger.info(
                    f"âœ“ Step5 å®Œæˆ: PageRank é‡æ’åº {len(pagerank_results)} ä¸ªæ®µè½, è€—æ—¶: {step5_time:.3f}ç§’")

                content_for_step6 = pagerank_results  # ä½¿ç”¨PageRankç»“æœ

            # æ­¥éª¤6: ç»Ÿä¸€è¿›è¡Œ Top-N ç­›é€‰ï¼ˆä¸¤ç§æ¨¡å¼éƒ½æ‰§è¡Œï¼‰
            step6_start = time.perf_counter()
            final_sections = await self._step6_get_topn_sections(
                sorted_contents=content_for_step6,  # æ ¹æ®æ¨¡å¼é€‰æ‹©çš„ç»“æœ
                top_k=config.rerank.max_results,
                config=config
            )
            step6_time = time.perf_counter() - step6_start
            step_times['Step6_Top-Nç­›é€‰'] = step6_time
            self.logger.info(f"âœ“ Step6 å®Œæˆ: æœ€ç»ˆè¿”å› {len(final_sections)} ä¸ªæ®µè½, è€—æ—¶: {step6_time:.3f}ç§’")

            # è®¡ç®—æ€»è€—æ—¶
            overall_time = time.perf_counter() - overall_start

            # è¾“å‡ºè€—æ—¶ç»Ÿè®¡æ±‡æ€»
            self.logger.info("\n" + "=" * 80)
            self.logger.info("ã€æ®µè½çº§ã€‘å„æ­¥éª¤è€—æ—¶æ±‡æ€»:")
            self.logger.info("-" * 80)
            self.logger.info(
                f"æŸ¥è¯¢å‘é‡ç”Ÿæˆ: {vector_time:.3f}ç§’ ({vector_time/overall_time*100:.1f}%)")
            for step_name, step_time in step_times.items():
                self.logger.info(
                    f"{step_name}: {step_time:.3f}ç§’ ({step_time/overall_time*100:.1f}%)")
            self.logger.info("-" * 80)
            self.logger.info(f"âœ“ æ€»è€—æ—¶: {overall_time:.3f}ç§’")
            self.logger.info("=" * 80)

            # ç›´æ¥è¿”å›æ®µè½åˆ—è¡¨
            return {"sections": final_sections}

        except Exception as e:
            self.logger.error(f"[æ®µè½çº§] æœç´¢å¤±è´¥: {e}", exc_info=True)
            return {"sections": []}  # å¤±è´¥æ—¶è¿”å›ç©ºå­—å…¸

    async def _step1_keys_to_contents(
        self,
        key_final: List[Dict[str, Any]],
        query: str,
        source_config_ids: List[str],
        query_vector: Optional[List[float]] = None,
        config: Optional[SearchConfig] = None
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤1: keyæ‰¾content

        å¤ç”¨çˆ¶ç±» _keys_to_events æ–¹æ³•è·å–äº‹ä»¶ï¼Œç„¶åè¿›ä¸€æ­¥è·å–è¿™äº›äº‹ä»¶å…³è”çš„åŸæ–‡å—å¹¶è®¡ç®—ç›¸ä¼¼åº¦ã€‚
        å¬å›è·¯å¾„ï¼šKey â†’ Entity â†’ Event â†’ Chunk

        Args:
            key_final: ä»Recallè¿”å›çš„key_finalæ•°æ®
            query: æŸ¥è¯¢æ–‡æœ¬
            source_config_ids: æ•°æ®æºé…ç½®IDåˆ—è¡¨
            query_vector: å¯é€‰çš„æŸ¥è¯¢å‘é‡
            config: æœç´¢é…ç½®

        Returns:
            ContentSearchResultå¯¹è±¡åˆ—è¡¨ï¼ŒæŒ‰ä½™å¼¦ç›¸ä¼¼åº¦é™åºæ’åº
        """
        try:
            self.logger.info(
                f"[æ®µè½çº§Step1] å¼€å§‹å¬å›: å¤„ç† {len(key_final)} ä¸ªkey, query='{query}'")

            if not key_final:
                return []

            # ğŸ†• è°ƒç”¨çˆ¶ç±»æ–¹æ³•è·å–äº‹ä»¶ï¼ˆKey â†’ Entity â†’ Eventï¼‰
            event_results = await self._keys_to_events(
                key_final=key_final,
                query=query,
                source_config_ids=source_config_ids,
                query_vector=query_vector,
                config=config
            )

            if not event_results:
                self.logger.warning("æœªæ‰¾åˆ°ç›¸å…³äº‹ä»¶")
                return []

            self.logger.info(f"[æ®µè½çº§Step1] æ‰¾åˆ° {len(event_results)} ä¸ªç›¸å…³äº‹ä»¶")

            # æå–äº‹ä»¶IDåˆ—è¡¨å’Œ event_to_entities æ˜ å°„
            event_ids = [result["event_id"] for result in event_results]

            # ä»äº‹ä»¶ä¸­æå– event_to_entities æ˜ å°„
            event_to_entities = {}
            for result in event_results:
                event_id = result["event_id"]
                # ä» clues ä¸­æå–å®ä½“ID
                entity_ids = [clue["id"] for clue in result.get("clues", [])]
                event_to_entities[event_id] = entity_ids

            async with self.session_factory() as session:
                # 1. è·å–äº‹ä»¶è¯¦æƒ…å¹¶æå– chunk_ids
                from sqlalchemy import select, and_
                from dataflow.db import SourceEvent

                event_detail_query = (
                    select(SourceEvent)
                    .where(
                        and_(
                            SourceEvent.source_config_id.in_(source_config_ids),
                            SourceEvent.id.in_(event_ids)
                        )
                    )
                )

                event_detail_result = await session.execute(event_detail_query)
                events = event_detail_result.scalars().all()

                # æ”¶é›† chunk_ids å’Œå»ºç«‹æ˜ å°„
                chunk_ids = set()
                event_to_chunk = {}
                event_title_map = {}

                for event in events:
                    if event.chunk_id:
                        event_to_chunk[event.id] = event.chunk_id
                        chunk_ids.add(event.chunk_id)
                        event_title_map[event.id] = event.title

                if not chunk_ids:
                    self.logger.warning("æ‰€æœ‰äº‹ä»¶éƒ½æ²¡æœ‰å…³è”åˆ°åŸæ–‡å—")
                    return []

                self.logger.info(
                    f"æ”¶é›†åˆ° {len(chunk_ids)} ä¸ªåŸæ–‡å—IDï¼ˆæ¥è‡ª {len(events)} ä¸ªäº‹ä»¶ï¼‰")

                # 2. è·å–åŸæ–‡å—æ•°æ®
                chunk_query = (
                    select(SourceChunk)
                    .where(
                        and_(
                            SourceChunk.source_config_id.in_(source_config_ids),
                            SourceChunk.id.in_(list(chunk_ids))
                        )
                    )
                    .order_by(SourceChunk.rank)
                )

                chunk_result = await session.execute(chunk_query)
                chunks = chunk_result.scalars().all()

                if not chunks:
                    self.logger.warning("æœªæ‰¾åˆ°ç›¸å…³åŸæ–‡å—")
                    return []

                self.logger.info(f"ä» MySQL æ‰¾åˆ° {len(chunks)} ä¸ªåŸæ–‡å—")

                # 3. æ„å»º chunk_to_events åå‘æ˜ å°„
                chunk_to_events = {}
                for event_id, chunk_id in event_to_chunk.items():
                    if chunk_id not in chunk_to_events:
                        chunk_to_events[chunk_id] = []
                    chunk_to_events[chunk_id].append(event_id)

                # 4. ä¸ºæ¯ä¸ª chunk æ„å»ºæ•°æ®
                from dataflow.modules.search.ranking.base_pagerank import ContentSearchResult

                content_results = []

                for chunk in chunks:
                    chunk_id = chunk.id

                    # è·å–å…³è”çš„äº‹ä»¶IDåˆ—è¡¨
                    related_event_ids = chunk_to_events.get(chunk_id, [])
                    if not related_event_ids:
                        self.logger.warning(f"åŸæ–‡å— {chunk_id[:8]}... æ²¡æœ‰æ‰¾åˆ°å…³è”çš„äº‹ä»¶")
                        continue

                    # æ”¶é›†æ‰€æœ‰å¬å›è¯¥ chunk çš„å®ä½“ï¼ˆcluesï¼‰
                    chunk_clues = []
                    seen_entity_ids = set()

                    for event_id in related_event_ids:
                        # è·å–è¯¥äº‹ä»¶å…³è”çš„æ‰€æœ‰å®ä½“
                        entity_ids_in_event = event_to_entities.get(event_id, [])

                        # ä» key_final ä¸­æŸ¥æ‰¾è¿™äº›å®ä½“çš„ä¿¡æ¯
                        for entity_id in entity_ids_in_event:
                            if entity_id not in seen_entity_ids:
                                # æŸ¥æ‰¾å¯¹åº”çš„ key
                                key_info = next(
                                    (k for k in key_final
                                     if ((k.get("key_id") or k.get("id")) == entity_id)),
                                    None
                                )
                                if key_info:
                                    chunk_clues.append(key_info)
                                    seen_entity_ids.add(entity_id)

                    # åˆ›å»º ContentSearchResult
                    result = ContentSearchResult(
                        search_type=f"SQL-1",
                        source_config_id=source_config_ids[0] if source_config_ids else "",
                        source_id=chunk.source_id,
                        chunk_id=chunk_id,
                        rank=chunk.rank,
                        heading=chunk.heading,
                        content=chunk.content,
                        score=0.0,  # å°†åœ¨åç»­è®¡ç®—
                        event_ids=related_event_ids,
                        clues=chunk_clues,
                    )
                    content_results.append(result)

                self.logger.info(f"æ„å»ºäº† {len(content_results)} ä¸ªåŸæ–‡å—å¯¹è±¡")

                # 5. ä» ES è·å–é¢„å­˜å‘é‡
                chunk_ids_list = [c.chunk_id for c in content_results]
                es_chunks_data = await self.content_repo.get_chunks_by_ids(
                    chunk_ids=chunk_ids_list,
                    include_vectors=True
                )

                chunk_vector_map = {
                    es_chunk.get('chunk_id'): es_chunk.get('content_vector')
                    for es_chunk in es_chunks_data
                    if es_chunk.get('chunk_id') and es_chunk.get('content_vector')
                }

                self.logger.info(
                    f"ä» ES è·å–åˆ° {len(chunk_vector_map)}/{len(chunk_ids_list)} ä¸ªåŸæ–‡å—å‘é‡"
                )

                # 6. è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†
                for result in content_results:
                    chunk_id = result.chunk_id
                    vector = chunk_vector_map.get(chunk_id)

                    if vector:
                        try:
                            query_np = np.array(query_vector, dtype=np.float32)
                            chunk_np = np.array(vector, dtype=np.float32)
                            cosine_score = float(
                                np.dot(query_np, chunk_np) /
                                (np.linalg.norm(query_np) * np.linalg.norm(chunk_np))
                            )
                            result.score = cosine_score
                        except Exception as e:
                            self.logger.warning(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
                            result.score = 0.0
                    else:
                        result.score = 0.0

                # 7. æŒ‰å¾—åˆ†æ’åº
                content_results.sort(key=lambda x: x.score, reverse=True)

                # 8. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
                if content_results:
                    self.logger.info(
                        f"ğŸ“Š æ®µè½å¬å›å®Œæˆ: å…± {len(content_results)} ä¸ªæ®µè½, "
                        f"ç›¸ä¼¼åº¦èŒƒå›´: {min(r.score for r in content_results):.4f} ~ "
                        f"{max(r.score for r in content_results):.4f}"
                    )

                # ğŸ†• å°†å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸è¿”å›
                results = []
                for r in content_results:
                    result_dict = {
                        "search_type": r.search_type,
                        "source_config_id": r.source_config_id,
                        "source_id": r.source_id,
                        "chunk_id": r.chunk_id,
                        "rank": r.rank,
                        "heading": r.heading,
                        "content": r.content,
                        "score": r.score,
                        "weight": r.weight,
                        "event_ids": r.event_ids,
                        "event": r.event,
                        "clues": r.clues
                    }
                    # æ·»åŠ ç»Ÿä¸€å­—æ®µ
                    result_dict["id"] = r.chunk_id  # ç»Ÿä¸€IDå­—æ®µ
                    result_dict["text"] = f"{r.heading} {r.content}"  # ç»Ÿä¸€æ–‡æœ¬å­—æ®µ
                    results.append(result_dict)
                return results

        except Exception as e:
            self.logger.error(f"[æ®µè½çº§Step1] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    async def _step5_pagerank_rerank(
        self,
        weighted_contents: List[Dict[str, Any]],
        config: Optional[SearchConfig] = None
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤5ï¼ˆæ®µè½çº§ï¼‰: ä½¿ç”¨ PageRank ç®—æ³•é‡æ’åº

        åŸºäºæ®µè½é—´çš„å…±åŒå®ä½“æ„å»ºå…³ç³»å›¾ï¼Œä½¿ç”¨ PageRank ç®—æ³•é‡æ–°æ’åºæ®µè½ã€‚

        å›¾æ„å»ºè§„åˆ™ï¼š
        - èŠ‚ç‚¹ï¼šæ¯ä¸ªæ®µè½
        - è¾¹ï¼šå¦‚æœä¸¤ä¸ªæ®µè½æœ‰å…±åŒçš„ keyï¼ˆå®ä½“ï¼‰ï¼Œåˆ™å®ƒä»¬ä¹‹é—´æœ‰è¿æ¥
        - è¾¹æƒé‡ï¼šå…±åŒ key çš„æƒé‡ç´¯åŠ 
        - åˆå§‹ PageRank å€¼ï¼šä½¿ç”¨ Step4 è®¡ç®—çš„æƒé‡

        Args:
            weighted_contents: ä» step4 è¿”å›çš„æ®µè½åˆ—è¡¨ï¼ˆå·²åŒ…å« weight å­—æ®µï¼‰
            config: æœç´¢é…ç½®

        Returns:
            æŒ‰ PageRank å¾—åˆ†é™åºæ’åºçš„æ®µè½åˆ—è¡¨
        """
        try:
            n = len(weighted_contents)

            if n == 0:
                self.logger.warning("[Step5] è¾“å…¥æ®µè½ä¸ºç©º")
                return []

            if n == 1:
                self.logger.info("[Step5] åªæœ‰1ä¸ªæ®µè½ï¼Œè·³è¿‡ PageRank")
                return weighted_contents

            self.logger.info("=" * 80)
            self.logger.info(f"[Step5] PageRank é‡æ’åºå¼€å§‹ï¼Œå…± {n} ä¸ªæ®µè½")
            self.logger.info("-" * 80)

            # 1. æ„å»ºæ®µè½ç´¢å¼•æ˜ å°„
            chunk_id_to_idx = {
                content["chunk_id"]: idx
                for idx, content in enumerate(weighted_contents)
            }

            # 2. æ„å»ºæ®µè½å…³ç³»å›¾ï¼ˆåŸºäºå…±åŒå®ä½“ï¼‰
            self.logger.info("[Step5] æ­£åœ¨æ„å»ºæ®µè½å…³ç³»å›¾...")
            graph = self._build_section_graph(weighted_contents, chunk_id_to_idx)

            # ç»Ÿè®¡å›¾ä¿¡æ¯
            total_edges = sum(len(edges) for edges in graph.values())
            self.logger.info(
                f"âœ“ å…³ç³»å›¾æ„å»ºå®Œæˆ: {n} ä¸ªèŠ‚ç‚¹, {total_edges} æ¡è¾¹"
            )

            # 3. å‡†å¤‡åˆå§‹ PageRank å€¼ï¼ˆä½¿ç”¨ Step4 çš„æƒé‡ï¼‰
            initial_weights = np.array([
                content.get("weight", 0.0)
                for content in weighted_contents
            ])

            self.logger.info(
                f"åˆå§‹æƒé‡ç»Ÿè®¡: min={initial_weights.min():.4f}, "
                f"max={initial_weights.max():.4f}, "
                f"mean={initial_weights.mean():.4f}"
            )

            # ä½¿ç”¨åŸºç±»æ–¹æ³•åˆå§‹åŒ– PageRank å€¼
            initial_pagerank = self._initialize_pagerank_values(initial_weights)

            # 4. æ‰§è¡Œ PageRank è¿­ä»£
            self.logger.info("[Step5] å¼€å§‹ PageRank è¿­ä»£è®¡ç®—...")
            final_pagerank = self._execute_pagerank_iteration(
                graph=graph,
                initial_pagerank=initial_pagerank,
                damping=0.85,
                max_iterations=100,
                tolerance=1e-6
            )

            self.logger.info(
                f"âœ“ PageRank è®¡ç®—å®Œæˆ: min={final_pagerank.min():.6f}, "
                f"max={final_pagerank.max():.6f}, "
                f"mean={final_pagerank.mean():.6f}"
            )

            # 5. å°† PageRank å¾—åˆ†èµ‹å€¼ç»™æ®µè½
            # ğŸ†• è®¡ç®— PageRank çš„ç¼©æ”¾å› å­ï¼Œä½¿å…¶ä¸åŸå§‹æƒé‡å¤„äºç›¸åŒé‡çº§
            max_original_weight = max(c.get("weight", 0.0) for c in weighted_contents)
            max_pagerank = float(final_pagerank.max()) if final_pagerank.max() > 0 else 1.0
            pagerank_scale = max_original_weight / max_pagerank if max_pagerank > 0 else 1.0
            
            for idx, content in enumerate(weighted_contents):
                raw_pagerank = float(final_pagerank[idx])
                scaled_pagerank = raw_pagerank * pagerank_scale  # ç¼©æ”¾åˆ°ä¸åŸå§‹æƒé‡ç›¸åŒé‡çº§
                
                content["pagerank_score"] = raw_pagerank
                content["scaled_pagerank"] = scaled_pagerank
                content.setdefault("original_weight", content.get("weight", 0.0))
                
                # æ··åˆï¼šä»¥ RRF æƒé‡ä¸ºä¸»ï¼ŒPageRank ä½œä¸º 20% å¾®è°ƒ
                content["weight"] = 0.8 * content["original_weight"] + 0.2 * scaled_pagerank
                content["score"] = content["weight"]  # åŒæ­¥æ›´æ–° score ç”¨äºè¿”å›

            # 6. ğŸ†• æŒ‰æ··åˆåçš„ weight é‡æ–°æ’åºï¼ˆè€Œä¸æ˜¯ pagerank_scoreï¼‰
            sorted_contents = sorted(
                weighted_contents,
                key=lambda x: x["weight"],
                reverse=True
            )

            # 7. æ˜¾ç¤º Top-10 æ®µè½çš„ PageRank å˜åŒ–
            self.logger.info("=" * 80)
            self.logger.info("[Step5] Top-10 æ®µè½ PageRank å¾—åˆ†:")
            self.logger.info("-" * 80)

            for rank, content in enumerate(sorted_contents[:10], 1):
                chunk_id = content.get("chunk_id", "")[:8]
                heading = content.get("heading", "")[:40]
                original_weight = content.get("original_weight", 0.0)
                pagerank_score = content.get("pagerank_score", 0.0)

                self.logger.info(
                    f"Rank {rank}: {chunk_id}... | "
                    f"åŸå§‹æƒé‡={original_weight:.4f} â†’ "
                    f"PageRank={pagerank_score:.6f} | "
                    f"æ ‡é¢˜: {heading}"
                )

            if len(sorted_contents) > 10:
                self.logger.info(f"... (è¿˜æœ‰ {len(sorted_contents) - 10} ä¸ªæ®µè½æœªæ˜¾ç¤º)")

            self.logger.info("=" * 80)
            self.logger.info(f"âœ“ [Step5] PageRank é‡æ’åºå®Œæˆï¼Œè¿”å› {len(sorted_contents)} ä¸ªæ®µè½")

            return sorted_contents

        except Exception as e:
            self.logger.error(f"[Step5] PageRank é‡æ’åºå¤±è´¥: {e}", exc_info=True)
            # å¤±è´¥æ—¶è¿”å›åŸå§‹æ’åº
            return weighted_contents

    def _build_section_graph(
        self,
        contents: List[Dict[str, Any]],
        chunk_id_to_idx: Dict[str, int]
    ) -> Dict[int, List[Tuple[int, float]]]:
        """
        æ„å»ºæ®µè½å…³ç³»å›¾ï¼ˆè°ƒç”¨åŸºç±»çš„ç»Ÿä¸€æ–¹æ³•ï¼‰

        è§„åˆ™ï¼š
        - å¦‚æœä¸¤ä¸ªæ®µè½æœ‰å…±åŒçš„å®ä½“ï¼ˆkeyï¼‰ï¼Œåˆ™å®ƒä»¬ä¹‹é—´æœ‰è¿æ¥
        - è¾¹æƒé‡ = å…±åŒå®ä½“çš„æƒé‡ç´¯åŠ 
        - æ— å‘å›¾ï¼ˆåŒå‘è¾¹ï¼‰

        Args:
            contents: æ®µè½åˆ—è¡¨
            chunk_id_to_idx: chunk_id åˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆæœªä½¿ç”¨ï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰

        Returns:
            é‚»æ¥è¡¨ {node_idx: [(target_idx, weight), ...]}
        """
        # ç›´æ¥è°ƒç”¨åŸºç±»çš„ç»Ÿä¸€æ— å‘å›¾æ„å»ºæ–¹æ³•
        return self.build_undirected_graph_from_entities(
            items=contents,
            item_type="æ®µè½"
        )

    async def _step6_get_topn_sections(
        self,
        sorted_contents: List[Dict[str, Any]],
        top_k: int,
        config: Optional[SearchConfig] = None
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤6: å–Top-Næ®µè½å¹¶è¿”å›

        å¤„ç†æµç¨‹ï¼š
        1. å–Top-kï¼šä»æ’åºåçš„ç»“æœä¸­å–å‰ k ä¸ªæ®µè½
        2. ç›´æ¥è¿”å›è¿™äº›æ®µè½

        Args:
            sorted_contents: ä»step4æ’åºåçš„æ®µè½åˆ—è¡¨ï¼ˆå·²æŒ‰æƒé‡é™åºæ’åºï¼‰
            top_k: å–å‰kä¸ªç»“æœ
            config: æœç´¢é…ç½®

        Returns:
            List[Dict[str, Any]]: æ®µè½åˆ—è¡¨ï¼Œæ¯ä¸ªæ®µè½åŒ…å«ï¼š
                - chunk_id: åŸæ–‡å—ID
                - heading: æ®µè½æ ‡é¢˜
                - content: æ®µè½å†…å®¹
                - weight: æƒé‡å¾—åˆ†
                - clues: çº¿ç´¢åˆ—è¡¨ï¼ˆå¬å›è¯¥æ®µè½çš„å®ä½“ï¼‰
        """
        try:
            self.logger.info(
                f"[æ®µè½çº§Step6] å¼€å§‹: ä» {len(sorted_contents)} ä¸ªæ®µè½ä¸­å–Top-{top_k}")

            # 1. å–Top-kæ®µè½
            topk_sections = sorted_contents[:top_k]
            self.logger.info(f"âœ“ [æ®µè½çº§Step6] æå–äº†Top-{len(topk_sections)}ä¸ªæ®µè½")

            # 2. æ˜¾ç¤ºTop-10æ®µè½ä¿¡æ¯
            self.logger.info("=" * 80)
            self.logger.info(
                f"ã€æ®µè½çº§Step6ã€‘Top-{min(len(topk_sections), 10)}æ®µè½è¯¦æƒ…:")
            self.logger.info("-" * 80)

            for idx, section in enumerate(topk_sections[:10], 1):
                heading = section["heading"][:50] if section["heading"] else ""
                weight = section["weight"]
                chunk_id = section["chunk_id"][:8]

                self.logger.info(
                    f"  æ®µè½{idx}: {chunk_id}... | W={weight:.4f} | '{heading}'"
                )

            if len(topk_sections) > 10:
                self.logger.info(
                    f"  ... (è¿˜æœ‰ {len(topk_sections) - 10} ä¸ªæ®µè½æœªæ˜¾ç¤º)")

            self.logger.info("=" * 80)
            self.logger.info(f"âœ“ [æ®µè½çº§Step6] å®Œæˆ: è¿”å› {len(topk_sections)} ä¸ªæ®µè½")

            return topk_sections

        except Exception as e:
            self.logger.error(f"[æ®µè½çº§Step6] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []  # å¤±è´¥æ—¶è¿”å›ç©ºåˆ—è¡¨
