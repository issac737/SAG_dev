"""
PageRank æœç´¢å™¨åŸºç±»

æä¾›äº‹é¡¹çº§å’Œæ®µè½çº§ PageRank çš„å…±åŒé€»è¾‘ï¼ŒåŒ…æ‹¬ï¼š
- å‘é‡ç”Ÿæˆå’Œç›¸ä¼¼åº¦è®¡ç®—
- ES æœç´¢æ¥å£
- PageRank è¿­ä»£è®¡ç®—
- å“åº”æ„å»º

å­ç±»å¯ä»¥ä½¿ç”¨è¿™äº›é€šç”¨å·¥å…·æ–¹æ³•ï¼Œå¹¶å®ç°è‡ªå·±çš„æœç´¢æµç¨‹ã€‚
"""

from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass
import numpy as np
import math
import time

from dataflow.core.storage.elasticsearch import get_es_client
from dataflow.core.storage.repositories.source_chunk_repository import SourceChunkRepository
from dataflow.core.storage.repositories.event_repository import EventVectorRepository
from dataflow.db import get_session_factory
from dataflow.exceptions import AIError
from dataflow.modules.load.processor import DocumentProcessor
from dataflow.modules.search.config import SearchConfig
from dataflow.utils import get_logger


@dataclass
class ContentSearchResult:
    """
    æœç´¢ç»“æœçš„ç»Ÿä¸€è¿”å›æ ¼å¼ï¼ˆSourceChunkæ¶æ„ï¼‰

    ç”¨äºè¡¨ç¤ºä»SQLæ•°æ®åº“æˆ–Embeddingå‘é‡æ•°æ®åº“æœç´¢åˆ°çš„å†…å®¹
    """
    # å¿…éœ€å­—æ®µ
    search_type: str      # "sql", "embedding" æˆ–å¸¦ç¼–å·çš„æ ¼å¼å¦‚ "SQL-1", "embedding-2"
    source_config_id: str # æ•°æ®æºé…ç½®ID (UUID)
    source_id: str        # æ–‡ç« ID (Article.id æˆ– SourceChunk.source_id)
    chunk_id: str         # åŸæ–‡å—ID (SourceChunk.id)
    rank: int             # åŸæ–‡å—åœ¨æ–‡ç« ä¸­çš„æ’åº
    heading: str          # åŸæ–‡å—æ ‡é¢˜
    content: str          # åŸæ–‡å—å†…å®¹
    score: float = 0.0    # ç›¸å…³æ€§å¾—åˆ†
    weight: float = 0.0   # æƒé‡å€¼ï¼ˆstep4è®¡ç®—åèµ‹å€¼ï¼‰
    event_ids: List[str] = None  # å…³è”çš„äº‹ä»¶IDåˆ—è¡¨
    event: str = ""  # èšåˆåçš„äº‹é¡¹æ‘˜è¦ï¼ˆå¤šä¸ªsummaryåˆå¹¶ï¼‰
    clues: List[Dict[str, Any]] = None  # å¬å›è¯¥æ®µè½çš„çº¿ç´¢åˆ—è¡¨ï¼ˆæ¥è‡ª key_final æˆ– queryï¼‰

    def __post_init__(self):
        """åˆå§‹åŒ–åéªŒè¯"""
        # åˆå§‹åŒ– event_ids ä¸ºç©ºåˆ—è¡¨
        if self.event_ids is None:
            self.event_ids = []

        # åˆå§‹åŒ– clues ä¸ºç©ºåˆ—è¡¨
        if self.clues is None:
            self.clues = []

        # å…è®¸ "sql", "embedding" æˆ–å¸¦ç¼–å·çš„æ ¼å¼å¦‚ "SQL-1", "embedding-2"
        valid_types = ["sql", "embedding"]
        is_valid = (
            self.search_type in valid_types or
            self.search_type.startswith("SQL-") or
            self.search_type.startswith("embedding-")
        )

        if not is_valid:
            raise ValueError(
                f"search_type å¿…é¡»æ˜¯ 'sql', 'embedding' æˆ–å¸¦ç¼–å·æ ¼å¼(å¦‚ 'SQL-1', 'embedding-1')ï¼Œ"
                f"å½“å‰å€¼: {self.search_type}"
            )

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "search_type": self.search_type,
            "source_config_id": self.source_config_id,
            "source_id": self.source_id,
            "chunk_id": self.chunk_id,
            "rank": self.rank,
            "heading": self.heading,
            "content": self.content,
            "score": self.score,
            "weight": self.weight,
            "event_ids": self.event_ids,
            "event": self.event,
            "clues": self.clues,
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ContentSearchResult":
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(
            search_type=data.get("search_type", "sql"),
            source_config_id=data["source_config_id"],
            source_id=data["source_id"],
            chunk_id=data["chunk_id"],
            rank=data.get("rank", 0),
            heading=data.get("heading", ""),
            content=data.get("content", ""),
            score=data.get("score", 0.0),
            weight=data.get("weight", 0.0),
            event_ids=data.get("event_ids", []),
            event=data.get("event", ""),
            clues=data.get("clues", []),
        )

    def __repr__(self) -> str:
        """å­—ç¬¦ä¸²è¡¨ç¤º"""
        return (
            f"ContentSearchResult(type={self.search_type}, "
            f"chunk_id={self.chunk_id}, "
            f"heading='{self.heading[:30]}...', "
            f"score={self.score:.3f})"
        )


class BasePageRankSearcher:
    """PageRank æœç´¢å™¨åŸºç±»"""

    def __init__(self, llm_client=None):
        """
        åˆå§‹åŒ–æœç´¢å™¨

        Args:
            llm_client: LLMå®¢æˆ·ç«¯ï¼ˆå¯é€‰ï¼‰
        """
        self.session_factory = get_session_factory()
        self.logger = get_logger("search.rerank.pagerank")

        # åˆå§‹åŒ– ES å®¢æˆ·ç«¯å’Œä»“åº“
        self.es_client = get_es_client()
        self.content_repo = SourceChunkRepository(self.es_client)
        self.event_repo = EventVectorRepository(self.es_client)

        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        self.processor = DocumentProcessor(llm_client=llm_client)

        self.logger.info(
            f"{self.__class__.__name__} åˆå§‹åŒ–å®Œæˆ",
            extra={"embedding_model": self.processor.embedding_model_name}
        )

    # ==================== å…·ä½“å®ç°æ–¹æ³•ï¼ˆå­ç±»å¯ç›´æ¥ä½¿ç”¨ï¼‰ ====================

    async def _generate_query_vector(
        self,
        query: str,
        config: Optional[SearchConfig] = None
    ) -> List[float]:
        """
        ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆæ”¯æŒç¼“å­˜ï¼‰

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            config: æœç´¢é…ç½®ï¼ˆå¯é€‰ï¼Œç”¨äºç¼“å­˜ï¼‰

        Returns:
            æŸ¥è¯¢å‘é‡
        """
        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„ query_embedding
            if config and config.has_query_embedding and config.query_embedding:
                query_embedding = config.query_embedding
                self.logger.debug(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œé•¿åº¦: {len(query_embedding)}")
                return query_embedding

            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = await self.processor.generate_embedding(query)
            self.logger.debug(f"âœ¨ ç”Ÿæˆqueryå‘é‡æˆåŠŸï¼Œé•¿åº¦: {len(query_embedding)}")

            # ç¼“å­˜åˆ° config
            if config:
                config.query_embedding = query_embedding
                config.has_query_embedding = True
                self.logger.debug("ğŸ“¦ Queryå‘é‡å·²ç¼“å­˜åˆ°config")

            return query_embedding

        except Exception as e:
            raise AIError(f"æŸ¥è¯¢å‘é‡ç”Ÿæˆå¤±è´¥: {e}") from e

    async def _cosine_similarity(
        self,
        vec1: List[float],
        vec2: List[float]
    ) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦ [0, 1]
        """
        if not vec1 or not vec2:
            return 0.0

        try:
            v1 = np.array(vec1, dtype=np.float32)
            v2 = np.array(vec2, dtype=np.float32)

            if len(v1) != len(v2):
                self.logger.warning(f"å‘é‡é•¿åº¦ä¸ä¸€è‡´: {len(v1)} vs {len(v2)}")
                return 0.0

            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            similarity = dot_product / (norm1 * norm2)
            return max(0.0, min(1.0, float(similarity)))

        except Exception as e:
            self.logger.warning(f"ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    async def _calculate_cosine_scores(
        self,
        query_vector: List[float],
        items: List[Dict[str, Any]]
    ) -> Dict[str, float]:
        """
        æ‰¹é‡è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆnumpy å‘é‡åŒ–ä¼˜åŒ–ï¼‰

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            items: é¡¹ç›®åˆ—è¡¨ï¼ˆå¿…é¡»åŒ…å« 'vector' æˆ– 'content_vector' å­—æ®µï¼‰

        Returns:
            {item_id: similarity_score}
        """
        scores = {}
        
        # æ”¶é›†æœ‰æ•ˆçš„ item_id å’Œå‘é‡
        valid_ids = []
        valid_vectors = []
        
        for item in items:
            item_id = item.get("id") or item.get("event_id") or item.get("chunk_id")
            if not item_id:
                continue

            # è·å–å‘é‡ï¼ˆåªä½¿ç”¨ content_vectorï¼‰
            vector = item.get("content_vector")

            if not vector:
                self.logger.debug(f"é¡¹ç›® {item_id[:8]}... ç¼ºå°‘å‘é‡")
                continue
            
            valid_ids.append(item_id)
            valid_vectors.append(vector)
        
        if not valid_vectors:
            return scores
        
        try:
            # æ‰¹é‡è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            query_vec = np.array(query_vector, dtype=np.float32)
            item_matrix = np.array(valid_vectors, dtype=np.float32)  # shape: (n, dim)
            
            # è®¡ç®— query èŒƒæ•°
            query_norm = np.linalg.norm(query_vec)
            if query_norm == 0:
                return scores
            
            # æ‰¹é‡è®¡ç®—ç‚¹ç§¯å’ŒèŒƒæ•°
            dot_products = item_matrix @ query_vec  # shape: (n,)
            item_norms = np.linalg.norm(item_matrix, axis=1)  # shape: (n,)
            
            # é¿å…é™¤é›¶
            valid_norms = item_norms > 0
            similarities = np.zeros(len(valid_ids), dtype=np.float32)
            similarities[valid_norms] = dot_products[valid_norms] / (item_norms[valid_norms] * query_norm)
            
            # Clip to [0, 1]
            similarities = np.clip(similarities, 0.0, 1.0)
            
            # æ„å»ºç»“æœå­—å…¸
            for i, item_id in enumerate(valid_ids):
                scores[item_id] = float(similarities[i])
                
        except Exception as e:
            self.logger.warning(f"æ‰¹é‡ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥ï¼Œå›é€€é€ä¸ªè®¡ç®—: {e}")
            # å›é€€åˆ°é€ä¸ªè®¡ç®—
            for item_id, vector in zip(valid_ids, valid_vectors):
                similarity = await self._cosine_similarity(query_vector, vector)
                scores[item_id] = similarity

        return scores

    async def _search_similar_items_from_es(
        self,
        query_vector: List[float],
        source_config_ids: List[str],
        k: int,
        index_name: str
    ) -> List[Dict[str, Any]]:
        """
        ä» ES æœç´¢ç›¸ä¼¼é¡¹ç›®ï¼ˆKNNï¼‰

        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            source_config_ids: æ•°æ®æºIDåˆ—è¡¨
            k: è¿”å›æ•°é‡
            index_name: ESç´¢å¼•åç§°

        Returns:
            ç›¸ä¼¼é¡¹ç›®åˆ—è¡¨
        """
        all_results = []

        for source_config_id in source_config_ids:
            try:
                # æ ¹æ®ç´¢å¼•ç±»å‹é€‰æ‹©ä¸åŒçš„ä»“åº“å’Œæ–¹æ³•
                if "chunk" in index_name.lower():
                    # æ®µè½æœç´¢ï¼šä½¿ç”¨ search_similar_by_content
                    results = await self.content_repo.search_similar_by_content(
                        query_vector=query_vector,
                        source_config_id=source_config_id,
                        k=k
                    )
                else:
                    # äº‹é¡¹æœç´¢ï¼šä½¿ç”¨ search_by_vector
                    results = await self.event_repo.search_by_vector(
                        vector=query_vector,
                        source_config_id=source_config_id,
                        top_k=k
                    )

                all_results.extend(results)

            except Exception as e:
                self.logger.warning(
                    f"ESæœç´¢å¤±è´¥ (source_config_id={source_config_id}): {e}"
                )
                continue

        return all_results

    def _initialize_pagerank_values(
        self,
        weights: np.ndarray
    ) -> np.ndarray:
        """
        åˆå§‹åŒ– PageRank å€¼

        å°†æƒé‡æ•°ç»„å½’ä¸€åŒ–ä½œä¸ºåˆå§‹ PageRank å€¼ã€‚å¦‚æœæ‰€æœ‰æƒé‡ä¸º0ï¼Œåˆ™ä½¿ç”¨å‡åŒ€åˆ†å¸ƒã€‚

        Args:
            weights: æƒé‡æ•°ç»„

        Returns:
            å½’ä¸€åŒ–çš„ PageRank åˆå§‹å€¼æ•°ç»„
        """
        n = len(weights)

        if weights.sum() > 0:
            pagerank = weights / weights.sum()  # å½’ä¸€åŒ–
            self.logger.debug(f"ä½¿ç”¨æƒé‡å½’ä¸€åŒ–ä½œä¸ºåˆå§‹PageRankå€¼")
        else:
            pagerank = np.ones(n) / n  # å‡åŒ€åˆ†å¸ƒ
            self.logger.warning(f"æ‰€æœ‰æƒé‡ä¸º0ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒä½œä¸ºåˆå§‹PageRankå€¼")

        return pagerank

    def _execute_pagerank_iteration(
        self,
        graph: Dict[int, List[Tuple[int, float]]],
        initial_pagerank: np.ndarray,
        damping: float = 0.85,
        max_iterations: int = 100,
        tolerance: float = 1e-6
    ) -> np.ndarray:
        """
        æ‰§è¡Œ PageRank è¿­ä»£è®¡ç®—

        Args:
            graph: é‚»æ¥è¡¨ {node_idx: [(target_idx, weight), ...]}
            initial_pagerank: åˆå§‹ PageRank å€¼
            damping: é˜»å°¼ç³»æ•°
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
            tolerance: æ”¶æ•›å®¹å·®

        Returns:
            PageRank å€¼æ•°ç»„
        """
        n = len(initial_pagerank)
        pagerank = initial_pagerank.copy()

        # é¢„è®¡ç®—å‡ºè¾¹æƒé‡å’Œ
        out_weights = {}
        for j in range(n):
            edges = graph.get(j, [])
            out_weights[j] = sum(w for _, w in edges) if edges else 0.0

        # è¿­ä»£è®¡ç®—
        for iteration in range(max_iterations):
            new_pagerank = np.ones(n) * (1 - damping) / n

            for j in range(n):
                if pagerank[j] == 0 or out_weights[j] == 0:
                    continue

                contribution_per_weight = damping * pagerank[j] / out_weights[j]

                for target_idx, edge_weight in graph.get(j, []):
                    new_pagerank[target_idx] += contribution_per_weight * edge_weight

            # æ£€æŸ¥æ”¶æ•›
            diff = np.abs(new_pagerank - pagerank).sum()
            if diff < tolerance:
                self.logger.info(f"âœ“ PageRankæ”¶æ•›äºç¬¬{iteration + 1}æ¬¡è¿­ä»£ (diff={diff:.6f})")
                return new_pagerank

            pagerank = new_pagerank

        self.logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°{max_iterations}ï¼Œæœªå®Œå…¨æ”¶æ•›")
        return pagerank

    def build_undirected_graph_from_entities(
        self,
        items: List[Dict[str, Any]],
        item_type: str = "item"
    ) -> Dict[int, List[Tuple[int, float]]]:
        """
        ç»Ÿä¸€çš„æ— å‘å›¾æ„å»ºæ–¹æ³•ï¼ˆæ®µè½çº§å’Œäº‹é¡¹çº§é€šç”¨ï¼‰

        åŸºäºå…±åŒå®ä½“æ„å»ºæ— å‘å›¾ï¼š
        - å¦‚æœä¸¤ä¸ª item æœ‰å…±åŒçš„å®ä½“ï¼ˆä» clues å­—æ®µè·å–ï¼‰ï¼Œåˆ™å»ºç«‹æ— å‘è¾¹
        - è¾¹æƒé‡ = å…±åŒå®ä½“çš„æƒé‡ç´¯åŠ çš„å¹³å‡å€¼
        - æ„å»ºåŒå‘è¾¹ï¼ˆi â†” jï¼‰

        Args:
            items: é¡¹ç›®åˆ—è¡¨ï¼ˆæ®µè½æˆ–äº‹é¡¹ï¼‰ï¼Œæ¯ä¸ªé¡¹ç›®åŒ…å«ï¼š
                - chunk_id æˆ– event_id: å”¯ä¸€æ ‡è¯†
                - clues: å®ä½“åˆ—è¡¨ï¼Œæ¯ä¸ªå®ä½“åŒ…å«ï¼š
                    - id æˆ– key_id: å®ä½“ID
                    - weight: å®ä½“æƒé‡
            item_type: é¡¹ç›®ç±»å‹ï¼Œç”¨äºæ—¥å¿—æ˜¾ç¤ºï¼ˆ"æ®µè½" æˆ– "äº‹é¡¹"ï¼‰

        Returns:
            é‚»æ¥è¡¨ {node_idx: [(target_idx, weight), ...]}

        ç¤ºä¾‹:
            items = [
                {
                    "chunk_id": "chunk_1",
                    "clues": [
                        {"id": "entity_1", "weight": 0.9},
                        {"id": "entity_2", "weight": 0.7}
                    ]
                },
                {
                    "chunk_id": "chunk_2",
                    "clues": [
                        {"id": "entity_2", "weight": 0.7},
                        {"id": "entity_3", "weight": 0.5}
                    ]
                }
            ]

            # æ„å»ºå›¾:
            # chunk_1 å’Œ chunk_2 æœ‰å…±åŒå®ä½“ entity_2
            # è¾¹æƒé‡ = (0.7 + 0.7) / 2 = 0.7
            # ç»“æœ: {0: [(1, 0.7)], 1: [(0, 0.7)]}
        """
        n = len(items)

        if n == 0:
            self.logger.warning(f"[å›¾æ„å»º] è¾“å…¥{item_type}ä¸ºç©º")
            return {}

        if n == 1:
            self.logger.info(f"[å›¾æ„å»º] åªæœ‰1ä¸ª{item_type}ï¼Œè¿”å›ç©ºå›¾")
            return {0: []}

        self.logger.info(f"[å›¾æ„å»º] å¼€å§‹æ„å»ºæ— å‘å›¾: {n} ä¸ª{item_type}")

        # åˆå§‹åŒ–é‚»æ¥è¡¨
        graph = {i: [] for i in range(n)}

        # ä¸ºæ¯ä¸ªé¡¹ç›®æå–å®ä½“ä¿¡æ¯
        # ç»“æ„: [{entity_id: entity_weight, ...}, ...]
        item_entities = []
        for idx, item in enumerate(items):
            clues = item.get("clues", [])
            entity_dict = {}
            for clue in clues:
                entity_id = clue.get("id") or clue.get("key_id")
                entity_weight = clue.get("weight", 0.0)
                if entity_id:
                    entity_dict[entity_id] = entity_weight

            item_entities.append(entity_dict)

            # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºæ¯ä¸ªé¡¹ç›®çš„å®ä½“æ•°
            if idx < 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                item_id = item.get("chunk_id") or item.get("event_id", f"item_{idx}")
                self.logger.debug(
                    f"  [{idx}] {item_id[:8]}... åŒ…å« {len(entity_dict)} ä¸ªå®ä½“"
                )

        # ç»Ÿè®¡è¾¹æ•°
        edge_count = 0
        edge_details = []  # å­˜å‚¨è¾¹çš„è¯¦ç»†ä¿¡æ¯ç”¨äºè°ƒè¯•

        # æ„å»ºæ— å‘å›¾ï¼ˆéå†ä¸Šä¸‰è§’çŸ©é˜µï¼Œé¿å…é‡å¤ï¼‰
        for i in range(n):
            for j in range(i + 1, n):
                # æ‰¾å…±åŒçš„å®ä½“
                common_entities = set(item_entities[i].keys()) & set(item_entities[j].keys())

                if common_entities:
                    # è®¡ç®—è¾¹æƒé‡ï¼šå…±åŒå®ä½“æƒé‡çš„ç´¯åŠ å¹³å‡
                    edge_weight = sum(
                        item_entities[i][entity_id] + item_entities[j][entity_id]
                        for entity_id in common_entities
                    ) / 2.0  # é™¤ä»¥2é¿å…é‡å¤è®¡æ•°

                    # æ·»åŠ åŒå‘è¾¹ï¼ˆæ— å‘å›¾ï¼‰
                    graph[i].append((j, edge_weight))
                    graph[j].append((i, edge_weight))
                    edge_count += 2

                    # è®°å½•è¾¹çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    if edge_count <= 20:  # åªè®°å½•å‰10æ¡è¾¹ï¼ˆåŒå‘=20æ¡ï¼‰
                        edge_details.append({
                            "from": i,
                            "to": j,
                            "weight": edge_weight,
                            "common_count": len(common_entities)
                        })

        # æ˜¾ç¤ºè¾¹çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå‰5æ¡ï¼‰
        if edge_details:
            self.logger.debug(f"  å‰{min(5, len(edge_details))}æ¡è¾¹è¯¦æƒ…:")
            for edge in edge_details[:5]:
                item_i_id = items[edge["from"]].get("chunk_id") or items[edge["from"]].get("event_id", f"item_{edge['from']}")
                item_j_id = items[edge["to"]].get("chunk_id") or items[edge["to"]].get("event_id", f"item_{edge['to']}")
                item_i_heading = items[edge["from"]].get("heading") or items[edge["from"]].get("title", "")
                item_j_heading = items[edge["to"]].get("heading") or items[edge["to"]].get("title", "")

                self.logger.debug(
                    f"    [{edge['from']}] '{item_i_heading[:20]}' <--> "
                    f"[{edge['to']}] '{item_j_heading[:20]}' | "
                    f"æƒé‡={edge['weight']:.3f}, å…±åŒå®ä½“={edge['common_count']}"
                )

        # ç»Ÿè®¡å›¾çš„ç‰¹å¾
        degrees = [len(edges) for edges in graph.values()]
        avg_degree = sum(degrees) / n if n > 0 else 0
        max_degree = max(degrees) if degrees else 0
        isolated_nodes = sum(1 for d in degrees if d == 0)

        self.logger.info(
            f"âœ“ [å›¾æ„å»º] å®Œæˆ: èŠ‚ç‚¹={n}, è¾¹={edge_count//2} (åŒå‘={edge_count})"
        )
        self.logger.info(
            f"  å›¾ç»Ÿè®¡: å¹³å‡åº¦={avg_degree:.1f}, æœ€å¤§åº¦={max_degree}, å­¤ç«‹èŠ‚ç‚¹={isolated_nodes}"
        )

        return graph

    def _extract_entity_ids_and_weights(
        self,
        key_final: List[Dict[str, Any]]
    ) -> Tuple[List[str], Dict[str, float]]:
        """
        ä» key_final ä¸­æå–å®ä½“IDåˆ—è¡¨å’Œæƒé‡æ˜ å°„

        Args:
            key_final: ä»Recallè¿”å›çš„key_finalæ•°æ®

        Returns:
            (entity_ids, entity_weight_map)
            - entity_ids: å®ä½“IDåˆ—è¡¨ï¼ˆå·²è¿‡æ»¤Noneï¼‰
            - entity_weight_map: {entity_id: weight}
        """
        entity_ids = [key.get("key_id") or key.get("id") for key in key_final]
        entity_weight_map = {
            key.get("key_id") or key.get("id"): key["weight"]
            for key in key_final
        }

        # è¿‡æ»¤æ‰å¯èƒ½ä¸º None çš„ ID
        entity_ids = [eid for eid in entity_ids if eid]

        return entity_ids, entity_weight_map

    async def _query_event_entities(
        self,
        session,
        entity_ids: List[str],
        source_config_ids: List[str]
    ) -> List:
        """
        æŸ¥è¯¢ EventEntity è¡¨ï¼Œè·å–å®ä½“å…³è”çš„äº‹ä»¶

        Args:
            session: æ•°æ®åº“ä¼šè¯
            entity_ids: å®ä½“IDåˆ—è¡¨
            source_config_ids: æ•°æ®æºé…ç½®IDåˆ—è¡¨

        Returns:
            [(event_id, entity_id, weight), ...] æŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        from sqlalchemy import select, and_
        from dataflow.db import EventEntity, SourceEvent

        query = (
            select(EventEntity.event_id, EventEntity.entity_id, EventEntity.weight)
            .join(SourceEvent, EventEntity.event_id == SourceEvent.id)
            .where(
                and_(
                    SourceEvent.source_config_id.in_(source_config_ids),
                    EventEntity.entity_id.in_(entity_ids)
                )
            )
            .distinct()
        )

        result = await session.execute(query)
        return result.fetchall()

    def _build_event_entity_mappings(
        self,
        event_entities: List,
        entity_weight_map: Dict[str, float]
    ) -> Tuple[Dict[str, List[str]], Dict[Tuple[str, str], float]]:
        """
        æ„å»ºäº‹ä»¶-å®ä½“æ˜ å°„å…³ç³»

        Args:
            event_entities: EventEntity æŸ¥è¯¢ç»“æœåˆ—è¡¨
            entity_weight_map: å®ä½“æƒé‡æ˜ å°„ {entity_id: weight}

        Returns:
            (event_to_entities, event_entity_weights)
            - event_to_entities: {event_id: [entity_ids]}
            - event_entity_weights: {(event_id, entity_id): ee_weight}
        """
        event_to_entities = {}
        event_entity_weights = {}

        for event_entity in event_entities:
            event_id = event_entity.event_id
            entity_id = event_entity.entity_id
            ee_weight = event_entity.weight or 1.0

            # æ˜ å°„å…³ç³»
            if event_id not in event_to_entities:
                event_to_entities[event_id] = []
            event_to_entities[event_id].append(entity_id)

            # æƒé‡ï¼ˆåªè®°å½•EventEntityæƒé‡ï¼‰
            event_entity_weights[(event_id, entity_id)] = ee_weight

        return event_to_entities, event_entity_weights

    def _filter_by_similarity_threshold(
        self,
        results: List[Any],
        config: Optional[SearchConfig],
        score_getter: callable,
        item_type: str = "é¡¹",
        display_formatter: Optional[callable] = None,
        show_top_n: int = 3
    ) -> List[Any]:
        """
        æ ¹æ®ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤ç»“æœ

        Args:
            results: ç»“æœåˆ—è¡¨
            config: æœç´¢é…ç½®
            score_getter: ä»ç»“æœé¡¹è·å–åˆ†æ•°çš„å‡½æ•°ï¼Œå¦‚ lambda r: r["score"]
            item_type: é¡¹ç›®ç±»å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰ï¼Œå¦‚ "äº‹é¡¹"ã€"æ®µè½"
            display_formatter: å¯é€‰çš„æ ¼å¼åŒ–å‡½æ•°ï¼Œç”¨äºæ˜¾ç¤ºè¿‡æ»¤åçš„ç»“æœ
                              æ¥æ”¶ä¸€ä¸ªç»“æœé¡¹ï¼Œè¿”å› (id_str, score, title_str) å…ƒç»„
            show_top_n: æ˜¾ç¤ºå‰ N ä¸ªè¿‡æ»¤åçš„ç»“æœï¼ˆé»˜è®¤ 3ï¼‰

        Returns:
            è¿‡æ»¤åçš„ç»“æœåˆ—è¡¨
        """
        original_count = len(results)

        # å¦‚æœæ²¡æœ‰é…ç½®æˆ–æ²¡æœ‰é˜ˆå€¼ï¼Œè·³è¿‡è¿‡æ»¤
        if not config or not config.rerank.score_threshold:
            self.logger.warning("æœªè®¾ç½®é˜ˆå€¼æˆ–configä¸ºç©ºï¼Œè·³è¿‡ç›¸ä¼¼åº¦è¿‡æ»¤")
            return results

        # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
        filtered_results = [
            r for r in results
            if score_getter(r) >= config.rerank.score_threshold
        ]

        # å¦‚æœè¿‡æ»¤åæ•°é‡å‡å°‘ï¼Œè¾“å‡ºæ—¥å¿—
        if len(filtered_results) < original_count:
            self.logger.info(
                f"ç›¸ä¼¼åº¦è¿‡æ»¤: {original_count} -> {len(filtered_results)} ä¸ª{item_type} "
                f"(é˜ˆå€¼={config.rerank.score_threshold:.2f})"
            )

            # å±•ç¤ºè¿‡æ»¤åä¿ç•™çš„ç»“æœ
            if filtered_results and display_formatter:
                self.logger.info("=" * 80)
                self.logger.info(
                    f"è¿‡æ»¤åä¿ç•™çš„ {len(filtered_results)} ä¸ª{item_type} (Top {show_top_n}):")
                self.logger.info("-" * 80)

                for idx, result in enumerate(filtered_results[:show_top_n], 1):
                    id_str, score, title_str = display_formatter(result)
                    self.logger.info(
                        f"  {idx}. {item_type} {id_str} | "
                        f"Cosine={score:.4f} | "
                        f"æ ‡é¢˜: {title_str}"
                    )

                if len(filtered_results) > show_top_n:
                    self.logger.info(f"  ... è¿˜æœ‰ {len(filtered_results) - show_top_n} ä¸ª{item_type}")

                self.logger.info("=" * 80)

        return filtered_results

    async def _build_response(
        self,
        config: SearchConfig,
        key_final: List[Dict],
        items: List[Any],
        item_to_clues: Dict[str, List[Dict]]
    ) -> Dict[str, Any]:
        """
        æ„å»ºç»Ÿä¸€çš„å“åº”æ ¼å¼

        Args:
            config: æœç´¢é…ç½®
            key_final: æœ€ç»ˆå®ä½“åˆ—è¡¨
            items: é¡¹ç›®åˆ—è¡¨ï¼ˆäº‹é¡¹æˆ–æ®µè½ï¼‰
            item_to_clues: é¡¹ç›®IDåˆ°çº¿ç´¢çš„æ˜ å°„

        Returns:
            å“åº”å­—å…¸
        """
        # æå– query å’Œ recall entities
        query_entity_ids = set()
        recall_entity_ids = set()

        for key in key_final:
            key_id = key.get("key_id")
            steps = key.get("steps", [])

            if not steps:
                continue

            if steps[0] == 0 or "query" in str(steps):
                query_entity_ids.add(key_id)
            else:
                recall_entity_ids.add(key_id)

        # è¿‡æ»¤æ‰åœ¨ query_entities ä¸­çš„ recall_entities
        recall_entity_ids = recall_entity_ids - query_entity_ids

        # æ„å»º entities ä¿¡æ¯
        query_entities = [
            {
                "id": key["key_id"],
                "name": key.get("name", ""),
                "type": key.get("type", ""),
                "weight": key.get("weight", 0)
            }
            for key in key_final
            if key.get("key_id") in query_entity_ids
        ]

        recall_entities = [
            {
                "id": key["key_id"],
                "name": key.get("name", ""),
                "type": key.get("type", ""),
                "weight": key.get("weight", 0)
            }
            for key in key_final
            if key.get("key_id") in recall_entity_ids
        ]

        return {
            "items": items,  # å­ç±»ä¼šé‡å‘½åä¸º "events" æˆ– "sections"
            "clues": {
                "origin_query": config.original_query or config.query,
                "final_query": config.query,
                "query_entities": query_entities,
                "recall_entities": recall_entities,
            }
        }

    # ==================== çº¿ç´¢ç”Ÿæˆå·¥å…·æ–¹æ³• ====================

    def _build_entity_node_data(
        self,
        entity_id: str,
        key_info: Dict[str, Any],
        recall_method: str = "entity"
    ) -> Dict[str, Any]:
        """
        æ„å»ºå®ä½“èŠ‚ç‚¹æ•°æ®

        Args:
            entity_id: å®ä½“ID
            key_info: å®ä½“ä¿¡æ¯å­—å…¸
            recall_method: å¬å›æ–¹æ³•ï¼ˆ"entity" æˆ– "query"ï¼‰

        Returns:
            å®ä½“èŠ‚ç‚¹å­—å…¸
        """
        return {
            "key_id": entity_id,
            "id": entity_id,
            "name": key_info.get("name", ""),
            "type": key_info.get("type", ""),
            "description": key_info.get("description", ""),
            "hop": key_info.get("hop", 0),
            "recall_method": recall_method
        }

    def _build_event_node_data(
        self,
        event_obj: Any,
        stage: str = "rerank",
        recall_method: str = "entity"
    ) -> Dict[str, Any]:
        """
        æ„å»ºäº‹é¡¹èŠ‚ç‚¹æ•°æ®

        Args:
            event_obj: äº‹é¡¹å¯¹è±¡ï¼ˆSourceEventï¼‰
            stage: é˜¶æ®µï¼ˆ"rerank", "search"ç­‰ï¼‰
            recall_method: å¬å›æ–¹æ³•ï¼ˆ"entity" æˆ– "query"ï¼‰

        Returns:
            äº‹é¡¹èŠ‚ç‚¹å­—å…¸
        """
        from dataflow.modules.search.tracker import Tracker

        return {
            "type": "event",
            "event_id": event_obj.id,
            "stage": stage,
            "recall_method": recall_method,
            "source_config_id": getattr(event_obj, "source_config_id", ""),
            "article_id": getattr(event_obj, "article_id", ""),
            "title": getattr(event_obj, "title", ""),
            "summary": getattr(event_obj, "summary", ""),
            "category": getattr(event_obj, "category", "")
        }

    def _build_clue_metadata(
        self,
        method: str,
        weight_score: float,
        similarity_score: float,
        rank: int,
        step: str = "",
        source: str = "",
        **kwargs
    ) -> Dict[str, Any]:
        """
        æ„å»ºçº¿ç´¢çš„metadata

        Args:
            method: æ–¹æ³•åï¼ˆ"weight_entity", "weight_query", "final_result"ï¼‰
            weight_score: æƒé‡åˆ†æ•°
            similarity_score: ç›¸ä¼¼åº¦åˆ†æ•°
            rank: æ’å
            step: æ­¥éª¤ï¼ˆç”¨äºfinalçº¿ç´¢ï¼‰
            source: æ¥æºï¼ˆ"entity" æˆ– "query"ï¼‰
            **kwargs: å…¶ä»–å…ƒæ•°æ®å­—æ®µ

        Returns:
            metadataå­—å…¸
        """
        metadata = {
            "method": method,
            "weight_score": weight_score,
            "similarity_score": similarity_score,
            "rank": rank,
        }

        if step:
            metadata["step"] = step

        if source:
            metadata["source"] = source

        # æ·»åŠ å…¶ä»–è‡ªå®šä¹‰å­—æ®µ
        metadata.update(kwargs)

        return metadata

    async def _keys_to_events(
        self,
        key_final: List[Dict[str, Any]],
        query: str,
        source_config_ids: List[str],
        query_vector: Optional[List[float]] = None,
        config: Optional[SearchConfig] = None
    ) -> List[Dict[str, Any]]:
        """
        é€šç”¨æ–¹æ³•ï¼šæ ¹æ® keys å¬å›ç›¸å…³äº‹é¡¹ï¼ˆKey â†’ Entity â†’ Eventï¼‰

        è¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„å®ä½“å¬å›æ–¹æ³•ï¼Œå¯ä»¥è¢«äº‹é¡¹çº§å’Œæ®µè½çº§æœç´¢å™¨å¤ç”¨ã€‚
        å·¥ä½œæµç¨‹ï¼š
        1. æå–å®ä½“IDå’Œæƒé‡
        2. é€šè¿‡EventEntityè¡¨æŸ¥æ‰¾ç›¸å…³äº‹ä»¶
        3. ä»ESæ‰¹é‡è·å–äº‹é¡¹çš„content_vector
        4. è®¡ç®—æ¯ä¸ªäº‹é¡¹ä¸queryçš„ä½™å¼¦ç›¸ä¼¼åº¦
        5. æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶è¿”å›

        Args:
            key_final: ä»Recallè¿”å›çš„å®ä½“åˆ—è¡¨
            query: æŸ¥è¯¢æ–‡æœ¬
            source_config_ids: æ•°æ®æºé…ç½®IDåˆ—è¡¨
            query_vector: å¯é€‰çš„æŸ¥è¯¢å‘é‡
            config: æœç´¢é…ç½®

        Returns:
            äº‹é¡¹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªäº‹é¡¹åŒ…å«ï¼š
            {
                "event_id": str,
                "event": SourceEventå¯¹è±¡,
                "similarity_score": float,
                "source": str,  # æ¥æºæ ‡è®°ï¼ˆ"entity" æˆ– "query"ï¼‰
                "clues": List[Dict]  # å¬å›è¯¥äº‹é¡¹çš„å®ä½“åˆ—è¡¨
            }
        """
        try:
            self.logger.debug(
                f"[_keys_to_events] å¼€å§‹å¬å›: {len(key_final)} ä¸ªkeys, query='{query}'")

            if not key_final:
                return []

            # 1. æå–å®ä½“IDå’Œæƒé‡
            entity_ids, entity_weight_map = self._extract_entity_ids_and_weights(key_final)

            if not entity_ids:
                self.logger.warning("key_final ä¸­æ²¡æœ‰æœ‰æ•ˆçš„å®ä½“ID")
                return []

            self.logger.debug(f"æå–åˆ° {len(entity_ids)} ä¸ªå®ä½“ID")

            # 2. æ„å»º entity_to_key æ˜ å°„
            entity_to_key = {}
            for key in key_final:
                key_id = key.get("key_id") or key.get("id")
                if key_id:
                    entity_to_key[key_id] = key

            async with self.session_factory() as session:
                # 3. æŸ¥è¯¢ç›¸å…³äº‹ä»¶
                event_entities = await self._query_event_entities(
                    session=session,
                    entity_ids=entity_ids,
                    source_config_ids=source_config_ids
                )

                if not event_entities:
                    self.logger.warning("æœªæ‰¾åˆ°ç›¸å…³äº‹ä»¶")
                    return []

                # 4. æ„å»ºäº‹ä»¶-å®ä½“æ˜ å°„
                event_to_entities, _ = self._build_event_entity_mappings(
                    event_entities=event_entities,
                    entity_weight_map=entity_weight_map
                )

                event_ids = list(event_to_entities.keys())
                self.logger.debug(f"æ‰¾åˆ° {len(event_ids)} ä¸ªç›¸å…³äº‹ä»¶")

                # 5. è·å–äº‹ä»¶è¯¦æƒ…ï¼ˆé¢„åŠ è½½å…³è”å…³ç³»ï¼‰
                from sqlalchemy import select, and_
                from sqlalchemy.orm import selectinload
                from dataflow.db import SourceEvent

                event_detail_query = (
                    select(SourceEvent)
                    .options(
                        selectinload(SourceEvent.source),
                        selectinload(SourceEvent.article)
                    )
                    .where(
                        and_(
                            SourceEvent.source_config_id.in_(source_config_ids),
                            SourceEvent.id.in_(event_ids)
                        )
                    )
                )

                event_detail_result = await session.execute(event_detail_query)
                events = event_detail_result.scalars().all()

                if not events:
                    self.logger.warning("æœªæ‰¾åˆ°äº‹ä»¶è¯¦æƒ…")
                    return []

                self.logger.debug(f"è·å–åˆ° {len(events)} ä¸ªäº‹ä»¶çš„è¯¦ç»†ä¿¡æ¯")

                # 6. ä»ESæ‰¹é‡è·å–äº‹ä»¶å‘é‡
                es_events_data = await self.event_repo.get_events_by_ids(event_ids=event_ids)

                # æ„å»º event_id -> content_vector æ˜ å°„
                event_vector_map = {}
                for es_event in es_events_data:
                    event_id = es_event.get('event_id')
                    content_vector = es_event.get('content_vector')
                    if event_id and content_vector:
                        event_vector_map[event_id] = content_vector

                self.logger.debug(
                    f"ä»ESè·å–åˆ° {len(event_vector_map)}/{len(event_ids)} ä¸ªäº‹ä»¶å‘é‡"
                )

                # 7. ç”ŸæˆæŸ¥è¯¢å‘é‡ï¼ˆå¦‚æœæ²¡æœ‰ä¼ å…¥ï¼‰
                if query_vector is None:
                    query_vector = await self._generate_query_vector(query, config)

                # 8. è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                event_results = []

                for event in events:
                    event_id = event.id
                    event_vector = event_vector_map.get(event_id)

                    if not event_vector:
                        self.logger.debug(f"äº‹ä»¶ {event_id[:8]}... æ— å‘é‡ï¼Œè·³è¿‡")
                        continue

                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                    try:
                        query_np = np.array(query_vector, dtype=np.float32)
                        event_np = np.array(event_vector, dtype=np.float32)

                        cosine_score = float(
                            np.dot(query_np, event_np) /
                            (np.linalg.norm(query_np) * np.linalg.norm(event_np))
                        )
                    except Exception as e:
                        self.logger.warning(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
                        cosine_score = 0.0

                    # æ„å»º clues åˆ—è¡¨
                    source_entity_ids = event_to_entities.get(event_id, [])
                    clues = []

                    for entity_id in source_entity_ids:
                        key_info = entity_to_key.get(entity_id)
                        if key_info:
                            clues.append({
                                "id": entity_id,
                                "key_id": entity_id,
                                "name": key_info.get("name", ""),
                                "weight": key_info.get("weight", 0.0),
                                "steps": key_info.get("steps", [1]),
                                "type": key_info.get("type", ""),
                                "hop": key_info.get("hop", 0)
                            })

                    # æ„å»ºç»“æœ
                    event_result = {
                        "event_id": event_id,
                        "event": event,
                        "similarity_score": cosine_score,
                        "source": "entity",  # æ¥æºæ ‡è®°ï¼ˆentity/queryï¼‰
                        "clues": clues
                    }
                    event_results.append(event_result)

                # 9. æŒ‰ç›¸ä¼¼åº¦æ’åº
                event_results.sort(key=lambda x: x["similarity_score"], reverse=True)

                self.logger.debug(
                    f"[_keys_to_events] å®Œæˆ: å¬å› {len(event_results)} ä¸ªäº‹ä»¶"
                )

                return event_results

        except Exception as e:
            self.logger.error(f"[_keys_to_events] æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            return []

    def _extract_item_text(self, item: Dict[str, Any]) -> str:
        """
        æå–é¡¹ç›®æ–‡æœ¬å†…å®¹
        """
        # ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€textå­—æ®µ
        if "text" in item:
            return item["text"]

        # æå–eventå¯¹è±¡
        if "event" in item:
            event = item["event"]
            if hasattr(event, 'title') and hasattr(event, 'content'):
                return f"{event.title or ''} {event.content or ''}".strip()

        # åˆå¹¶headingå’Œcontent
        heading = item.get("heading", "")
        content = item.get("content", "")
        return f"{heading} {content}".strip()

    async def _step4_calculate_weights(
        self,
        items: List[Dict[str, Any]],
        key_final: List[Dict[str, Any]],
        config: Optional[SearchConfig] = None,
        item_type: str = "é¡¹ç›®",
        store_detailed_scores: bool = False,
        top_n: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Step4: ç»Ÿä¸€æƒé‡è®¡ç®—å’ŒRRFæ’åºï¼ˆæ”¯æŒæ®µè½çº§å’Œäº‹é¡¹çº§ï¼‰

        æ ¸å¿ƒåŠŸèƒ½ï¼š
        1. æ”¯æŒ4è·¯RRFèåˆï¼šsimilarity / relation_chain / density / bm25
        2. å¯¹äºBM25å¬å›çš„äº‹é¡¹ï¼ˆsource="bm25"ï¼‰ï¼Œè·³è¿‡å‰ä¸‰é¡¹è®¡ç®—ï¼Œç›´æ¥ä½¿ç”¨bm25_rank
        3. RRFå‚æ•°ï¼šw_sim=1.5, w_rel=0.5, w_den=0.2, w_bm25=1.0
        """
        if not items:
            return []

        try:
            # è·å–ç›®æ ‡ç»´åº¦ï¼ˆç”¨äºåŠ æƒï¼‰
            target_types = set(config.target_entity_types) if config and config.target_entity_types else set()
            if target_types:
                self.logger.info(f"ğŸ¯ ç›®æ ‡ç»´åº¦åŠ æƒ: {list(target_types)}")

            self.logger.info("=" * 80)
            self.logger.info(
                f"ã€{item_type}çº§Step4ã€‘å¼€å§‹è®¡ç®—{item_type}æƒé‡ï¼ˆ4è·¯RRFèåˆï¼šsimilarity/relation/density/bm25ï¼‰"
            )
            self.logger.info("-" * 80)

            # å­˜å‚¨å„ç»„ä»¶çš„åˆ†æ•°
            similarity_scores = {}
            relation_scores = {}
            density_scores = {}
            bm25_scores = {}

            # ä¸ºæ‰€æœ‰äº‹é¡¹è®¡ç®—å„ç»„ä»¶åˆ†æ•°
            for item in items:
                # æ”¯æŒç»Ÿä¸€å­—æ®µ 'id' å’Œå‘åå…¼å®¹çš„ 'chunk_id' / 'event_id'
                item_id = item.get("id") or item.get("chunk_id") or item.get("event_id")
                if not item_id:
                    self.logger.warning(f"è·³è¿‡æ²¡æœ‰IDçš„é¡¹ç›®: {item}")
                    continue

                # ==================== similarityåˆ†æ•°ï¼ˆæ‰€æœ‰äº‹é¡¹éƒ½æœ‰ï¼‰ ====================
                # ä»itemä¸­è·å–é¢„è®¡ç®—çš„similarity_scoreï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨scoreå­—æ®µ
                similarity_score = item.get("similarity_score") or item.get("score", 0.0)
                similarity_scores[item_id] = similarity_score

                # ==================== relationå’Œdensityåˆ†æ•°ï¼ˆåŸºäºå®ä½“çº¿ç´¢ï¼‰ ====================
                relation_chain_score = 0.0
                density_score = 0.0

                # è·å–å®ä½“çº¿ç´¢ï¼ˆBM25å¬å›çš„äº‹é¡¹å¯èƒ½æ²¡æœ‰ï¼‰
                item_clues = item.get("clues", [])

                if item_clues:
                    # æå–æ–‡æœ¬å†…å®¹ç”¨äºç»Ÿè®¡
                    full_text = self._extract_item_text(item)

                    # è®¡ç®—æ¯ä¸ªå®ä½“çš„è´¡çŒ®
                    for clue in item_clues:
                        key_name = clue.get("name", "")
                        key_weight = clue.get("weight", 0.0)
                        key_type = clue.get("type", "")
                        hop = clue.get("hop", 0)
                        key_steps = clue.get("steps", [1])
                        step = key_steps[0] if key_steps else 1

                        # ç»Ÿè®¡keyåœ¨æ–‡æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°
                        count = full_text.count(key_name) if key_name else 0

                        # å¤šè·³è¡°å‡å› å­
                        hop_factor = 1.0 / (1.0 + hop)

                        # ç›®æ ‡ç»´åº¦åŠ æƒ
                        target_boost = 1.5 if key_type in target_types else 1.0

                        if count > 0:
                            # å…³ç³»é“¾å¾—åˆ†
                            relation_contribution = hop_factor * target_boost * key_weight
                            relation_chain_score += relation_contribution

                            # ä¿¡æ¯å¯†åº¦å¾—åˆ†
                            density_contribution = key_weight * math.log(1 + count) / step
                            density_score += density_contribution

                relation_scores[item_id] = relation_chain_score
                density_scores[item_id] = density_score

                # ==================== BM25åˆ†æ•°ï¼ˆåªæœ‰bm25å¬å›çš„äº‹é¡¹æœ‰ï¼‰ ====================
                bm25_rank = item.get("bm25_rank")
                if bm25_rank is not None:
                    # æœ‰bm25æ’åï¼Œè®¡ç®—å€’æ•°åˆ†æ•°
                    bm25_scores[item_id] = 1.0 / bm25_rank
                # å¦åˆ™ä¸å‚ä¸bm25ç»´åº¦è®¡ç®—

            # ==================== ç»Ÿä¸€RRFèåˆè®¡ç®— ====================
            def build_rank_map(score_dict: Dict[str, float], reverse: bool = True) -> Dict[str, int]:
                if not score_dict:
                    return {}
                sorted_items = sorted(score_dict.items(), key=lambda x: x[1], reverse=reverse)
                return {id_val: idx + 1 for idx, (id_val, _) in enumerate(sorted_items)}

            # ä¸ºæ¯ä¸ªç»„ä»¶æ„å»ºæ’å
            rank_sim = build_rank_map(similarity_scores)
            rank_rel = build_rank_map(relation_scores)
            rank_den = build_rank_map(density_scores)
            rank_bm25 = build_rank_map(bm25_scores)

            # RRFå‚æ•°ï¼ˆ4è·¯èåˆï¼‰ - ä» config.rerank è¯»å–
            k_rrf = config.rerank.rrf_k if config and config.rerank else 100
            w_sim = config.rerank.rrf_weight_similarity if config and config.rerank else 1.5
            w_rel = config.rerank.rrf_weight_relation if config and config.rerank else 0.5
            w_den = config.rerank.rrf_weight_density if config and config.rerank else 0.2
            w_bm25 = config.rerank.rrf_weight_bm25 if config and config.rerank else 2.0

            self.logger.info(f"RRFå‚æ•°: k={k_rrf}, weights=[w_sim={w_sim}, w_rel={w_rel}, w_den={w_den}, w_bm25={w_bm25}]")

            # è®¡ç®—RRFå¹¶å­˜å‚¨åˆ°items
            for item in items:
                item_id = item.get("id") or item.get("chunk_id") or item.get("event_id")
                if not item_id:
                    continue

                # # åˆ¤æ–­æ˜¯å¦ä¸ºBM25äº‹é¡¹
                # is_bm25 = item.get("source") == "bm25"

                # if is_bm25:
                #     # BM25 äº‹é¡¹ï¼šåªä½¿ç”¨ BM25 æ’åè®¡ç®— RRF
                #     bm25_rank = rank_bm25.get(item_id, len(items))
                #     rrf_score = w_bm25 / (k_rrf + bm25_rank)

                #     # è®¾ç½®ç›¸å…³å­—æ®µ
                #     item["weight"] = rrf_score
                #     item["score"] = rrf_score
                #     item["original_weight"] = rrf_score

                #     # BM25 äº‹é¡¹çš„ç‰¹æ®Šæ ‡è®°
                #     item["is_bm25"] = True
                #     item["bm25_rank"] = bm25_rank

                #     # å­˜å‚¨è¯¦ç»†åˆ†æ•°ï¼ˆå¯é€‰ï¼‰
                #     if store_detailed_scores:
                #         item["similarity_score"] = 0.0
                #         item["relation_chain_score"] = 0.0
                #         item["density_score"] = 0.0
                #         item["bm25_contribution"] = rrf_score

                # 4è·¯RRFèåˆï¼ˆåŠ¨æ€å¤„ç†ç¼ºå¤±ç»´åº¦ï¼‰
                sim_rank = rank_sim[item_id]
                rel_rank = rank_rel[item_id]
                den_rank = rank_den[item_id]

                # åˆ¤æ–­è¯¥äº‹é¡¹æ˜¯å¦æœ‰bm25åˆ†æ•°
                has_bm25 = item_id in bm25_scores
                bm25_rank = rank_bm25.get(item_id, len(items)) if has_bm25 else None

                # è®¡ç®—å„ç»´åº¦è´¡çŒ®
                contributions = {
                    "similarity": w_sim / (k_rrf + sim_rank),
                    "relation": w_rel / (k_rrf + rel_rank),
                    "density": w_den / (k_rrf + den_rank),
                }

                # æ€»æƒé‡åˆ†æ¯ï¼ˆæ ¹æ®å®é™…å­˜åœ¨çš„ç»´åº¦åŠ¨æ€è°ƒæ•´ï¼‰
                total_weight = w_sim + w_rel + w_den

                if has_bm25:
                    contributions["bm25"] = w_bm25 / (k_rrf + bm25_rank)
                    total_weight += w_bm25

                # åŠ¨æ€RRFåˆ†æ•°ï¼ˆæŒ‰å®é™…å­˜åœ¨çš„ç»´åº¦åŠ æƒï¼‰
                rrf_score = sum(contributions.values())

                # è®¾ç½®ç›¸å…³å­—æ®µ
                item["weight"] = rrf_score
                item["score"] = rrf_score
                item["original_weight"] = rrf_score
                item["has_bm25"] = has_bm25
                item["bm25_rank"] = bm25_rank
                item["recall_sources"] = item.get("recall_sources", [])

                # å­˜å‚¨è¯¦ç»†åˆ†æ•°ï¼ˆå¯é€‰ï¼‰
                if store_detailed_scores:
                    item["similarity_score"] = similarity_scores.get(item_id, 0.0)
                    item["relation_chain_score"] = relation_scores.get(item_id, 0.0)
                    item["density_score"] = density_scores.get(item_id, 0.0)
                    item["similarity_contribution"] = contributions.get("similarity", 0.0)
                    item["relation_contribution"] = contributions.get("relation", 0.0)
                    item["density_contribution"] = contributions.get("density", 0.0)
                    item["bm25_contribution"] = contributions.get("bm25", 0.0)

            # ==================== æ’åºå’Œè¿‡æ»¤ ====================
            # æŒ‰RRFå¾—åˆ†æ’åº
            sorted_items = sorted(
                items,
                key=lambda x: x["weight"],
                reverse=True
            )

            # æ˜¾ç¤ºTop-N
            display_n = min(top_n, len(sorted_items))
            self.logger.info("")
            self.logger.info("=" * 80)
            self.logger.info(
                f"ã€{item_type}çº§Step4ã€‘4è·¯RRF æ’åºç»“æœï¼ˆTop-{display_n}ï¼‰ï¼š"
            )
            self.logger.info("-" * 80)

            for rank, item in enumerate(sorted_items[:display_n], 1):
                item_id = item.get("id") or item.get("chunk_id") or item.get("event_id")
                weight = item["weight"]
                recall_sources = item.get("recall_sources", [])

                # æ˜¾ç¤ºå¬å›æ¥æº
                source_str = ",".join(recall_sources) if recall_sources else "unknown"

                # è·å–å„ç»´åº¦è´¡çŒ®å€¼
                title_preview = ""
                if item.get("event"):
                    title_preview = item["event"].title[:40] if item["event"].title else "æ— æ ‡é¢˜"
                elif item.get("heading"):
                    title_preview = item["heading"][:40]

                # æ˜¾ç¤º4è·¯èåˆçš„è¯¦ç»†ä¿¡æ¯
                if store_detailed_scores:
                    sim = item.get("similarity_score", 0.0)
                    rel = item.get("relation_chain_score", 0.0)
                    den = item.get("density_score", 0.0)
                    bm25_rank = item.get("bm25_rank", "N/A")

                    sim_contrib = item.get("similarity_contribution", 0.0)
                    rel_contrib = item.get("relation_contribution", 0.0)
                    den_contrib = item.get("density_contribution", 0.0)
                    bm25_contrib = item.get("bm25_contribution", 0.0)

                    self.logger.info(
                        f"Rank {rank}: {item_id[:8]}... | "
                        f"RRF={weight:.4f} | "
                        f"Sim={sim:.4f}({sim_contrib:.3f}) | "
                        f"Rel={rel:.4f}({rel_contrib:.3f}) | "
                        f"Den={den:.4f}({den_contrib:.3f}) | "
                        f"BM25:R={bm25_rank}({bm25_contrib:.3f}) | "
                        f"æ¥æº=[{source_str}] | "
                        f"æ ‡é¢˜: {title_preview}"
                    )
                else:
                    # ç®€åŒ–æ˜¾ç¤º
                    self.logger.info(
                        f"Rank {rank}: {item_id[:8]}... | "
                        f"RRF={weight:.4f} | "
                        f"æ¥æº=[{source_str}] | "
                        f"æ ‡é¢˜: {title_preview}"
                    )

            if len(sorted_items) > display_n:
                # ç»Ÿè®¡å„ç±»å¬å›æ¥æºçš„äº‹é¡¹æ•°
                entity_only = sum(1 for item in sorted_items if "entity" in item.get("recall_sources", []) and "bm25" not in item.get("recall_sources", []))
                bm25_only = sum(1 for item in sorted_items if "bm25" in item.get("recall_sources", []) and "entity" not in item.get("recall_sources", []))
                both = sum(1 for item in sorted_items if "entity" in item.get("recall_sources", []) and "bm25" in item.get("recall_sources", []))

                self.logger.info(
                    f"... (è¿˜æœ‰ {len(sorted_items) - display_n} ä¸ª{item_type}æœªæ˜¾ç¤ºï¼Œ"
                    f"æ€»è®¡: {entity_only}ä¸ªä»…entity + {bm25_only}ä¸ªä»…bm25 + {both}ä¸ªä¸¤è€…éƒ½æœ‰)"
                )

            self.logger.info("=" * 80)

            # ç»Ÿè®¡ä¿¡æ¯
            total_top_n = len(sorted_items[:config.rerank.max_results if config else 50])
            bm25_in_top_n = sum(1 for item in sorted_items[:config.rerank.max_results if config else 50] if item.get("has_bm25", False))

            self.logger.info(f"BM25ç›¸å…³äº‹é¡¹åœ¨Top-{total_top_n}ä¸­å æ¯”: {bm25_in_top_n}/{total_top_n} ({bm25_in_top_n/(total_top_n or 1)*100:.1f}%)")

            # æŒ‰æ¥æºç»Ÿè®¡åˆ†å¸ƒ
            entity_only_top = sum(1 for item in sorted_items[:total_top_n] if item.get("recall_sources") == ["entity"])
            bm25_only_top = sum(1 for item in sorted_items[:total_top_n] if item.get("recall_sources") == ["bm25"])
            both_sources_top = sum(1 for item in sorted_items[:total_top_n] if set(item.get("recall_sources", [])) == {"entity", "bm25"})

            self.logger.info(f"  - ä»…entityå¬å›: {entity_only_top}ä¸ª")
            self.logger.info(f"  - ä»…bm25å¬å›: {bm25_only_top}ä¸ª")
            self.logger.info(f"  - ä¸¤è€…éƒ½å¬å›: {both_sources_top}ä¸ª")

            # é˜ˆå€¼è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
            if (config and config.rerank.score_threshold):
                original_count = len(sorted_items)
                # è¿™é‡Œå¯ä»¥æ·»åŠ åŸºäºé˜ˆå€¼çš„è¿‡æ»¤é€»è¾‘
                if len(sorted_items) < original_count:
                    self.logger.info(
                        f"ğŸ¯ æƒé‡è¿‡æ»¤: {original_count} -> {len(sorted_items)} ä¸ª"
                        f"{item_type} (é˜ˆå€¼={config.rerank.score_threshold:.2f})"
                    )

            self.logger.info(
                f"âœ“ ã€{item_type}çº§Step4ã€‘å®Œæˆ: "
                f"è®¡ç®—å¹¶æ’åº {len(sorted_items)} ä¸ª{item_type}ï¼Œæ˜¾ç¤º Top-{display_n}"
            )

            return sorted_items

        except Exception as e:
            self.logger.error(
                f"ã€{item_type}çº§Step4ã€‘æ‰§è¡Œå¤±è´¥: {e}",
                exc_info=True
            )
            return []


