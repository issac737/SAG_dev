"""
å®ä½“å¬å›æ¨¡å—ï¼ˆRecallï¼‰

å®ç°6æ­¥éª¤çš„å¤åˆæœç´¢ç®—æ³•ï¼š
1. queryæ‰¾keyï¼šLLMæŠ½å–queryçš„ç»“æ„åŒ–å±æ€§ï¼Œé€šè¿‡å‘é‡ç›¸ä¼¼åº¦æ‰¾åˆ°å…³è”å®ä½“
2. keyæ‰¾eventï¼šé€šè¿‡[key-query-related]ç”¨sqlæ‰¾åˆ°æ‰€æœ‰å…³è”äº‹é¡¹
3. è¿‡æ»¤Eventï¼ˆæƒé‡æ’åºï¼‰ï¼š
   - è®¡ç®—eventæƒé‡ï¼ševent_weight = balance * e1_weight + (1-balance) * key_weight_sum
   - æŒ‰æƒé‡æ’åºç­›é€‰eventsï¼ˆä½¿ç”¨max_eventsé™åˆ¶ï¼‰
   - ä¿ç•™æ‰€æœ‰keyï¼ˆæ­¥éª¤1å·²é€šè¿‡max_entitiesé™åˆ¶æ•°é‡ï¼‰
4. è®¡ç®—event-keyæƒé‡å‘é‡ï¼šæ ¹æ®æ¯ä¸ªeventåŒ…å«keyçš„æƒ…å†µè®¡ç®—æƒé‡
5. åå‘è®¡ç®—keyæƒé‡å‘é‡ï¼šæ ¹æ®eventæƒé‡åå‘è®¡ç®—keyé‡è¦æ€§
6. æå–é‡è¦çš„keyï¼šé€šè¿‡é˜ˆå€¼æˆ–top-næ–¹å¼æå–é‡è¦key
"""

from typing import Any, Dict, List, Optional, Tuple, Set
from dataclasses import dataclass
import time
import numpy as np

from sqlalchemy import select, or_
from sqlalchemy.orm import selectinload

from dataflow.core.ai.base import BaseLLMClient
from dataflow.core.ai.models import LLMMessage, LLMRole
from dataflow.core.prompt.manager import PromptManager
from dataflow.core.storage.elasticsearch import get_es_client
from dataflow.core.storage.repositories.entity_repository import EntityVectorRepository
from dataflow.core.storage.repositories.event_repository import EventVectorRepository
from dataflow.db import SourceEvent, Entity, EventEntity, get_session_factory
from dataflow.exceptions import AIError
from dataflow.modules.load.processor import DocumentProcessor
from dataflow.modules.search.config import SearchConfig, RecallMode
from dataflow.modules.search.tracker import Tracker  # ğŸ†• ç»Ÿä¸€ä½¿ç”¨Tracker
from dataflow.utils import get_logger

logger = get_logger("search.recall")


@dataclass
class RecallResult:
    """å®ä½“å¬å›ç»“æœ"""
    # æŸ¥è¯¢è¿½è¸ªä¿¡æ¯
    original_query: str  # åŸå§‹æŸ¥è¯¢æ–‡æœ¬ï¼ˆç”¨äºè°ƒè¯•å’Œè¿½è¸ªï¼‰

    # æœ€ç»ˆç»“æœ
    # [{"key": str, "weight": float, "steps": List[int]}, ...]
    key_final: List[Dict[str, Any]]

    # ä¸­é—´ç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
    key_query_related: List[Dict[str, Any]]  # æ­¥éª¤1ç»“æœ
    event_key_query_related: List[str]       # æ­¥éª¤2ç»“æœ
    event_related: List[str]                 # æ­¥éª¤3ç»“æœ
    event_key_weights: Dict[str, float]      # æ­¥éª¤4ç»“æœ
    key_event_weights: Dict[str, float]      # æ­¥éª¤5ç»“æœ

    # æ€§èƒ½è¿½è¸ªä¿¡æ¯
    step_timings: Dict[str, float]           # å„æ­¥éª¤è€—æ—¶ï¼ˆå•ä½ï¼šç§’ï¼‰
    step1_substep_timings: Optional[Dict[str, float]]  # æ­¥éª¤1å­æ­¥éª¤è€—æ—¶ï¼ˆå¯é€‰ï¼‰


class RecallSearcher:
    """å®ä½“å¬å›æœç´¢å™¨ - å®ç°6æ­¥éª¤å¤åˆæœç´¢ç®—æ³•"""

    def __init__(
        self,
        llm_client: BaseLLMClient,
        prompt_manager: PromptManager,
    ):
        """
        åˆå§‹åŒ–å®ä½“å¬å›æœç´¢å™¨

        Args:
            llm_client: LLMå®¢æˆ·ç«¯
            prompt_manager: æç¤ºè¯ç®¡ç†å™¨
        """
        self.llm_client = llm_client
        self.prompt_manager = prompt_manager
        self.session_factory = get_session_factory()
        self.logger = get_logger("search.recall")

        # åˆå§‹åŒ–Elasticsearchä»“åº“
        self.es_client = get_es_client()
        self.entity_repo = EntityVectorRepository(self.es_client)
        self.event_repo = EventVectorRepository(self.es_client)

        # åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨ç”¨äºç”Ÿæˆå‘é‡
        self.processor = DocumentProcessor(llm_client=llm_client)

        self.logger.info(
            "å®ä½“å¬å›æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ",
            extra={
                "embedding_model_name": self.processor.embedding_model_name,
            },
        )

    def _filter_entities_by_type(
        self,
        entities: List[Dict[str, Any]],
        config: SearchConfig,
        context: str = "",
        include_tags_backup: bool = True
    ) -> List[Dict[str, Any]]:
        """
        ç»Ÿä¸€çš„å®ä½“ç±»å‹è¿‡æ»¤æ–¹æ³•
        
        ä¼˜å…ˆä½¿ç”¨ focus_entity_typesï¼ˆç™½åå•ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨ exclude_entity_typesï¼ˆé»‘åå•ï¼‰
        
        Args:
            entities: å¾…è¿‡æ»¤çš„å®ä½“åˆ—è¡¨
            config: æœç´¢é…ç½®ï¼ˆåŒ…å« focus_entity_types å’Œ exclude_entity_typesï¼‰
            context: ä¸Šä¸‹æ–‡æè¿°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            include_tags_backup: ç™½åå•æ¨¡å¼ä¸‹æ˜¯å¦åŒ…å« tags ä½œä¸ºå€™è¡¥ï¼ˆåº”å¯¹åˆ†ç±»é”™è¯¯ï¼‰
            
        Returns:
            è¿‡æ»¤åçš„å®ä½“åˆ—è¡¨
        """
        if not entities:
            return entities
            
        original_count = len(entities)
        
        # ä¼˜å…ˆä½¿ç”¨ç™½åå•ï¼ˆfocus_entity_typesï¼‰
        if config.focus_entity_types:
            focus_types = set(config.focus_entity_types)
            
            # tags ä½œä¸ºå…œåº•ï¼ˆåº”å¯¹åˆ†ç±»åå·®ï¼‰
            if include_tags_backup:
                focus_types.add("tags")
            
            filtered = [e for e in entities if e.get("type") in focus_types]
            filtered_count = original_count - len(filtered)
            if filtered_count > 0:
                self.logger.info(
                    f"ğŸ¯ {context}ç™½åå•è¿‡æ»¤: {original_count} â†’ {len(filtered)} "
                    f"(-{filtered_count}ä¸ª, èšç„¦ç±»å‹: {list(focus_types)[:8]})"
                )
            return filtered
        
        # å›é€€åˆ°é»‘åå•ï¼ˆexclude_entity_typesï¼‰
        if config.exclude_entity_types:
            exclude_types = set(config.exclude_entity_types)
            filtered = [e for e in entities if e.get("type") not in exclude_types]
            filtered_count = original_count - len(filtered)
            if filtered_count > 0:
                self.logger.info(
                    f"ğŸš« {context}é»‘åå•è¿‡æ»¤: {original_count} â†’ {len(filtered)} "
                    f"(-{filtered_count}ä¸ª, æ’é™¤ç±»å‹: {list(exclude_types)})"
                )
            return filtered
        
        return entities

    async def search(self, config: SearchConfig) -> RecallResult:
        """
        æ‰§è¡Œ8æ­¥éª¤æœç´¢ç®—æ³•

        Args:
            config: æœç´¢é…ç½®

        Returns:
            å®ä½“å¬å›ç»“æœ
        """
        try:
            # ä¿å­˜åŸå§‹queryç”¨äºç»“æœè¿½è¸ªï¼ˆå¿…é¡»åœ¨step1ä¹‹å‰ï¼‰
            original_query = config.query

            # ğŸ†• åˆ›å»ºçº¿ç´¢æ„å»ºå™¨
            tracker = Tracker(config)

            # åˆå§‹åŒ–æ­¥éª¤è®¡æ—¶å™¨
            step_timings = {}
            total_start = time.perf_counter()

            source_config_ids = config.get_source_config_ids()
            self.logger.info(
                f"å¼€å§‹å®ä½“å¬å›ï¼šsource_config_ids={source_config_ids[:5]}{'...' if len(source_config_ids) > 5 else ''}, "
                f"source_config_id_count={len(source_config_ids)}, query={config.query}"
            )

            # === æ­¥éª¤1: queryæ‰¾keyï¼ˆè¯­ä¹‰æ‰©å±•ï¼‰ ===
            step1_start = time.perf_counter()
            key_query_related, k1_weights, step1_substep_timings = await self._step1_query_to_keys(config)
            step1_end = time.perf_counter()
            step_timings["step1"] = step1_end - step1_start
            self.logger.info(f"æ­¥éª¤1å®Œæˆï¼šæ‰¾åˆ° {len(key_query_related)} ä¸ªç›¸å…³keyï¼Œè€—æ—¶: {step_timings['step1']:.3f}s")

            # ğŸ†• è®°å½•çº¿ç´¢ï¼šquery â†’ entityï¼ˆä»…å¿«é€Ÿæ¨¡å¼ï¼‰
            # æ™®é€šæ¨¡å¼ä¸‹ï¼Œçº¿ç´¢å·²ç»åœ¨ step1 å†…éƒ¨é€šè¿‡ extracted_entity â†’ entity è®°å½•äº†
            # å¿«é€Ÿæ¨¡å¼ä¸‹ï¼Œéœ€è¦åœ¨è¿™é‡Œè®°å½• query â†’ entityï¼Œå› ä¸ºæ˜¯ç›´æ¥ç”¨ query å¬å›çš„
            if config.recall.use_fast_mode:
                for entity in key_query_related:
                    # è·å–å®ä½“æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                    entity_weight = entity.get("weight")
                    metadata = {
                        "method": "vector_search",
                        "step": "step1",
                        # ğŸ†• æ·»åŠ æ¥æºå±æ€§
                        "source_attribute": entity.get("source_attribute")
                    }
                    # åªæœ‰toèŠ‚ç‚¹æ˜¯å®ä½“æ—¶æ‰å­˜å‚¨weight
                    if entity_weight is not None:
                        metadata["weight"] = entity_weight

                    tracker.add_clue(
                        stage="recall",
                        from_node=Tracker.build_query_node(config),
                        to_node=Tracker.build_entity_node(entity),
                        confidence=entity.get("similarity", 0.0),  # ç»Ÿä¸€ä½¿ç”¨similarity
                        metadata=metadata
                    )
                self.logger.debug(f"âœ… å¿«é€Ÿæ¨¡å¼ï¼šä¸º {len(key_query_related)} ä¸ªå®ä½“åˆ›å»ºäº† query â†’ entity çº¿ç´¢")

            # ğŸ” æ˜¾ç¤ºå¬å›å®ä½“çš„è¯¦ç»†ä¿¡æ¯
            if key_query_related:
                self.logger.info(f"ğŸ“‹ æ­¥éª¤1å¬å›å®ä½“è¯¦æƒ… (å…±{len(key_query_related)}ä¸ª):")
                for idx, entity in enumerate(key_query_related, 1):
                    self.logger.info(
                        f"  {idx}. å®ä½“ID: {entity.get('entity_id')}, "
                        f"åç§°: '{entity.get('name')}', "
                        f"ç±»å‹: {entity.get('type')}, "
                        f"ç›¸ä¼¼åº¦: {entity.get('similarity', 0.0):.4f}, "
                        f"æ¥æºå±æ€§: '{entity.get('source_attribute')}'"
                    )


            # å­˜å‚¨queryå¬å›çš„æ‰€æœ‰keyåˆ°configä¸­
            config.query_recalled_keys = key_query_related
            self.logger.debug(
                f"å·²å°† {len(key_query_related)} ä¸ªqueryå¬å›çš„keyå­˜å‚¨åˆ°config.query_recalled_keys")

            # === æ­¥éª¤2: keyæ‰¾eventï¼ˆç²¾å‡†åŒ¹é…ï¼‰ ===
            step2_start = time.perf_counter()
            event_key_query_related = await self._step2_keys_to_events(config, key_query_related)
            step2_end = time.perf_counter()
            step_timings["step2"] = step2_end - step2_start
            self.logger.info(
                f"æ­¥éª¤2å®Œæˆï¼šæ‰¾åˆ° {len(event_key_query_related)} ä¸ªkeyç›¸å…³eventï¼Œè€—æ—¶: {step_timings['step2']:.3f}s")

            # === æ­¥éª¤2.5: queryç›´æ¥æœç´¢eventsï¼ˆå¯é€‰ï¼‰===
            query_events = []
            query_event_similarities = {}  # ğŸ†• å­˜å‚¨queryå¬å›eventsçš„ç›¸ä¼¼åº¦
            if config.recall.use_query_event_search:
                step2_5_start = time.perf_counter()
                query_events, query_event_similarities = await self._step2_query_to_events(config)
                step2_5_end = time.perf_counter()
                step_timings["step2_5"] = step2_5_end - step2_5_start
                self.logger.info(
                    f"æ­¥éª¤2.5å®Œæˆï¼šæ‰¾åˆ° {len(query_events)} ä¸ªqueryç›¸å…³eventï¼Œ"
                    f"è€—æ—¶: {step_timings['step2_5']:.3f}s"
                )

            # åˆå¹¶eventsï¼ˆå»é‡ï¼‰
            all_events = list(set(event_key_query_related + query_events))
            self.logger.info(
                f"ğŸ“Š Eventsåˆå¹¶: keyç›¸å…³={len(event_key_query_related)}, "
                f"queryç›¸å…³={len(query_events)}, æ€»è®¡={len(all_events)}"
            )

            # === æ­¥éª¤3: è¿‡æ»¤Eventï¼ˆæƒé‡æ’åºï¼‰ ===
            step3_start = time.perf_counter()
            event_related, key_query_related, e1_weights = await self._step3_filter_events(
                all_events,
                key_query_related,
                k1_weights,
                config,
                query_event_similarities,  # ğŸ†• ä¼ é€’queryå¬å›eventsçš„ç›¸ä¼¼åº¦
            )
            step3_end = time.perf_counter()
            step_timings["step3"] = step3_end - step3_start
            self.logger.info(
                f"æ­¥éª¤3å®Œæˆï¼šè¿‡æ»¤å {len(event_related)} ä¸ªevent, {len(key_query_related)} ä¸ªkeyï¼Œè€—æ—¶: {step_timings['step3']:.3f}s")

            # ğŸ” æ˜¾ç¤ºkeyä¿ç•™æƒ…å†µï¼ˆæ­¥éª¤3ä¸å†è¿‡æ»¤keyï¼‰
            original_key_count = len(key_query_related)
            retained_keys_count = len(key_query_related)

            self.logger.info(
                f"ğŸ” [Step3] Keyä¿ç•™æƒ…å†µ: "
                f"æ­¥éª¤1å¬å›={original_key_count}ä¸ª â†’ "
                f"æ­¥éª¤3ä¿ç•™å…¨éƒ¨={retained_keys_count}ä¸ª "
                f"(æ­¥éª¤1å·²é€šè¿‡max_entities={config.recall.max_entities}é™åˆ¶æ•°é‡)"
            )

            # ğŸ” æ˜¾ç¤ºæ­¥éª¤3ä¿ç•™çš„keyè¯¦æƒ…
            if key_query_related:
                # æ­¥éª¤3ä¿ç•™äº†æ‰€æœ‰keyï¼Œç›´æ¥ä½¿ç”¨key_query_related
                retained_key_infos = key_query_related

                self.logger.info(f"ğŸ“‹ æ­¥éª¤3è¿‡æ»¤åä¿ç•™çš„keyè¯¦æƒ… (å…±{len(retained_key_infos)}ä¸ª):")
                for idx, key_info in enumerate(retained_key_infos, 1):
                    self.logger.info(
                        f"  {idx}. å®ä½“ID: {key_info['entity_id']}, "
                        f"åç§°: '{key_info['name']}', "
                        f"ç±»å‹: {key_info['type']}, "
                        f"åŸå§‹ç›¸ä¼¼åº¦: {key_info.get('similarity', 0.0):.4f}, "
                        f"æ¥æºå±æ€§: '{key_info.get('source_attribute', 'N/A')}'"
                    )
            else:
                self.logger.warning("âš ï¸ æ­¥éª¤3åæ²¡æœ‰ä¿ç•™ä»»ä½•keyï¼Œåç»­æ­¥éª¤å°†æ— ç»“æœ")


            # === æ­¥éª¤4: è®¡ç®—event-keyæƒé‡å‘é‡ ===
            step4_start = time.perf_counter()
            event_key_weights = await self._step4_calculate_event_key_weights(
                event_related, key_query_related, k1_weights, e1_weights, config
            )
            step4_end = time.perf_counter()
            step_timings["step4"] = step4_end - step4_start
            self.logger.info(
                f"æ­¥éª¤4å®Œæˆï¼šè®¡ç®—äº† {len(event_key_weights)} ä¸ªeventçš„keyæƒé‡ï¼Œè€—æ—¶: {step_timings['step4']:.3f}s")

            # === æ­¥éª¤5: åå‘è®¡ç®—keyæƒé‡å‘é‡ ===
            step5_start = time.perf_counter()
            key_event_weights = await self._step5_calculate_key_event_weights(
                event_related, key_query_related, event_key_weights, k1_weights, config
            )
            step5_end = time.perf_counter()
            step_timings["step5"] = step5_end - step5_start
            self.logger.info(f"æ­¥éª¤5å®Œæˆï¼šè®¡ç®—äº† {len(key_event_weights)} ä¸ªkeyçš„åå‘æƒé‡ï¼Œè€—æ—¶: {step_timings['step5']:.3f}s")


            # === æ­¥éª¤6: æå–é‡è¦çš„key ===
            step6_start = time.perf_counter()
            key_final = await self._step6_extract_important_keys(
                key_event_weights, config, key_query_related  # ğŸ†• ä¼ å…¥ key_query_related
            )
            step6_end = time.perf_counter()
            step_timings["step6"] = step6_end - step6_start
            self.logger.info(f"æ­¥éª¤6å®Œæˆï¼šæå–äº† {len(key_final)} ä¸ªé‡è¦keyï¼Œè€—æ—¶: {step_timings['step6']:.3f}s")

            # ğŸ” åˆ†ææœ€ç»ˆkeyçš„è¿‡æ»¤æƒ…å†µ
            if key_final:
                self.logger.info(
                    f"ğŸ” [Step6] æœ€ç»ˆç»“æœ: "
                    f"æ­¥éª¤1å¬å›={len(key_query_related)}ä¸ª â†’ "
                    f"æ­¥éª¤3ä¿ç•™={len(key_query_related)}ä¸ª â†’ "
                    f"æ­¥éª¤6æå–={len(key_final)}ä¸ª"
                )


            # === ğŸ†• æ­¥éª¤6å®Œæˆåï¼šç”Ÿæˆæœ€ç»ˆçº¿ç´¢ (display_level="final") ===
            # ä¸ºæœ€ç»ˆä¿ç•™çš„entityç”Ÿæˆçº¿ç´¢ï¼Œæ ¹æ® query_source å†³å®šçº¿ç´¢æ¥æº
            # - origin query å¬å›çš„å®ä½“ï¼šorigin_query â†’ entity
            # - rewrite query å¬å›çš„å®ä½“ï¼šrewrite_query â†’ entity
            if key_final:
                self.logger.info(f"ğŸ¯ [Step6] ç”Ÿæˆ {len(key_final)} æ¡æœ€ç»ˆçº¿ç´¢ (display_level=final)")

                for key in key_final:
                    # ä» key_query_related ä¸­æ‰¾åˆ°åŸå§‹entityä¿¡æ¯
                    original_entity = next(
                        (e for e in key_query_related if e["entity_id"] == key["key_id"]),
                        None
                    )

                    # è·å–å®ä½“æƒé‡ä¿¡æ¯
                    entity_weight = key.get("weight")

                    if original_entity:
                        # ğŸ”‘ æ ¹æ® query_source å†³å®šçº¿ç´¢çš„èµ·ç‚¹
                        query_source = original_entity.get("query_source", "origin")  # é»˜è®¤ origin
                        use_origin = (query_source == "origin")

                        # query å¬å›çš„ key
                        metadata = {
                            "method": "final_result",
                            "step": "step6",
                            "steps": key.get("steps", [1]),
                            "source_attribute": original_entity.get("source_attribute"),
                            "query_source": query_source  # ğŸ†• è®°å½•æ¥æº
                        }
                        if entity_weight is not None:
                            metadata["weight"] = entity_weight

                        tracker.add_clue(
                            stage="recall",
                            from_node=Tracker.build_query_node(config, use_origin=use_origin),  # ğŸ”‘ æ ¹æ®æ¥æºé€‰æ‹© query
                            to_node=Tracker.build_entity_node(original_entity),
                            confidence=original_entity.get("similarity", 0.0),
                            relation="è¯­ä¹‰ç›¸ä¼¼" if query_source == "origin" else ("LLMå®ä½“è¯†åˆ«" if query_source == "rewrite" else "åˆ†è¯å¬å›"),  # ğŸ”‘ åŒºåˆ†å…³ç³»
                            display_level="final",
                            metadata=metadata
                        )
                        self.logger.debug(
                            f"âœ… [Step6] ä¸º {query_source} query å¬å›çš„å®ä½“ '{original_entity['name']}' ç”Ÿæˆ final çº¿ç´¢"
                        )
                    else:
                        # ğŸ†• ä» event æ¿€æ´»çš„ keyï¼ˆä¸åœ¨ key_query_related ä¸­ï¼‰
                        # ä½¿ç”¨ key_final ä¸­å·²æœ‰çš„ä¿¡æ¯æ„å»º entity èŠ‚ç‚¹
                        event_activated_entity = {
                            "entity_id": key["key_id"],
                            "name": key["name"],
                            "type": key["type"],
                        }
                        metadata = {
                            "method": "event_activated",
                            "step": "step6",
                            "steps": key.get("steps", [1]),
                            "source_attribute": "event_activation"  # æ ‡è®°æ¥æºä¸ºäº‹ä»¶æ¿€æ´»
                        }
                        if entity_weight is not None:
                            metadata["weight"] = entity_weight

                        tracker.add_clue(
                            stage="recall",
                            from_node=Tracker.build_query_node(config),
                            to_node=Tracker.build_entity_node(event_activated_entity),
                            confidence=entity_weight if entity_weight else 0.0,  # ä½¿ç”¨æƒé‡ä½œä¸ºç½®ä¿¡åº¦
                            relation="äº‹ä»¶å…³è”",  # ğŸ†• åŒºåˆ†äºè¯­ä¹‰ç›¸ä¼¼
                            display_level="final",
                            metadata=metadata
                        )
                        self.logger.debug(
                            f"âœ… [Step6] ä¸ºäº‹ä»¶æ¿€æ´»çš„ key '{key['name']}' ç”Ÿæˆçº¿ç´¢"
                        )

                self.logger.info(
                    f"âœ… [Step6] æœ€ç»ˆçº¿ç´¢ç”Ÿæˆå®Œæˆï¼Œå‰ç«¯å¯æ ¹æ®è¿™äº› final çº¿ç´¢åæ¨å®Œæ•´æ¨ç†è·¯å¾„"
                )
            else:
                self.logger.warning(
                    f"âš ï¸ [Step6] æ²¡æœ‰ç”Ÿæˆä»»ä½•æœ€ç»ˆçº¿ç´¢ï¼key_final ä¸ºç©ºã€‚"
                    f"è¿™å¯èƒ½å¯¼è‡´å‰ç«¯ç²¾ç®€æ¨¡å¼å›¾è°±ä¸ºç©ºã€‚"
                    f"å»ºè®®æ£€æŸ¥é…ç½®å‚æ•°ï¼štop_n_keys={config.recall.final_entity_count}, "
                    f"final_key_threshold={config.recall.entity_weight_threshold}"
                )


            # === æ„å»ºRecallé˜¶æ®µçº¿ç´¢ ===
            # ä½¿ç”¨config.query_recalled_keysï¼ˆå·²åœ¨step6ä¸­è¿‡æ»¤å¹¶æ›´æ–°ä¸ºkey_finalæ ¼å¼ï¼‰
            recall_clues = await self._build_recall_clues(
                config, config.query_recalled_keys)
            config.recall_clues = recall_clues
            self.logger.info(
                f"âœ¨ æ„å»ºäº† {len(recall_clues)} æ¡Recallçº¿ç´¢ (query â†’ entity), "
                f"è¿™äº›æ˜¯æ­¥éª¤1ç›´æ¥å¬å›ä¸”åœ¨æœ€ç»ˆç»“æœä¸­çš„å®ä½“"
            )

            # è®¡ç®—recallæ€»è€—æ—¶
            total_end = time.perf_counter()
            step_timings["total"] = total_end - total_start
            self.logger.info(
                f"å®ä½“å¬å›å®Œæˆï¼šè¿”å› {len(key_final)} ä¸ªé‡è¦keyï¼Œæ€»è€—æ—¶: {step_timings['total']:.3f}s"
            )

            result = RecallResult(
                original_query=original_query,
                key_final=key_final,
                key_query_related=key_query_related,
                event_key_query_related=event_key_query_related,
                event_related=event_related,
                event_key_weights=event_key_weights,
                key_event_weights=key_event_weights,
                step_timings=step_timings,
                step1_substep_timings=step1_substep_timings,
            )

            return result

        except Exception as e:
            self.logger.error(f"å®ä½“å¬å›å¤±è´¥: {e}", exc_info=True)
            raise

    # === æ­¥éª¤å®ç°æ–¹æ³• ===

    async def _step1_query_to_keys(
        self, config: SearchConfig
    ) -> Tuple[List[Dict[str, Any]], Dict[str, float], Dict[str, float]]:
        """
        æ­¥éª¤1: queryæ‰¾keyï¼ˆè¯­ä¹‰æ‰©å±•ï¼‰
        LLMæŠ½å–queryçš„ç»“æ„åŒ–å±æ€§ï¼Œé€šè¿‡å‘é‡ç›¸ä¼¼åº¦æ‰¾åˆ°å…³è”å®ä½“

        å¦‚æœå¯ç”¨äº†queryé‡å†™ï¼Œä¼šç›´æ¥ä¿®æ”¹config.queryä¸ºé‡å†™åçš„queryï¼Œ
        è¿™æ ·åç»­çš„æ¨¡å—éƒ½ä¼šè‡ªåŠ¨ä½¿ç”¨é‡å†™åçš„query

        Returns:
            Tuple[List[Dict[str, Any]], Dict[str, float], Dict[str, float]]:
                (key_query_related, k1_weights, step1_substep_timings)
        """
        # TODO: å®Œå–„LLMå±æ€§æŠ½å–å®ç°
        # å½“å‰å®ç°ï¼š
        # 1. ä½¿ç”¨ç®€å•è§„åˆ™ä»queryä¸­æå–å±æ€§ï¼ˆå ä½ç¬¦ï¼‰
        # 2. å°†å±æ€§è½¬æ¢ä¸ºå‘é‡ï¼ˆå ä½ç¬¦å®ç°ï¼‰
        # 3. ä½¿ç”¨å‘é‡æœç´¢æ‰¾åˆ°ç›¸ä¼¼å®ä½“

        self.logger.info(
            f"æ­¥éª¤1å¼€å§‹: query='{config.query}', "
            f"key_similarity_threshold={config.recall.entity_similarity_threshold}, "
            f"max_keys={config.recall.max_entities}, "
            f"source_config_ids={config.get_source_config_ids()[:5]}{'...' if len(config.get_source_config_ids()) > 5 else ''}, "
            f"source_config_id_count={len(config.get_source_config_ids())}, "
            f"use_fast_mode={config.recall.use_fast_mode}"
        )

        step1_substep_timings = {}
        # é‡ç½®æœ¬æ¬¡æŸ¥è¯¢çš„åˆ†è¯å®ä½“é›†åˆï¼Œç”¨äºåç»­åŠ¨æ€æƒé‡è°ƒæ•´
        config.tokenizer_entity_ids = set()

        # å¿«é€Ÿæ¨¡å¼ï¼šç›´æ¥ç”¨queryçš„embeddingå¬å›keyï¼Œè·³è¿‡LLMå±æ€§æŠ½å–å’Œqueryé‡å†™
        if config.recall.use_fast_mode:
            self.logger.info("ğŸš€ ä½¿ç”¨å¿«é€Ÿæ¨¡å¼ï¼šè·³è¿‡LLMå±æ€§æŠ½å–ï¼Œç›´æ¥ä½¿ç”¨query embeddingå¬å›key")

            # å¿«é€Ÿæ¨¡å¼ä¸‹ä¹Ÿéœ€è¦è®¾ç½®origin_queryï¼ˆæœªé‡å†™ï¼‰
            config.original_query = config.query

            try:
                substep_start = time.perf_counter()
                # ç”ŸæˆåŸå§‹queryçš„embedding
                self.logger.debug(f"å¼€å§‹ä¸ºquery '{config.query}' ç”Ÿæˆå‘é‡...")
                query_embedding = await self.processor.generate_embedding(config.query)
                self.logger.info(f"âœ… Queryå‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_embedding)}")
                step1_substep_timings["fast_generate_embedding"] = time.perf_counter() - substep_start

                # ç¼“å­˜query_embeddingåˆ°configï¼Œé¿å…é‡å¤ç”Ÿæˆ
                config.query_embedding = query_embedding
                config.has_query_embedding = True
                self.logger.debug("ğŸ“¦ Queryå‘é‡å·²ç¼“å­˜åˆ°configä¸­")

                substep_start = time.perf_counter()
                # ç›´æ¥æœç´¢entityï¼ˆä¸é™åˆ¶entity_typeï¼‰
                source_config_ids = config.get_source_config_ids()
                self.logger.debug(
                    f"å¼€å§‹å‘é‡æœç´¢: k={config.recall.vector_top_k}, source_config_ids={source_config_ids[:5]}{'...' if len(source_config_ids) > 5 else ''} (æ€»é‡={len(source_config_ids)})")
                similar_entities = await self.entity_repo.search_similar(
                    query_vector=query_embedding,
                    k=config.recall.vector_top_k,
                    source_config_ids=config.get_source_config_ids(),  # ä½¿ç”¨å¤šæºæ”¯æŒ
                    entity_type=None,  # ä¸é™åˆ¶ç±»å‹
                    include_type_threshold=True,
                )
                step1_substep_timings["fast_vector_search"] = time.perf_counter() - substep_start

                self.logger.info(f"ğŸ“Š å¿«é€Ÿæ¨¡å¼æœç´¢åˆ° {len(similar_entities)} ä¸ªå€™é€‰å®ä½“")

                # ğŸ†• ç»Ÿä¸€ç±»å‹è¿‡æ»¤ï¼ˆä¼˜å…ˆç™½åå•ï¼Œå›é€€é»‘åå•ï¼‰
                similar_entities = self._filter_entities_by_type(
                    similar_entities, config, context="å¿«é€Ÿæ¨¡å¼"
                )

                substep_start = time.perf_counter()
                # è¿‡æ»¤é˜ˆå€¼
                key_query_related = []
                k1_weights = {}
                passed_count = 0

                for entity in similar_entities:
                    similarity = float(entity.get("_score", 0.0))

                    if similarity >= config.recall.entity_similarity_threshold:
                        # è·å–ç±»å‹é˜ˆå€¼å’Œæƒé‡
                        type_threshold = entity.get("type_threshold", 0.800)
                        type_weight = entity.get("type_weight", 1.0)
                        final_threshold = config.recall.entity_similarity_threshold
                        # è®¡ç®—åŠ æƒåˆ†æ•°ï¼šsimilarity Ã— type_weight
                        effective_score = similarity * type_weight
                        key_query_related.append({
                            "entity_id": entity["entity_id"],
                            "name": entity["name"],
                            "type": entity["type"],
                            "similarity": similarity,
                            "type_weight": type_weight,
                            "effective_score": effective_score,
                            "source_attribute": config.query,
                            "type_threshold": type_threshold,
                            "final_threshold": final_threshold,
                        })
                        k1_weights[entity["entity_id"]] = effective_score
                        passed_count += 1

                step1_substep_timings["fast_filter_threshold"] = time.perf_counter() - substep_start

                self.logger.info(
                    f"ğŸ“ˆ å¿«é€Ÿæ¨¡å¼é˜ˆå€¼è¿‡æ»¤ç»“æœ: "
                    f"é€šè¿‡ {passed_count}/{len(similar_entities)}"
                )

                substep_start = time.perf_counter()
                # å»é‡å¹¶é™åˆ¶æ•°é‡
                seen_entities = set()
                unique_keys = []
                for key_info in key_query_related:
                    entity_id = key_info["entity_id"]
                    if entity_id not in seen_entities:
                        seen_entities.add(entity_id)
                        unique_keys.append(key_info)

                key_query_related = unique_keys[:config.recall.max_entities]
                step1_substep_timings["fast_deduplicate"] = time.perf_counter() - substep_start

                # ğŸ†• åˆ†è¯å™¨è¡¥å……å¬å›
                if config.recall.use_tokenizer:
                    substep_start = time.perf_counter()
                    existing_ids = {e["entity_id"] for e in key_query_related}

                    tokenizer_entities, new_count = await self._tokenizer_match_entities(
                        query=config.query,
                        source_config_ids=config.get_source_config_ids(),
                        top_k=config.recall.tokenizer_top_k,
                        exclude_types=list(config.exclude_entity_types) if not config.focus_entity_types else None,
                        focus_types=config.focus_entity_types or None,
                        existing_entity_ids=existing_ids
                    )
                    step1_substep_timings["fast_tokenizer"] = time.perf_counter() - substep_start

                    # åˆå¹¶ç»“æœ
                    if tokenizer_entities:
                        for entity in tokenizer_entities:
                            # æ™®é€šæ¨¡å¼ï¼šk1_weights ç›´æ¥ä½¿ç”¨ type_weightï¼Œä¸è®¡ç®— similarity
                            type_weight = entity.get("type_weight", 1.0)
                            entity["effective_score"] = type_weight  # ç”¨äºæ’åº
                            key_query_related.append(entity)
                            k1_weights[entity["entity_id"]] = type_weight
                            config.tokenizer_entity_ids.add(entity["entity_id"])

                        self.logger.info(f"ğŸ”€ å¿«é€Ÿæ¨¡å¼åˆ†è¯åˆå¹¶: +{new_count} ä¸ªå®ä½“")

                    # æŒ‰åŠ æƒåˆ†æ•°æ’åºåé™åˆ¶æœ€ç»ˆæ•°é‡
                    key_query_related = sorted(key_query_related, key=lambda x: x.get("effective_score", 0), reverse=True)
                    key_query_related = key_query_related[:config.recall.max_entities]

                self.logger.info(
                    f"ğŸ“‹ å¿«é€Ÿæ¨¡å¼å®Œæˆ: æœ€ç»ˆè¿”å› {len(key_query_related)} ä¸ªkey"
                )

                if len(key_query_related) > 0:
                    top_entities = sorted(
                        key_query_related, key=lambda x: x.get("effective_score", 0), reverse=True)[:3]
                    top_info = [
                        f"'{e['name']}'({e['type']}, {e['similarity']:.3f})"
                        for e in top_entities
                    ]
                    self.logger.info(f"ğŸ† Top 3 ç›¸ä¼¼å®ä½“: {', '.join(top_info)}")

                return key_query_related, k1_weights, step1_substep_timings

            except Exception as e:
                self.logger.error(f"âŒ å¿«é€Ÿæ¨¡å¼å¤±è´¥: {e}")
                import traceback
                self.logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                raise

        # === æ™®é€šæ¨¡å¼ï¼ˆæ–°ç­–ç•¥ï¼‰ï¼šQueryâ†’Event + Keysäº¤é›†è¿‡æ»¤ ===
        self.logger.info("ğŸ”„ ä½¿ç”¨æ™®é€šæ¨¡å¼ï¼ˆæ–°ç­–ç•¥ï¼‰ï¼šQueryâ†’Event + Keysäº¤é›†è¿‡æ»¤")

        # ğŸ†• åˆ›å»ºçº¿ç´¢æ„å»ºå™¨ï¼ˆç»Ÿä¸€æ–¹å¼ï¼‰
        tracker = Tracker(config)

        # ä¿å­˜åŸå§‹query
        original_query = config.query
        config.original_query = original_query

        # =====================================================
        # æ­¥éª¤0: ç”Ÿæˆ Query Embeddingï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
        # =====================================================
        if not getattr(config, 'has_query_embedding', False) or config.query_embedding is None:
            self.logger.info("ğŸ“Œ æ–°ç­–ç•¥æ­¥éª¤0: ç”ŸæˆQueryå‘é‡")
            substep_start = time.perf_counter()
            query_embedding = await self.processor.generate_embedding(config.query)
            config.query_embedding = query_embedding
            config.has_query_embedding = True
            step1_substep_timings["normal_generate_embedding"] = time.perf_counter() - substep_start
            self.logger.info(f"âœ… Queryå‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_embedding)}")

        # =====================================================
        # æ­¥éª¤1: Query ç›´æ¥å¬å› Eventï¼ˆé«˜é˜ˆå€¼ï¼Œä¿è¯è´¨é‡ï¼‰
        # =====================================================
        self.logger.info("ğŸ“Œ æ–°ç­–ç•¥æ­¥éª¤1: Queryâ†’Eventï¼ˆé«˜é˜ˆå€¼å¬å›é«˜è´¨é‡äº‹é¡¹ï¼‰")
        substep_start = time.perf_counter()
        
        high_quality_events = await self.event_repo.search_similar_by_content(
            query_vector=config.query_embedding,
            k=config.recall.query_event_max,
            source_config_ids=config.get_source_config_ids()
        )
        
        # æŒ‰é˜ˆå€¼è¿‡æ»¤
        high_quality_events = [
            e for e in high_quality_events 
            if e.get("_score", 0) >= config.recall.query_event_threshold
        ]
        high_quality_event_ids = {e["event_id"] for e in high_quality_events}
        
        step1_substep_timings["new_step1_query_to_events"] = time.perf_counter() - substep_start
        self.logger.info(
            f"âœ… æ­¥éª¤1å®Œæˆ: å¬å› {len(high_quality_events)} ä¸ªé«˜è´¨é‡äº‹é¡¹ "
            f"(é˜ˆå€¼={config.recall.query_event_threshold})"
        )

        # =====================================================
        # æ­¥éª¤1.2: ä»é«˜è´¨é‡äº‹é¡¹åå‘å¬å›èƒŒæ™¯å®ä½“ï¼ˆç»™LLMå‚è€ƒï¼‰
        # =====================================================
        background_entities = []
        if config.recall.background_entity_enabled and high_quality_events:
            self.logger.info("ğŸ“Œ æ–°ç­–ç•¥æ­¥éª¤1.2: ä»é«˜è´¨é‡äº‹é¡¹åå‘å¬å›èƒŒæ™¯å®ä½“")
            substep_start = time.perf_counter()
            
            # å– top-N é«˜è´¨é‡äº‹é¡¹
            top_n = min(config.recall.background_event_top_n, len(high_quality_events))
            top_hq_event_ids = [e["event_id"] for e in high_quality_events[:top_n]]
            
            # åå‘æŸ¥æ‰¾è¿™äº›äº‹é¡¹å…³è”çš„å®ä½“
            background_entities = await self._reverse_find_entities_by_events(
                event_ids=top_hq_event_ids,
                source_config_ids=config.get_source_config_ids(),
                min_name_length=config.recall.background_entity_min_name_length,
                max_count=config.recall.background_entity_max,
                focus_types=config.focus_entity_types or None
            )
            
            step1_substep_timings["new_step1_2_background_entities"] = time.perf_counter() - substep_start
            
            if background_entities:
                # æ˜¾ç¤ºèƒŒæ™¯å®ä½“ï¼ˆæŒ‰çƒ­åº¦æ’åºçš„å‰5ä¸ªï¼‰
                bg_preview = [f"[{e['type']}]{e['name']}(çƒ­åº¦={e.get('event_count', 0)})" 
                              for e in background_entities[:5]]
                self.logger.info(
                    f"âœ… æ­¥éª¤1.2å®Œæˆ: åå‘å¬å› {len(background_entities)} ä¸ªèƒŒæ™¯å®ä½“ "
                    f"(æ¥è‡ªtop-{top_n}é«˜è´¨é‡äº‹é¡¹)"
                )
                self.logger.info(f"ğŸ“‹ èƒŒæ™¯å®ä½“ç¤ºä¾‹: {bg_preview}")
            else:
                self.logger.info("âš ï¸ æ­¥éª¤1.2: æœªæ‰¾åˆ°èƒŒæ™¯å®ä½“")
        
        # ä¿å­˜èƒŒæ™¯å®ä½“åˆ° configï¼Œä¾›åç»­æ­¥éª¤ä½¿ç”¨
        config.background_entities = background_entities

        # =====================================================
        # æ­¥éª¤1.5: å‘é‡å¬å›å®ä½“ï¼ˆä½œä¸º LLM few-shotsï¼Œå¯é€‰ï¼‰
        # =====================================================
        if config.recall.candidate_entities_enabled:
            self.logger.info("ğŸ“Œ æ–°ç­–ç•¥æ­¥éª¤1.5: å‘é‡å¬å›å€™é€‰å®ä½“ï¼ˆä½œä¸º few-shotsï¼‰")
            substep_start = time.perf_counter()
            candidate_entities, k1_weights = await self._vector_search_entities(config)
            step1_substep_timings["normal_vector_search"] = time.perf_counter() - substep_start

            if not candidate_entities:
                self.logger.warning("âš ï¸ å‘é‡å¬å›æœªæ‰¾åˆ°ä»»ä½•å€™é€‰å®ä½“")
                candidate_entities = []
        else:
            self.logger.info("â­ï¸ æ­¥éª¤1.5: å‘é‡å¬å›å€™é€‰å®ä½“å·²ç¦ç”¨")
            candidate_entities = []
            k1_weights = {}

        # === æ­¥éª¤2: è·å–å®ä½“ç±»å‹ ===
        self.logger.info("ğŸ“Œ æ™®é€šæ¨¡å¼æ­¥éª¤2: è·å–å®ä½“ç±»å‹")
        substep_start = time.perf_counter()
        entity_types = await self._get_entity_types_for_source(
            config.get_source_config_ids()
        )
        step1_substep_timings["normal_get_entity_types"] = time.perf_counter() - substep_start

        if not entity_types:
            self.logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å®ä½“ç±»å‹ï¼Œè·³è¿‡LLMæ‰©å±•")
            # æ²¡æœ‰å®ä½“ç±»å‹æ—¶ï¼Œç›´æ¥è¿”å›å€™é€‰å®ä½“
            return candidate_entities, k1_weights, step1_substep_timings

        # === æ­¥éª¤3: LLMåˆå¹¶è°ƒç”¨ï¼ˆæŸ¥è¯¢é‡å†™ + èšç„¦ç±»å‹ + å®ä½“è¯†åˆ«ï¼‰ ===
        if config.recall.llm_filter_enabled:
            self.logger.info("ğŸ“Œ æ™®é€šæ¨¡å¼æ­¥éª¤3: LLMåˆå¹¶è°ƒç”¨ï¼ˆæŸ¥è¯¢é‡å†™ + èšç„¦ç±»å‹ + å®ä½“è¯†åˆ«ï¼‰")
            substep_start = time.perf_counter()
            rewritten_query, entity_names, focus_types = await self._llm_rewrite_and_extract_entities(
                query=config.query,
                candidate_entities=candidate_entities,
                entity_types=entity_types,
                config=config,
                background_entities=background_entities  # ğŸ†• ä¼ å…¥èƒŒæ™¯å®ä½“
            )
            step1_substep_timings["normal_llm_rewrite_extract"] = time.perf_counter() - substep_start
            
            # focus_types å·²åœ¨ _llm_rewrite_and_extract_entities ä¸­ä¿å­˜åˆ° config
            if focus_types:
                self.logger.info(f"ğŸ¯ èšç„¦å®ä½“ç±»å‹: {focus_types} (å°†ç”¨äºåç»­è¿‡æ»¤)")

            # å¤„ç†æŸ¥è¯¢é‡å†™ç»“æœï¼ˆå§‹ç»ˆæ‰§è¡Œï¼‰
            if rewritten_query and rewritten_query != config.query:
                self.logger.info(f"ğŸ“ Queryé‡å†™: '{config.query}' â†’ '{rewritten_query}'")

                # è®°å½• origin_query â†’ rewrite_query çº¿ç´¢
                origin_query_node = Tracker.build_query_node(config, use_origin=True)
                config.query = rewritten_query  # æ›´æ–° query
                rewrite_query_node = Tracker.build_query_node(config, use_origin=False)

                # ğŸ†• é‡æ–°ç”Ÿæˆ embeddingï¼ˆä½¿ç”¨é‡å†™åçš„ queryï¼‰
                config.query_embedding = await self.processor.generate_embedding(rewritten_query)
                self.logger.info(f"âœ… é‡å†™åQueryå‘é‡é‡æ–°ç”Ÿæˆï¼Œç»´åº¦: {len(config.query_embedding)}")

                tracker.add_clue(
                    stage="prepare",  # ğŸ”§ æŸ¥è¯¢é‡å†™å±äºå‡†å¤‡é˜¶æ®µ
                    from_node=origin_query_node,
                    to_node=rewrite_query_node,
                    confidence=1.0,
                    relation="æŸ¥è¯¢é‡å†™",
                    display_level="intermediate",
                    metadata={"method": "query_rewrite", "step": "step3"}
                )
                self.logger.info(f"ğŸ“ å·²è®°å½• origin_query â†’ rewrite_query çº¿ç´¢")
            else:
                self.logger.info(f"ğŸ“ Queryä¿æŒä¸å˜: '{config.query}'")

            # === æ­¥éª¤4: ç²¾ç¡®æœç´¢ï¼ˆæ ¹æ®recall_modeé€‰æ‹©ESæˆ–MySQLï¼‰ ===
            # æ‰€æœ‰LLMè¯†åˆ«çš„å®ä½“éƒ½ç»è¿‡ç²¾ç¡®æœç´¢
            if config.recall.recall_mode == RecallMode.EXACT:
                self.logger.info("ğŸ“Œ æ™®é€šæ¨¡å¼æ­¥éª¤4: SQLç²¾ç¡®æœç´¢ï¼ˆMySQLï¼‰")

                # Add logging for entity count before exact search
                self.logger.info(f"ğŸ“ SQLç²¾ç¡®æœç´¢å®ä½“æ•°é‡: {len(entity_names)}ä¸ªå®ä½“å°†å‚ä¸ç²¾ç¡®æœç´¢")

                substep_start = time.perf_counter()
                exact_matched_entities = await self._mysql_exact_search_entities(
                    expanded_entities=entity_names,
                    source_config_ids=config.get_source_config_ids(),
                    limit_per_name=config.recall.sql_fuzzy_search_limit,
                    exclude_types=config.exclude_entity_types if not config.focus_entity_types else None,
                    focus_types=config.focus_entity_types or None,
                )
                step1_substep_timings["normal_mysql_exact"] = time.perf_counter() - substep_start
            else:
                self.logger.info("ğŸ“Œ æ™®é€šæ¨¡å¼æ­¥éª¤4: ESç²¾ç¡®æœç´¢")

                # Add logging for entity count before exact search
                self.logger.info(f"ğŸ“ ESç²¾ç¡®æœç´¢å®ä½“æ•°é‡: {len(entity_names)}ä¸ªå®ä½“å°†å‚ä¸ç²¾ç¡®æœç´¢")

                substep_start = time.perf_counter()
                exact_matched_entities = await self._es_exact_search_entities(
                    expanded_entities=entity_names,
                    source_config_ids=config.get_source_config_ids(),
                    limit_per_name=config.recall.sql_fuzzy_search_limit,
                )
                step1_substep_timings["normal_es_exact"] = time.perf_counter() - substep_start

            # ä¸ºç²¾ç¡®æœç´¢çš„å®ä½“è®°å½•çº¿ç´¢ï¼Œå¹¶æ ‡è®°æ¥æº
            # ğŸ”‘ ç²¾ç¡®æœç´¢æ˜¯åŸºäº LLM ä»é‡å†™åçš„ query ä¸­è¯†åˆ«çš„å®ä½“åç§°
            # æ‰€ä»¥çº¿ç´¢åº”è¯¥ä» rewrite_query å‡ºå‘ï¼Œå¹¶æ ‡è®° query_source="rewrite"
            for entity in exact_matched_entities:
                # ğŸ†• æ ‡è®°å®ä½“æ¥æºäº rewrite queryï¼ˆLLMè¯†åˆ«åŸºäºé‡å†™åçš„queryï¼‰
                entity["query_source"] = "rewrite"

                real_entity_dict = {
                    "entity_id": entity["entity_id"],
                    "name": entity["name"],
                    "type": entity["type"],
                }
                metadata = {
                    "method": entity.get("match_method", "exact_search"),
                    "step": "step1",
                    "source_attribute": entity.get("source_attribute"),
                }
                tracker.add_clue(
                    stage="recall",
                    from_node=Tracker.build_query_node(config, use_origin=False),  # ğŸ”‘ ä½¿ç”¨é‡å†™åçš„ query
                    to_node=Tracker.build_entity_node(real_entity_dict),
                    confidence=entity.get("similarity", 0.0),
                    relation="LLMå®ä½“è¯†åˆ«" if config.recall.recall_mode == RecallMode.EXACT else "LLMå®ä½“è¯†åˆ«",
                    display_level="intermediate",
                    metadata=metadata
                )

            # === æ­¥éª¤5: åˆå¹¶ç»“æœ ===
            # æ‰€æœ‰å®ä½“éƒ½ç»è¿‡äº†æœç´¢ï¼Œç›´æ¥ä½¿ç”¨æœç´¢ç»“æœï¼ˆä¸å†ç¡¬æ€§æˆªæ–­25ä¸ªï¼‰
            self.logger.info("ğŸ“Œ æ–°ç­–ç•¥æ­¥éª¤5: åˆå¹¶æœç´¢ç»“æœ")
            substep_start = time.perf_counter()
            key_query_related, k1_weights = self._merge_exact_search_results(
                exact_matched_entities=exact_matched_entities,
                max_count=config.recall.key_max_count,  # ä½¿ç”¨å®‰å…¨ä¸Šé™ï¼Œä¸æ˜¯ç¡¬æ€§25
                entity_types=entity_types  # ä¼ é€’å®ä½“ç±»å‹ä»¥è®¡ç®— type_weight å’Œ effective_score
            )
            step1_substep_timings["normal_merge_results"] = time.perf_counter() - substep_start
        else:
            # æœªå¯ç”¨LLMè¿‡æ»¤ï¼Œç›´æ¥ä½¿ç”¨å€™é€‰å®ä½“
            self.logger.info("â­ï¸ LLMè¿‡æ»¤æœªå¯ç”¨ï¼Œç›´æ¥ä½¿ç”¨å‘é‡å¬å›ç»“æœ")
            key_query_related = candidate_entities[:config.recall.key_max_count]

        # =====================================================
        # æ­¥éª¤6: Queryå¬å›Eventç”¨äºäº¤é›†è¿‡æ»¤ï¼ˆä½é˜ˆå€¼ï¼‰
        # =====================================================
        self.logger.info("ğŸ“Œ æ–°ç­–ç•¥æ­¥éª¤6: Queryâ†’Eventï¼ˆä½é˜ˆå€¼ç”¨äºäº¤é›†è¿‡æ»¤ï¼‰")
        substep_start = time.perf_counter()
        
        filter_events = await self.event_repo.search_similar_by_content(
            query_vector=config.query_embedding,
            k=config.recall.filter_event_max,
            source_config_ids=config.get_source_config_ids()
        )
        
        # æŒ‰ä½é˜ˆå€¼è¿‡æ»¤
        filter_events = [
            e for e in filter_events 
            if e.get("_score", 0) >= config.recall.filter_event_threshold
        ]
        filter_event_ids = {e["event_id"] for e in filter_events}
        
        step1_substep_timings["new_step6_filter_events"] = time.perf_counter() - substep_start
        self.logger.info(
            f"âœ… æ­¥éª¤6å®Œæˆ: å¬å› {len(filter_events)} ä¸ªè¿‡æ»¤äº‹é¡¹ "
            f"(é˜ˆå€¼={config.recall.filter_event_threshold})"
        )

        # =====================================================
        # æ­¥éª¤7: Keysâ†’SQLâ†’Eventsï¼Œç„¶åä¸æ­¥éª¤6å–äº¤é›†è¿‡æ»¤
        # =====================================================
        self.logger.info("ğŸ“Œ æ–°ç­–ç•¥æ­¥éª¤7: Keysâ†’Events + äº¤é›†è¿‡æ»¤")
        substep_start = time.perf_counter()
        
        # è·å–æ‰€æœ‰ keys å…³è”çš„ events
        all_key_ids = [k["entity_id"] for k in key_query_related]
        
        if all_key_ids:
            # æŸ¥è¯¢ key-event å…³è”
            async with self.session_factory() as session:
                from dataflow.db import EventEntity
                query = (
                    select(EventEntity.entity_id, EventEntity.event_id)
                    .where(EventEntity.entity_id.in_(all_key_ids))
                )
                result = await session.execute(query)
                key_event_relations = result.fetchall()
            
            # æ„å»º key â†’ events æ˜ å°„
            key_to_events: Dict[str, Set[str]] = {}
            for entity_id, event_id in key_event_relations:
                if entity_id not in key_to_events:
                    key_to_events[entity_id] = set()
                key_to_events[entity_id].add(event_id)
            
            # äº¤é›†è¿‡æ»¤ï¼šåªä¿ç•™å…³è”åˆ° filter_event_ids çš„ keys
            valid_keys = []
            filtered_out_count = 0
            
            for key in key_query_related:
                entity_id = key["entity_id"]
                key_events = key_to_events.get(entity_id, set())
                
                # æ£€æŸ¥æ˜¯å¦æœ‰äº¤é›†
                intersection = key_events & filter_event_ids
                
                if intersection:
                    # æœ‰äº¤é›†ï¼Œä¿ç•™è¿™ä¸ª key
                    key["related_event_count"] = len(intersection)
                    valid_keys.append(key)
                else:
                    # æ— äº¤é›†ï¼Œè¿‡æ»¤æ‰
                    filtered_out_count += 1
                    self.logger.debug(
                        f"âš ï¸ Key '{key['name']}' è¢«äº¤é›†è¿‡æ»¤æ‰ "
                        f"(å…³è”events={len(key_events)}, ä¸queryæ— äº¤é›†)"
                    )
            
            key_query_related = valid_keys
            self.logger.info(
                f"âœ… äº¤é›†è¿‡æ»¤: {filtered_out_count} ä¸ªkeyè¢«è¿‡æ»¤, "
                f"å‰©ä½™ {len(key_query_related)} ä¸ªæœ‰æ•ˆkey"
            )
        
        step1_substep_timings["new_step7_intersection_filter"] = time.perf_counter() - substep_start

        # =====================================================
        # æ­¥éª¤7.5: å…œåº•æœºåˆ¶ - å¦‚æœäº¤é›†è¿‡æ»¤åå®Œå…¨ä¸ºç©ºï¼ŒåŠ å…¥ top5 èƒŒæ™¯å®ä½“
        # =====================================================
        if len(key_query_related) == 0 and background_entities:
            self.logger.warning("âš ï¸ äº¤é›†è¿‡æ»¤åæ— æœ‰æ•ˆkeyï¼Œå¯ç”¨èƒŒæ™¯å®ä½“å…œåº•")

            # å– top5 èƒŒæ™¯å®ä½“ä½œä¸ºå…œåº•
            top_n = min(5, len(background_entities))
            for bg_entity in background_entities[:top_n]:
                type_weight = bg_entity.get("type_weight", 1.0)

                # ä¸ºèƒŒæ™¯å®ä½“æ·»åŠ å¿…è¦å­—æ®µ
                bg_entity["similarity"] = 0.7  # å…œåº•é»˜è®¤ç›¸ä¼¼åº¦ï¼ˆä»…ç”¨äºæ˜¾ç¤ºï¼‰
                bg_entity["effective_score"] = type_weight  # ç”¨äºæ’åºï¼Œä½† k1_weights åªç”¨ type_weight
                bg_entity["query_source"] = "background_fallback"  # æ ‡è®°ä¸ºå…œåº•æ¥æº

                key_query_related.append(bg_entity)
                # æ™®é€šæ¨¡å¼ï¼šk1_weights ç›´æ¥ä½¿ç”¨ type_weightï¼Œä¸ä½¿ç”¨ effective_score
                k1_weights[bg_entity["entity_id"]] = type_weight
            
            # æ˜¾ç¤ºå…œåº•çš„èƒŒæ™¯å®ä½“
            fallback_preview = [
                f"[{e['type']}]{e['name']}(æƒé‡={e.get('type_weight', 1):.2f})"
                for e in background_entities[:top_n]
            ]
            self.logger.info(f"ğŸ“‹ å…œåº•åŠ å…¥Top{top_n}èƒŒæ™¯å®ä½“: {fallback_preview}")

        # =====================================================
        # æ­¥éª¤8: åˆ†è¯å™¨è¡¥å……å¬å›ï¼ˆå¯é€‰ï¼‰
        # =====================================================
        if config.recall.use_tokenizer and len(key_query_related) < 10:
            # åªæœ‰å½“æœ‰æ•ˆkeyså¤ªå°‘æ—¶æ‰å¯ç”¨åˆ†è¯è¡¥å……
            self.logger.info("ğŸ“Œ æ–°ç­–ç•¥æ­¥éª¤8: åˆ†è¯å™¨è¡¥å……å¬å›ï¼ˆkeysä¸è¶³æ—¶å¯ç”¨ï¼‰")
            substep_start = time.perf_counter()
            existing_ids = {e["entity_id"] for e in key_query_related}

            tokenizer_entities, new_count = await self._tokenizer_match_entities(
                query=config.query,
                source_config_ids=config.get_source_config_ids(),
                top_k=config.recall.tokenizer_top_k,
                exclude_types=list(config.exclude_entity_types) if not config.focus_entity_types else None,
                focus_types=config.focus_entity_types or None,
                existing_entity_ids=existing_ids
            )
            step1_substep_timings["normal_tokenizer"] = time.perf_counter() - substep_start

            if tokenizer_entities:
                # åˆ†è¯å®ä½“ä¹Ÿéœ€è¦äº¤é›†è¿‡æ»¤
                for entity in tokenizer_entities:
                    entity_id = entity["entity_id"]

                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    if entity_id in existing_ids:
                        continue

                    # æ ‡è®°æ¥æº
                    entity["query_source"] = "tokenizer"

                    # æ™®é€šæ¨¡å¼ï¼šk1_weights ç›´æ¥ä½¿ç”¨ type_weightï¼Œä¸è®¡ç®— similarity
                    type_weight = entity.get("type_weight", 1.0)
                    similarity = entity.get("similarity", config.recall.tokenizer_similarity)  # ä»…ç”¨äºæ˜¾ç¤º
                    entity["effective_score"] = type_weight  # ç”¨äºæ’åº

                    key_query_related.append(entity)
                    k1_weights[entity["entity_id"]] = type_weight
                    config.tokenizer_entity_ids.add(entity_id)

                    # è®°å½•çº¿ç´¢
                    tracker.add_clue(
                        stage="recall",
                        from_node=Tracker.build_query_node(config),
                        to_node=Tracker.build_entity_node({
                            "entity_id": entity["entity_id"],
                            "name": entity["name"],
                            "type": entity["type"],
                        }),
                        confidence=similarity,
                        relation="åˆ†è¯å¬å›",
                        display_level="intermediate",
                        metadata={"method": "tokenizer", "step": "step_tokenizer"}
                    )

                self.logger.info(f"ğŸ”€ åˆ†è¯è¡¥å……: +{new_count} ä¸ªå®ä½“")

        # =====================================================
        # æ­¥éª¤8.5: èƒŒæ™¯å®ä½“å…œåº•ï¼ˆå½“keysä¸è¶³æ—¶ä»èƒŒæ™¯å®ä½“è¡¥å……ï¼‰
        # =====================================================
        fallback_threshold = config.recall.background_fallback_threshold
        fallback_max = config.recall.background_fallback_max
        
        if len(key_query_related) < fallback_threshold and background_entities:
            self.logger.info(
                f"ğŸ“Œ æ–°ç­–ç•¥æ­¥éª¤8.5: èƒŒæ™¯å®ä½“å…œåº• "
                f"(å½“å‰keys={len(key_query_related)} < é˜ˆå€¼{fallback_threshold})"
            )
            
            # è·å–å·²å­˜åœ¨çš„å®ä½“ID
            existing_ids = {k["entity_id"] for k in key_query_related}
            
            # ä»èƒŒæ™¯å®ä½“ä¸­è¡¥å……ï¼ˆå·²æŒ‰çƒ­åº¦æ’åºï¼‰
            added_count = 0
            for entity in background_entities:
                if entity["entity_id"] not in existing_ids:
                    # æ™®é€šæ¨¡å¼ï¼šk1_weights ç›´æ¥ä½¿ç”¨ type_weightï¼Œä¸è®¡ç®— similarity
                    type_weight = entity.get("type_weight", 1.0)

                    # æ ‡è®°æ¥æºå¹¶è®¾ç½®æƒé‡
                    entity["source"] = "background_fallback"
                    entity["similarity"] = config.recall.background_entity_default_similarity  # ä»…ç”¨äºæ˜¾ç¤º
                    entity["type_weight"] = type_weight
                    entity["effective_score"] = type_weight  # ç”¨äºæ’åºï¼Œä½† k1_weights åªç”¨ type_weight
                    entity["source_attribute"] = f"èƒŒæ™¯å®ä½“(çƒ­åº¦={entity.get('event_count', 0)})"

                    key_query_related.append(entity)
                    k1_weights[entity["entity_id"]] = type_weight
                    existing_ids.add(entity["entity_id"])
                    added_count += 1
                    
                    if added_count >= fallback_max:
                        break
            
            if added_count > 0:
                self.logger.info(
                    f"ğŸ”’ èƒŒæ™¯å…œåº•: è¡¥å…… {added_count} ä¸ªèƒŒæ™¯å®ä½“ "
                    f"(æ¥è‡ªé«˜è´¨é‡äº‹é¡¹ï¼ŒæŒ‰çƒ­åº¦æ’åº)"
                )
                # æ‰“å°è¡¥å……çš„å®ä½“
                added_names = [
                    f"[{e['type']}]{e['name']}(çƒ­åº¦={e.get('event_count', 0)})" 
                    for e in background_entities[:added_count]
                ][:5]
                self.logger.info(f"ğŸ“‹ å…œåº•å®ä½“ç¤ºä¾‹: {added_names}")
        else:
            if len(key_query_related) >= fallback_threshold:
                self.logger.info(
                    f"âœ… æ­¥éª¤8.5: keyså……è¶³ ({len(key_query_related)} >= {fallback_threshold})ï¼Œæ— éœ€å…œåº•"
                )

        # =====================================================
        # æ­¥éª¤9: åŒç­–ç•¥æˆªæ–­ï¼ˆç›¸ä¼¼åº¦é˜ˆå€¼ + å®‰å…¨ä¸Šé™ï¼‰
        # =====================================================
        self.logger.info("ğŸ“Œ æ–°ç­–ç•¥æ­¥éª¤9: åŒç­–ç•¥æˆªæ–­")
        
        # å…ˆæŒ‰ effective_score è¿‡æ»¤
        before_count = len(key_query_related)
        key_query_related = [
            k for k in key_query_related 
            if k.get("effective_score", 0) >= config.recall.key_score_threshold
        ]
        score_filtered = before_count - len(key_query_related)
        
        # å†æŒ‰å®‰å…¨ä¸Šé™æˆªæ–­
        key_query_related = sorted(
            key_query_related, 
            key=lambda x: x.get("effective_score", 0), 
            reverse=True
        )[:config.recall.key_max_count]
        
        self.logger.info(
            f"âœ… åŒç­–ç•¥æˆªæ–­: åˆ†æ•°è¿‡æ»¤æ‰{score_filtered}ä¸ª "
            f"(é˜ˆå€¼={config.recall.key_score_threshold}), "
            f"æœ€ç»ˆä¿ç•™ {len(key_query_related)} ä¸ªkey "
            f"(ä¸Šé™={config.recall.key_max_count})"
        )

        # æ›´æ–° k1_weightsï¼ˆåªä¿ç•™æœ‰æ•ˆçš„ï¼‰
        valid_entity_ids = {k["entity_id"] for k in key_query_related}
        k1_weights = {eid: w for eid, w in k1_weights.items() if eid in valid_entity_ids}

        # ä¿å­˜é«˜è´¨é‡äº‹é¡¹IDåˆ°configï¼Œä¾›åç»­æ­¥éª¤ä½¿ç”¨
        config.high_quality_event_ids = high_quality_event_ids

        # æ±‡æ€»æ—¥å¿—
        self.logger.info(
            f"ğŸ“‹ æ–°ç­–ç•¥å¬å›å®Œæˆ: "
            f"é«˜è´¨é‡äº‹é¡¹={len(high_quality_event_ids)}, "
            f"æœ€ç»ˆkeys={len(key_query_related)}"
        )

        if len(key_query_related) > 0:
            # æ˜¾ç¤ºæœ€é«˜åŠ æƒåˆ†æ•°çš„å‡ ä¸ªå®ä½“
            top_entities = sorted(
                key_query_related, key=lambda x: x.get("effective_score", 0), reverse=True)[:3]
            top_info = [
                f"'{e['name']}'({e['type']}, sim={e.get('similarity', 0):.3f}, w={e.get('type_weight', 1):.2f})"
                for e in top_entities
            ]
            self.logger.info(f"ğŸ† Top 3 å®ä½“: {', '.join(top_info)}")
        else:
            self.logger.error("âŒ æ™®é€šæ¨¡å¼æ­¥éª¤1æœ€ç»ˆç»“æœ: æœªæ‰¾åˆ°ä»»ä½•Keysï¼")

        return key_query_related, k1_weights, step1_substep_timings

    async def _step2_keys_to_events(
        self, config: SearchConfig, key_query_related: List[Dict[str, Any]]
    ) -> List[str]:
        """
        æ­¥éª¤2: keyæ‰¾eventï¼ˆç²¾å‡†åŒ¹é…ï¼‰
        é€šè¿‡[key-query-related]ç”¨sqlæ‰¾åˆ°æ‰€æœ‰å…³è”äº‹é¡¹

        åŒæ—¶è®°å½•çº¿ç´¢ï¼šentity â†’ event
        """
        if not key_query_related:
            return []

        key_entity_ids = [key["entity_id"] for key in key_query_related]

        # ğŸ†• æ„å»º entity_id â†’ source_attribute æ˜ å°„
        entity_source_map = {
            key["entity_id"]: key.get("source_attribute")
            for key in key_query_related
        }

        # ğŸ†• åˆ›å»ºçº¿ç´¢æ„å»ºå™¨è®°å½•çº¿ç´¢
        tracker = Tracker(config)

        async with self.session_factory() as session:
            # æŸ¥è¯¢entity-eventå…³ç³»ï¼ˆè¿”å›å®Œæ•´æ˜ å°„ï¼Œç”¨äºè®°å½•çº¿ç´¢ï¼‰
            query = (
                select(EventEntity.entity_id, EventEntity.event_id)
                .where(EventEntity.entity_id.in_(key_entity_ids))
            )

            result = await session.execute(query)
            entity_event_pairs = result.fetchall()

            # ğŸ†• è®°å½•çº¿ç´¢ï¼šentity â†’ eventï¼ˆä½¿ç”¨æ ‡å‡†èŠ‚ç‚¹ï¼ŒæŸ¥è¯¢eventå¯¹è±¡è·å–å®Œæ•´ä¿¡æ¯ï¼‰
            # å…ˆæ‰¹é‡æŸ¥è¯¢eventå¯¹è±¡
            event_ids_for_query = list(
                set(event_id for _, event_id in entity_event_pairs))
            events_query = select(SourceEvent).where(
                SourceEvent.id.in_(event_ids_for_query))
            events_result = await session.execute(events_query)
            events = {event.id: event for event in events_result.scalars().all()}

            # åŒæ—¶æŸ¥è¯¢entityå¯¹è±¡
            entities_query = select(Entity).where(
                Entity.id.in_(key_entity_ids))
            entities_result = await session.execute(entities_query)
            entities = {
                entity.id: entity for entity in entities_result.scalars().all()}

            # è®°å½•æ¯ä¸ªentityâ†’eventçš„çº¿ç´¢
            for entity_id, event_id in entity_event_pairs:
                entity_obj = entities.get(entity_id)
                event_obj = events.get(event_id)

                # æ„å»ºentityå’ŒeventèŠ‚ç‚¹
                if entity_obj:
                    entity_dict = {
                        "id": entity_obj.id,
                        "entity_id": entity_obj.id,  # å…¼å®¹å­—æ®µ
                        "name": entity_obj.name,
                        "type": entity_obj.type,
                        "description": entity_obj.description or "",
                        # ğŸ†• æ·»åŠ æ¥æºå±æ€§
                        "source_attribute": entity_source_map.get(entity_id)
                    }
                else:
                    # Fallback
                    entity_dict = {
                        "id": entity_id,
                        "entity_id": entity_id,
                        # ğŸ†• æ·»åŠ æ¥æºå±æ€§
                        "source_attribute": entity_source_map.get(entity_id)
                    }

                if event_obj:
                    # ä»å®ä½“å­—å…¸ä¸­è·å–ç›¸ä¼¼åº¦ä½œä¸ºconfidenceï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    entity_similarity = entity_dict.get("similarity", 1.0)
                    metadata = {
                        "method": "database_lookup",
                        "step": "step2",
                        # ğŸ†• æ·»åŠ åˆ°metadata
                        "source_attribute": entity_dict.get("source_attribute")
                    }
                    # toèŠ‚ç‚¹æ˜¯äº‹ä»¶ï¼Œä¸å­˜å‚¨weight

                    # tracker.add_clue(
                    #     stage="recall",
                    #     from_node=Tracker.build_entity_node(entity_dict),
                    #     to_node=tracker.get_or_create_event_node(event_obj, "recall"),
                    #     confidence=entity_similarity,  # ä½¿ç”¨å®ä½“çš„ç›¸ä¼¼åº¦
                    #     display_level="intermediate",  # ğŸ†• ä¸­é—´ç»“æœ
                    #     metadata=metadata
                    # )

            # è¿”å›å»é‡çš„event_ids
            event_ids = list(
                set(event_id for _, event_id in entity_event_pairs))

        return event_ids

    async def _step2_query_to_events(
        self, config: SearchConfig
    ) -> Tuple[List[str], Dict[str, float]]:
        """
        Step 2.5: ä½¿ç”¨é‡å†™åçš„queryç›´æ¥æœç´¢events

        é€šè¿‡å‘é‡ç›¸ä¼¼åº¦ï¼ˆæ··åˆcontentå’Œtitleï¼‰æœç´¢ä¸queryç›¸å…³çš„events

        Args:
            config: æœç´¢é…ç½®

        Returns:
            Tuple[List[str], Dict[str, float]]: (äº‹ä»¶IDåˆ—è¡¨, {event_id: hybrid_similarity})
        """
        if not config.query:
            self.logger.warning("æŸ¥è¯¢ä¸ºç©ºï¼Œè·³è¿‡queryç›´æ¥æœç´¢events")
            return [], {}

        self.logger.info(
            f"æ­¥éª¤2.5å¼€å§‹: ä½¿ç”¨queryæœç´¢eventsï¼Œ"
            f"query='{config.query}', "
            f"weight_ratio={config.recall.query_event_weight_ratio}, "
            f"threshold={config.recall.event_similarity_threshold}"
        )

        try:
            # ç¡®ä¿query_embeddingå·²ç”Ÿæˆ
            if not config.has_query_embedding or not config.query_embedding:
                config.query_embedding = await self.processor.generate_embedding(config.query)
                config.has_query_embedding = True
                self.logger.debug(f"ğŸ“¦ ä¸ºqueryç”Ÿæˆå‘é‡ï¼Œç»´åº¦: {len(config.query_embedding)}")

            query_vector = config.query_embedding

            # ä½¿ç”¨ESå‘é‡æœç´¢è·å–å€™é€‰events
            # æœç´¢content_vectorï¼Œè·å–äº‹ä»¶åŸºæœ¬ä¿¡æ¯
            candidate_events = await self.event_repo.search_similar_by_content(
                query_vector=query_vector,
                k=config.recall.max_events,  # å¤šå–ä¸€äº›ï¼Œç”¨äºåç»­è¿‡æ»¤
                source_config_ids=config.get_source_config_ids(),
            )

            if not candidate_events:
                self.logger.info("æ­¥éª¤2.5: æœªæ‰¾åˆ°å€™é€‰events")
                return [], {}

            self.logger.info(f"ğŸ“Š æ­¥éª¤2.5: æ‰¾åˆ° {len(candidate_events)} ä¸ªå€™é€‰events")

            # è®¡ç®—æ··åˆç›¸ä¼¼åº¦å¹¶è¿‡æ»¤
            filtered_event_ids = []
            weight_ratio = config.recall.query_event_weight_ratio

            for event in candidate_events:
                event_id = event.get("event_id")
                if not event_id:
                    continue

                # è·å–contentç›¸ä¼¼åº¦
                content_similarity = float(event.get("_score", 0.0))

                # ç›´æ¥ä½¿ç”¨ content_similarityï¼Œä¸å†è®¡ç®—æ··åˆç›¸ä¼¼åº¦ï¼ˆå‡å°‘ ES å‹åŠ›ï¼‰
                hybrid_similarity = content_similarity

                # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
                if hybrid_similarity >= config.recall.event_similarity_threshold:
                    filtered_event_ids.append({
                        "event_id": event_id,
                        "title": event.get("title", ""),
                        "hybrid_similarity": hybrid_similarity,
                        "content_similarity": content_similarity,
                        "title_similarity": content_similarity,  # ä¸å†ä½¿ç”¨ titleï¼Œä¸ content ä¿æŒä¸€è‡´
                    })
                    self.logger.debug(
                        f"Event {event_id} é€šè¿‡è¿‡æ»¤: "
                        f"hybrid={hybrid_similarity:.4f}, "
                        f"content={content_similarity:.4f}"
                    )

            # æŒ‰æ··åˆç›¸ä¼¼åº¦æ’åº
            filtered_event_ids.sort(key=lambda x: x["hybrid_similarity"], reverse=True)

            # æå–event_ids
            result_event_ids = [e["event_id"] for e in filtered_event_ids]

            self.logger.info(
                f"ğŸ“ˆ æ­¥éª¤2.5å®Œæˆ: "
                f"å€™é€‰{len(candidate_events)}ä¸ª â†’ "
                f"è¿‡æ»¤å{len(result_event_ids)}ä¸ªevents"
            )

            # æ˜¾ç¤ºtop events
            if filtered_event_ids:
                top_events = filtered_event_ids[:3]
                for idx, event in enumerate(top_events, 1):
                    self.logger.info(
                        f"  {idx}. Event {event['event_id']}: "
                        f"æ ‡é¢˜='{event['title'][:50]}...', "
                        f"æ··åˆç›¸ä¼¼åº¦={event['hybrid_similarity']:.4f}"
                    )

            # ğŸ†• è®°å½•çº¿ç´¢ï¼šquery â†’ event
            tracker = Tracker(config)
            query_node = Tracker.build_query_node(config)

            # æ‰¹é‡è·å–eventå¯¹è±¡
            event_ids = [e["event_id"] for e in filtered_event_ids]
            if event_ids:
                async with self.session_factory() as session:
                    from dataflow.db import SourceEvent
                    query = select(SourceEvent).where(SourceEvent.id.in_(event_ids))
                    result = await session.execute(query)
                    event_objs = {event.id: event for event in result.scalars().all()}

                    for event_info in filtered_event_ids:
                        event_id = event_info["event_id"]
                        event_obj = event_objs.get(event_id)

                        if event_obj:
                            metadata = {
                                "method": "hybrid_vector_search",
                                "step": "step2_5",
                                "hybrid_similarity": event_info["hybrid_similarity"],
                                "content_similarity": event_info["content_similarity"],
                                "title_similarity": event_info["title_similarity"],
                                "weight_ratio": config.recall.query_event_weight_ratio
                            }

                            # tracker.add_clue(
                            #     stage="recall",
                            #     from_node=query_node,
                            #     to_node=tracker.get_or_create_event_node(event_obj, "recall"),
                            #     confidence=event_info["hybrid_similarity"],
                            #     relation="è¯­ä¹‰ç›¸ä¼¼",
                            #     display_level="intermediate",
                            #     metadata=metadata
                            # )

                self.logger.debug(f"âœ… æ­¥éª¤2.5è®°å½• {len(filtered_event_ids)} æ¡ query â†’ event çº¿ç´¢")

            # æ„å»ºç›¸ä¼¼åº¦æ˜ å°„
            query_event_similarities = {e["event_id"]: e["hybrid_similarity"] for e in filtered_event_ids}
            return result_event_ids, query_event_similarities

        except Exception as e:
            self.logger.error(f"æ­¥éª¤2.5æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            import traceback
            self.logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return [], {}


    @staticmethod
    def _cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        import numpy as np
        return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))


    async def _step3_filter_events(
        self,
        event_key_query_related: List[str],
        key_query_related: List[Dict[str, Any]],
        k1_weights: Dict[str, float],
        config: SearchConfig,
        query_event_similarities: Optional[Dict[str, float]] = None,
    ) -> Tuple[List[str], List[Dict[str, Any]], Dict[str, float]]:
        """
        æ­¥éª¤3: è¿‡æ»¤Eventï¼ˆæƒé‡æ’åºï¼‰

        1. ä½¿ç”¨ key å¬å› + query å¬å›çš„ events
        2. è®¡ç®—æ¯ä¸ªeventçš„ç»¼åˆæƒé‡ï¼š
           event_weight = balance * e1_weight + (1-balance) * key_weight_sum
        3. æŒ‰æƒé‡æ’åºç­›é€‰eventsï¼ˆä½¿ç”¨max_eventsé™åˆ¶ï¼‰
        4. ä¿ç•™æ‰€æœ‰keyï¼ˆä¸åœ¨æ­¥éª¤3è¿‡æ»¤keyï¼Œå› ä¸ºæ­¥éª¤1å·²é™åˆ¶æ•°é‡ï¼‰
        """
        # 1. æå–event IDsï¼ˆåŒ…å« key å¬å› + query å¬å›ï¼‰
        all_event_ids_set = set(event_key_query_related)
        all_event_ids = list(all_event_ids_set)

        self.logger.info(
            f"ğŸ“Š [Step3] Eventsæ•°é‡: æ€»è®¡={len(all_event_ids)}"
        )

        # ä»ESæ‰¹é‡è·å–äº‹ä»¶ä¿¡æ¯å’Œå‘é‡
        events_from_es = await self.event_repo.get_events_by_ids(all_event_ids)

        # æ„å»ºäº‹ä»¶IDåˆ°å‘é‡å’Œentity_idsçš„æ˜ å°„
        event_vectors = {}
        event_title_vectors = {}  # ğŸ†• å­˜å‚¨titleå‘é‡
        event_entities = {}  # event_id -> list of entity_ids
        for event in events_from_es:
            event_id = event.get("event_id")
            content_vector = event.get("content_vector")
            title_vector = event.get("title_vector")  # ğŸ†• è·å–titleå‘é‡
            entity_ids = event.get("entity_ids", [])
            if event_id and content_vector:
                event_vectors[event_id] = content_vector
                event_entities[event_id] = entity_ids
            if event_id and title_vector:
                event_title_vectors[event_id] = title_vector

        self.logger.info(
            f"ä»ESè·å– {len(all_event_ids)} ä¸ªäº‹ä»¶ï¼Œ"
            f"æˆåŠŸè·å–contentå‘é‡ {len(event_vectors)} ä¸ªï¼Œ"
            f"æˆåŠŸè·å–titleå‘é‡ {len(event_title_vectors)} ä¸ªï¼Œ"
            f"æˆåŠŸè·å–entity_ids {len(event_entities)} ä¸ª"
        )

        # ç¡®ä¿ query_embedding å­˜åœ¨
        if not config.has_query_embedding or not config.query_embedding:
            config.query_embedding = await self.processor.generate_embedding(config.query)
            config.has_query_embedding = True

        # è®¡ç®— e1_weightsï¼ˆäº‹ä»¶ä¸æŸ¥è¯¢çš„ç›¸ä¼¼åº¦ï¼Œç»Ÿä¸€ä½¿ç”¨hybrid_similarityï¼‰
        e1_weights = {}
        query_event_sims = query_event_similarities or {}
        weight_ratio = config.recall.query_event_weight_ratio  # contentæƒé‡

        for event_id in all_event_ids:
            # ä¼˜å…ˆä½¿ç”¨æ­¥éª¤2.5ä¼ é€’çš„ç›¸ä¼¼åº¦ï¼ˆqueryå¬å›çš„eventsï¼‰
            if event_id in query_event_sims:
                e1_weights[event_id] = query_event_sims[event_id]
            else:
                # å¯¹äºkeyå¬å›çš„eventsï¼Œä¹Ÿè®¡ç®—hybrid_similarity
                content_vector = event_vectors.get(event_id)
                title_vector = event_title_vectors.get(event_id)

                if content_vector:
                    content_similarity = self._cosine_similarity(
                        config.query_embedding,
                        content_vector
                    )

                    if title_vector:
                        title_similarity = self._cosine_similarity(
                            config.query_embedding,
                            title_vector
                        )
                    else:
                        # æ²¡æœ‰titleå‘é‡æ—¶ä½¿ç”¨contentç›¸ä¼¼åº¦ä½œä¸ºfallback
                        title_similarity = content_similarity

                    # è®¡ç®—hybrid_similarity: ratio * content + (1 - ratio) * title
                    hybrid_similarity = (
                        weight_ratio * content_similarity +
                        (1 - weight_ratio) * title_similarity
                    )

                    # åº”ç”¨é˜ˆå€¼è¿‡æ»¤
                    if hybrid_similarity >= config.recall.event_similarity_threshold:
                        e1_weights[event_id] = hybrid_similarity

        # 3. è®¡ç®—æ¯ä¸ªeventçš„ç»¼åˆæƒé‡
        event_weights = {}
        balance = config.recall.step4_event_key_balance

        for event_id in all_event_ids:
            # query-eventç›¸ä¼¼åº¦æƒé‡
            e1_weight = e1_weights.get(event_id, 0.0)

            # ä»ESç»“æœè·å–è¯¥eventåŒ…å«çš„æ‰€æœ‰key
            event_keys = event_entities.get(event_id, [])

            # è®¡ç®—è¯¥eventåŒ…å«çš„æ‰€æœ‰keyçš„k1_weightsä¹‹å’Œ
            key_weight_sum = sum(
                k1_weights.get(key_id, 0.0) for key_id in event_keys
            )

            # ç»¼åˆæƒé‡ = balance * e1_weight + (1-balance) * key_weight_sum
            combined_weight = balance * e1_weight + (1 - balance) * key_weight_sum
            event_weights[event_id] = combined_weight

        # 4. æŒ‰æƒé‡æ’åºç­›é€‰eventsï¼ˆä½¿ç”¨max_eventsé™åˆ¶ï¼‰
        sorted_events = sorted(
            event_weights.items(), key=lambda x: x[1], reverse=True
        )

        # å¤ç”¨max_eventsé™åˆ¶æ•°é‡
        event_related = [
            eid for eid, _ in sorted_events[: config.recall.max_events]
        ]

        self.logger.info(
            f"ğŸ“Š [Step3] Eventsæƒé‡ç­›é€‰: key_events={len(all_event_ids)} â†’ "
            f"top-{config.recall.max_events}={len(event_related)}"
        )

        # æ˜¾ç¤ºTop 5 eventsçš„æƒé‡
        if len(event_related) > 0:
            top5_events = sorted_events[: min(5, len(sorted_events))]
            self.logger.debug(
                f"ğŸ† Top {len(top5_events)} Events: {top5_events}"
            )

        # 5. ä¿ç•™æ‰€æœ‰keyï¼ˆä¸åœ¨æ­¥éª¤3è¿‡æ»¤keyï¼‰
        # ç›´æ¥è¿”å›å®Œæ•´çš„key_query_relatedï¼Œé¿å…ä¿¡æ¯ä¸¢å¤±
        self.logger.info(
            f"ğŸ“Š [Step3] Keysä¿ç•™: input={len(key_query_related)} â†’ "
            f"ä¿ç•™å…¨éƒ¨={len(key_query_related)} (æ­¥éª¤1å·²é™åˆ¶æ•°é‡)"
        )

        # ç¼“å­˜event_entitiesæ˜ å°„ï¼Œä¾›åç»­æ­¥éª¤ä½¿ç”¨
        config.event_entities_cache = event_entities

        return event_related, key_query_related, e1_weights

    async def _step4_calculate_event_key_weights(
        self,
        event_related: List[str],
        key_query_related: List[Dict[str, Any]],
        k1_weights: Dict[str, float],
        e1_weights: Dict[str, float],
        config: SearchConfig,
    ) -> Dict[str, float]:
        """
        æ­¥éª¤4: è®¡ç®—event-keyæƒé‡å‘é‡
        å…¬å¼: W = 0.5*s(e,Q) + ln(1 + Î£ k1_weight * ln(1+count))
        """
        if not event_related or not key_query_related:
            return {}

        import math

        # ä»å®Œæ•´ä¿¡æ¯ä¸­æå–entity_idåˆ—è¡¨
        key_related = [key_info["entity_id"] for key_info in key_query_related]

        # æ„å»º entity_id â†’ name æ˜ å°„ï¼ˆç”¨äºç»Ÿè®¡å‡ºç°æ¬¡æ•°ï¼‰
        entity_names = {
            key_info["entity_id"]: key_info["name"]
            for key_info in key_query_related
        }

        event_key_weights = {}

        try:
            # ä½¿ç”¨ç¼“å­˜çš„event_entitiesæ˜ å°„
            event_entities_cache = getattr(config, 'event_entities_cache', {})

            # ä»ESè·å–eventå†…å®¹ï¼ˆç”¨äºç»Ÿè®¡keyå‡ºç°æ¬¡æ•°ï¼‰
            events_data = await self.event_repo.get_events_by_ids(event_related)
            event_contents = {
                e.get("event_id"): f"{e.get('title', '')} {e.get('content', '')}"
                for e in events_data
            }

            for event_id in event_related:
                # 1. è·å– event ä¸ query çš„ç›¸ä¼¼åº¦ s(e_j, Q)
                e1_weight = e1_weights.get(event_id, 0.0)

                # 2. è·å–è¯¥ event çš„å†…å®¹
                full_text = event_contents.get(event_id, "")

                # 3. è·å–è¯¥ event åŒ…å«çš„ keysï¼ˆåªä¿ç•™step1å¬å›çš„ï¼‰
                event_keys = event_entities_cache.get(event_id, [])
                event_keys = [k for k in event_keys if k in key_related]

                # 4. è®¡ç®— Î£ W_{K,f}(k_i) * ln(1 + count(k_i, e_j))
                key_weight_sum = 0.0
                for key_id in event_keys:
                    k1_weight = k1_weights.get(key_id, 0.0)
                    key_name = entity_names.get(key_id, "")

                    # ç»Ÿè®¡ key åœ¨ event åŸæ–‡ä¸­å‡ºç°çš„æ¬¡æ•°ï¼ˆåŒºåˆ†å¤§å°å†™ï¼‰
                    count = full_text.count(key_name) if key_name else 0

                    # W_{K,f}(k_i) * ln(1 + count)
                    key_weight_sum += k1_weight * math.log(1 + count)

                # 5. åº”ç”¨å®Œæ•´å…¬å¼ï¼šW = 0.5 * s(e_j, Q) + ln(1 + key_weight_sum)
                total_weight = 0.5 * e1_weight + math.log(1 + key_weight_sum)
                event_key_weights[event_id] = total_weight

        except Exception as e:
            self.logger.error(f"æ­¥éª¤4è®¡ç®—event-keyæƒé‡å¤±è´¥: {e}", exc_info=True)
            raise

        return event_key_weights

    async def _step5_calculate_key_event_weights(
        self,
        event_related: List[str],
        key_query_related: List[Dict[str, Any]],
        event_key_weights: Dict[str, float],
        k1_weights: Dict[str, float],
        config: SearchConfig,
    ) -> Dict[str, float]:
        """
        æ­¥éª¤5: åå‘è®¡ç®—keyæƒé‡å‘é‡
        æ ¹æ®æ¯ä¸ªeventçš„æƒé‡åå‘è®¡ç®—keyçš„é‡è¦æ€§

        å…¬å¼: W_key-event(k_i) = avg(W_event-key(e_j)) Ã— core_boost
        å…¶ä¸­ e_j contains k_i
        """
        if not event_related:
            return {}

        key_event_weights = {}

        try:
            # ä½¿ç”¨ç¼“å­˜çš„event_entitiesæ˜ å°„
            event_entities_cache = getattr(config, 'event_entities_cache', {})

            # æ„å»ºåå‘ç´¢å¼•ï¼škey_id â†’ [event_ids]ï¼ˆä¸€æ¬¡éå†ï¼ŒåŒæ—¶æ”¶é›†æ‰€æœ‰keyï¼‰
            all_keys_in_events = set()
            key_to_events: Dict[str, List[str]] = {}
            for event_id in event_related:
                event_keys = event_entities_cache.get(event_id, [])
                all_keys_in_events.update(event_keys)
                for key_id in event_keys:
                    if key_id not in key_to_events:
                        key_to_events[key_id] = []
                    key_to_events[key_id].append(event_id)

            self.logger.info(f"ğŸ“Š [Step5] ä» {len(event_related)} ä¸ª event ä¸­æå–åˆ° {len(all_keys_in_events)} ä¸ª key")

            # æ”¶é›†æ‰€æœ‰eventæ¿€æ´»çš„æ–°keyï¼ˆä¸åœ¨k1_weightsä¸­çš„ï¼‰
            new_key_ids = [kid for kid in all_keys_in_events if kid not in k1_weights]
            self.logger.info(f"ğŸ“Š [Step5] å…¶ä¸­ {len(new_key_ids)} ä¸ªæ˜¯eventæ¿€æ´»çš„æ–°key")

            # ğŸ†• è·å–åŸå§‹æŸ¥è¯¢ï¼ˆç”¨äºæ ¸å¿ƒå®ä½“æ£€æµ‹ï¼‰
            original_query_lower = config.original_query.lower() if config.original_query else ""
            
            # ğŸ†• é¢„åŠ è½½å®ä½“åç§°ï¼ˆç”¨äºæ ¸å¿ƒå®ä½“æ£€æµ‹ï¼‰- æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–
            entity_names_map = {}
            if all_keys_in_events:
                async with self.session_factory() as session:
                    result = await session.execute(
                        select(Entity.id, Entity.name).where(Entity.id.in_(list(all_keys_in_events)))
                    )
                    for row in result:
                        entity_names_map[row[0]] = row[1]
            
            # ä¸ºæ¯ä¸ªkeyè®¡ç®—æƒé‡ï¼ˆä½¿ç”¨åå‘ç´¢å¼•ï¼ŒO(1)æŸ¥è¡¨ï¼‰
            for key_id in all_keys_in_events:
                # ç›´æ¥ä»åå‘ç´¢å¼•è·å–åŒ…å«è¯¥keyçš„events
                key_events = key_to_events.get(key_id, [])

                # ğŸ†• 1. Event è´¡çŒ®ï¼šä½¿ç”¨å¹³å‡å€¼ï¼ˆå½’ä¸€åŒ–ï¼‰ï¼Œæ¶ˆé™¤ event æ•°é‡åå·®
                event_weights_list = [event_key_weights.get(eid, 0.0) for eid in key_events]
                if event_weights_list:
                    avg_event_weight = sum(event_weights_list) / len(event_weights_list)
                else:
                    avg_event_weight = 0.0

                # ğŸ†• 2. æ ¸å¿ƒå®ä½“æ£€æµ‹ï¼šå®ä½“åç§°æ˜¯å¦å‡ºç°åœ¨åŸå§‹é—®é¢˜ä¸­
                entity_name = entity_names_map.get(key_id, "")
                is_core_entity = entity_name and entity_name.lower() in original_query_lower
                core_boost = 1.5 if is_core_entity else 1.0

                # ğŸ†• 3. å®Œæ•´å…¬å¼ï¼ševentè´¡çŒ® + æ ¸å¿ƒå®ä½“åŠ æˆ
                # è®¾è®¡ç†å¿µï¼š
                # - event è´¡çŒ®ä½¿ç”¨å¹³å‡å€¼å½’ä¸€åŒ–ï¼Œé¿å…é«˜é¢‘å®ä½“å ä¼˜
                # - æ ¸å¿ƒå®ä½“ï¼ˆé—®é¢˜ä¸­ç›´æ¥æåˆ°çš„ï¼‰è·å¾— 1.5 å€åŠ æˆä¿æŠ¤
                key_event_weights[key_id] = avg_event_weight * core_boost

        except Exception as e:
            self.logger.error(f"æ­¥éª¤5è®¡ç®—key-eventæƒé‡å¤±è´¥: {e}", exc_info=True)
            raise

        return key_event_weights

    async def _step6_extract_important_keys(
        self,
        key_event_weights: Dict[str, float],
        config: SearchConfig,
        key_query_related: List[Dict[str, Any]] = None,  # ğŸ†• æ–°å¢å‚æ•°ï¼Œç”¨äºè·å– query_source
    ) -> List[Dict[str, Any]]:
        """
        æ­¥éª¤6: æå–é‡è¦çš„key
        è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼æˆ–æå–top-né‡è¦çš„key

        Args:
            key_event_weights: keyæƒé‡å­—å…¸
            config: æœç´¢é…ç½®
            key_query_related: æ­¥éª¤1å¬å›çš„keyåˆ—è¡¨ï¼ˆåŒ…å« query_source ä¿¡æ¯ï¼‰
        """
        # è·å–keyçš„è¯¦ç»†ä¿¡æ¯
        key_final = []

        if not key_event_weights:
            return key_final

        # ğŸ†• æ„å»º key_id â†’ query_source çš„æ˜ å°„
        key_source_map = {}
        if key_query_related:
            for key in key_query_related:
                key_id = key.get("entity_id")
                query_source = key.get("query_source", "origin")  # é»˜è®¤ origin
                if key_id:
                    key_source_map[key_id] = query_source
        # é’ˆå¯¹åˆ†è¯å¬å›çš„keyåšåŠ¨æ€æƒé‡æå‡ï¼Œé¿å…å…¶å› åˆ†æ•°åä½è¢«æˆªæ–­
        tokenizer_ids = getattr(config, "tokenizer_entity_ids", set())
        tokenizer_gap = getattr(config.recall, "tokenizer_priority_gap", 0.0)
        if tokenizer_gap > 0 and tokenizer_ids:
            token_weights = [
                weight for key_id, weight in key_event_weights.items()
                if key_id in tokenizer_ids
            ]
            non_token_weights = [
                weight for key_id, weight in key_event_weights.items()
                if key_id not in tokenizer_ids
            ]
            if token_weights and non_token_weights:
                min_token = min(token_weights)
                max_non_token = max(non_token_weights)
                desired_min = max_non_token + tokenizer_gap
                if min_token < desired_min:
                    bias = desired_min - min_token
                    boosted = 0
                    for key_id in tokenizer_ids:
                        if key_id in key_event_weights:
                            key_event_weights[key_id] += bias
                            boosted += 1
                    if boosted > 0:
                        self.logger.info(
                            f"ğŸ†™ åˆ†è¯keyåŠ¨æ€åŠ æƒ: +{bias:.3f}, è¦†ç›–{boosted}ä¸ªå®ä½“ï¼Œç¡®ä¿å…¶æƒé‡é¢†å…ˆæ™®é€škey {tokenizer_gap:.3f}"
                        )

        # æŒ‰æƒé‡æ’åº
        sorted_keys = sorted(key_event_weights.items(),
                             key=lambda x: x[1], reverse=True)

        # åº”ç”¨é˜ˆå€¼æˆ–top-nç­›é€‰
        # Always apply threshold filter first
        filtered_keys = [
            (key_id, weight) for key_id, weight in sorted_keys
            if weight >= config.recall.entity_weight_threshold
        ]

        # Then apply top-N if configured
        if config.recall.final_entity_count:
            selected_keys = filtered_keys[: config.recall.final_entity_count]
        else:
            selected_keys = filtered_keys

        # è·å–keyçš„è¯¦ç»†ä¿¡æ¯
        if selected_keys:
            key_ids = [key_id for key_id, _ in selected_keys]

            try:
                async with self.session_factory() as session:
                    query = select(Entity).where(Entity.id.in_(key_ids))
                    result = await session.execute(query)
                    entities = {
                        entity.id: entity for entity in result.scalars().all()}

                for key_id, weight in selected_keys:
                    entity = entities.get(key_id)
                    if entity:
                        # ğŸ†• ä» key_source_map è·å– query_source
                        query_source = key_source_map.get(key_id, "origin")  # é»˜è®¤ origin

                        key_final.append({
                            "key_id": key_id,
                            "name": entity.name,
                            "type": entity.type,
                            "weight": weight,
                            "steps": [1],  # ç¬¬ä¸€é˜¶æ®µï¼Œæ‰€æœ‰å€¼éƒ½ä¸º1
                            "query_source": query_source,  # ğŸ†• æ·»åŠ  query_source
                        })
            except Exception as e:
                self.logger.error(f"æ­¥éª¤6æå–é‡è¦keyså¤±è´¥: {e}", exc_info=True)
                raise

        # ç­›é€‰å‡ºæœ€ç»ˆè¢«ä½¿ç”¨çš„queryå¬å›çš„key
        if key_final and config.query_recalled_keys:
            # æ„å»ºkey_finalçš„key_idåˆ°keyå¯¹è±¡çš„æ˜ å°„
            key_final_map = {key["key_id"]: key for key in key_final}

            # è®°å½•åŸå§‹æ•°é‡
            original_count = len(config.query_recalled_keys)

            # ç­›é€‰å‡ºåœ¨key_finalä¸­çš„queryå¬å›çš„keyï¼Œå¹¶ä½¿ç”¨key_finalä¸­çš„keyå¯¹è±¡
            used_query_keys = []
            for query_key in config.query_recalled_keys:
                entity_id = query_key["entity_id"]
                if entity_id in key_final_map:
                    # ä½¿ç”¨key_finalä¸­çš„keyå¯¹è±¡ï¼ˆåŒ…å«weightå’Œstepsç­‰ä¿¡æ¯ï¼‰
                    used_query_keys.append(key_final_map[entity_id])

            # æ›´æ–°config.query_recalled_keysï¼Œåªä¿ç•™æœ€ç»ˆè¢«ä½¿ç”¨çš„keyï¼ˆæ¥è‡ªkey_finalï¼‰
            config.query_recalled_keys = used_query_keys

            self.logger.info(
                f"æ­¥éª¤6: queryå¬å›çš„keyä¸­æ€»å…±{original_count}ä¸ª "
                f"æœ‰{len(used_query_keys)}ä¸ªè¢«ä¿ç•™åœ¨key_finalä¸­ï¼ˆä½¿ç”¨key_finalä¸­çš„keyå¯¹è±¡ï¼‰"
            )

            if used_query_keys:
                # æ˜¾ç¤ºè¢«ä¿ç•™çš„queryå¬å›çš„key
                used_key_names = [key["name"] for key in used_query_keys[:5]]
                self.logger.debug(
                    f"è¢«ä¿ç•™çš„queryå¬å›keyï¼ˆå‰5ä¸ªï¼‰: {', '.join(used_key_names)}")

        return key_final



    async def _build_recall_clues(
        self,
        config: SearchConfig,
        key_query_related: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        æ„å»ºRecallé˜¶æ®µçš„çº¿ç´¢ï¼ˆquery â†’ entityï¼‰

        ä½¿ç”¨ç»Ÿä¸€çš„Trackeræ„å»ºï¼Œç¡®ä¿æ•°æ®ç»“æ„ä¸€è‡´æ€§

        Args:
            config: æœç´¢é…ç½®
            key_query_related: queryå¬å›çš„å®ä½“åˆ—è¡¨

        Returns:
            Recallé˜¶æ®µçš„çº¿ç´¢åˆ—è¡¨
        """
        from dataflow.modules.search.tracker import Tracker

        clues = []

        # query â†’ entityçº¿ç´¢
        for entity in key_query_related:
            # ç»Ÿä¸€ä½¿ç”¨similarityä½œä¸ºconfidence
            confidence = entity.get("similarity", 0.0)

            # è·å–å®ä½“æƒé‡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            entity_weight = entity.get("weight")
            metadata = {
                "similarity": entity.get("similarity", 0.0),
                "method": entity.get("method", "vector_search"),
                "source_attribute": entity.get("source_attribute")  # ğŸ†• æ·»åŠ æ¥æºå±æ€§
            }
            # åªæœ‰toèŠ‚ç‚¹æ˜¯å®ä½“æ—¶æ‰å­˜å‚¨weight
            if entity_weight is not None:
                metadata["weight"] = entity_weight

            # ä½¿ç”¨ç»Ÿä¸€æ„å»ºå™¨åˆ›å»ºçº¿ç´¢
            clue = Tracker.build_recall_clue(
                config=config,
                entity=entity,
                confidence=confidence,
                metadata=metadata
            )
            clues.append(clue)

            # å°†toèŠ‚ç‚¹ï¼ˆentityèŠ‚ç‚¹ï¼‰å­˜å…¥ç¼“å­˜ï¼Œä¾›expandé˜¶æ®µä½¿ç”¨
            to_node = clue.get("to")
            if to_node and to_node.get("id"):
                config.entity_node_cache[to_node["id"]] = to_node

        return clues

    # === æ™®é€šæ¨¡å¼æ–°å¢æ–¹æ³• ===

    async def _get_entity_types_for_source(
        self, source_config_ids: List[str]
    ) -> List[Dict[str, Any]]:
        """
        è·å–ä¿¡æ¯æºçš„æ‰€æœ‰å®ä½“ç±»å‹

        Args:
            source_config_ids: ä¿¡æ¯æºIDåˆ—è¡¨

        Returns:
            å®ä½“ç±»å‹åˆ—è¡¨: [{"type": "person", "name": "äººç‰©", "description": "..."}, ...]
        """
        from dataflow.db import EntityType

        entity_types = []

        async with self.session_factory() as session:
            # æŸ¥è¯¢æŒ‡å®šä¿¡æ¯æºçš„å®ä½“ç±»å‹ + å…¨å±€ç±»å‹
            query = (
                select(EntityType)
                .where(
                    (EntityType.source_config_id.in_(source_config_ids)) |
                    (EntityType.source_config_id.is_(None))  # å…¨å±€ç±»å‹
                )
                .where(EntityType.is_active == True)
            )

            result = await session.execute(query)
            types = result.scalars().all()

            for entity_type in types:
                entity_types.append({
                    "type": entity_type.type,
                    "name": entity_type.name,
                    "description": entity_type.description or ""
                })

        self.logger.info(f"è·å–åˆ° {len(entity_types)} ä¸ªå®ä½“ç±»å‹")
        return entity_types

    def _build_rewrite_and_extract_schema(self) -> Dict[str, Any]:
        """
        æ„å»ºæŸ¥è¯¢é‡å†™+èšç„¦ç±»å‹è¯†åˆ«+ç›®æ ‡ç±»å‹è¯†åˆ«+å®ä½“è¯†åˆ«çš„JSON Schema
        """
        return {
            "type": "object",
            "properties": {
                "rewritten_query": {
                    "type": "string",
                    "description": "Rewritten query text"
                },
                "focus_entity_types": {
                    "type": "array",
                    "description": "2-6 entity types to START the search from (clues leading to answer)",
                    "items": {
                        "type": "string",
                        "description": "Entity type identifier (e.g. person, location, work)"
                    }
                },
                "target_entity_types": {
                    "type": "array",
                    "description": "1-3 entity types where the ANSWER is likely found. For 'Who directed X?' target is 'person'. For 'What city is X in?' target includes 'person' (who knows the city) and 'location'",
                    "items": {
                        "type": "string",
                        "description": "Entity type identifier"
                    }
                },
                "entities": {
                    "type": "array",
                    "description": "Extracted entity names (only of focus types)",
                    "items": {
                        "type": "string",
                        "description": "Entity name"
                    }
                }
            },
            "required": ["rewritten_query", "focus_entity_types", "target_entity_types", "entities"]
        }

    async def _llm_rewrite_and_extract_entities(
        self,
        query: str,
        candidate_entities: List[Dict[str, Any]],
        entity_types: List[Dict[str, Any]],
        config: SearchConfig,
        background_entities: Optional[List[Dict[str, Any]]] = None  # ğŸ†• èƒŒæ™¯å®ä½“
    ) -> Tuple[str, List[str], List[str]]:
        """
        åˆå¹¶çš„LLMè°ƒç”¨ï¼šæŸ¥è¯¢é‡å†™ + èšç„¦ç±»å‹è¯†åˆ« + å®ä½“è¯†åˆ«

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            candidate_entities: å‘é‡å¬å›çš„å€™é€‰å®ä½“åˆ—è¡¨ï¼ˆä½œä¸º few-shotsï¼‰
            entity_types: å¯ç”¨çš„å®ä½“ç±»å‹åˆ—è¡¨
            config: æœç´¢é…ç½®
            background_entities: èƒŒæ™¯å®ä½“åˆ—è¡¨ï¼ˆæ¥è‡ªé«˜è´¨é‡äº‹é¡¹ï¼ŒæŒ‰çƒ­åº¦æ’åºï¼‰

        Returns:
            Tuple[rewritten_query, entity_names, focus_types]:
                - rewritten_query: é‡å†™åçš„æŸ¥è¯¢
                - entity_names: è¯†åˆ«å‡ºçš„æ‰€æœ‰ç›¸å…³å®ä½“åç§°åˆ—è¡¨
                - focus_types: LLMè¯†åˆ«çš„èšç„¦å®ä½“ç±»å‹åˆ—è¡¨
        """
        from datetime import datetime

        # è·å–å½“å‰æ—¶é—´
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # æ„å»ºå€™é€‰å®ä½“çš„æ ¼å¼åŒ–æ–‡æœ¬ï¼ˆç®€åŒ–æ ¼å¼ï¼‰
        if candidate_entities:
            candidates_text = "\n".join([
                f"{i}. [{e['type']}] {e['name']}"
                for i, e in enumerate(candidate_entities)
            ])
        else:
            candidates_text = "(No candidates)"

        # ğŸ†• æ„å»ºèƒŒæ™¯å®ä½“çš„æ ¼å¼åŒ–æ–‡æœ¬ï¼ˆæ¥è‡ªé«˜è´¨é‡äº‹é¡¹ï¼ŒæŒ‰çƒ­åº¦æ’åºï¼‰
        if background_entities:
            background_text = "\n".join([
                f"{i+1}. [{e['type']}] {e['name']} (çƒ­åº¦={e.get('event_count', 0)})"
                for i, e in enumerate(background_entities)
            ])
            self.logger.info(f"ğŸ“‹ ä¼ ç»™LLMçš„èƒŒæ™¯å®ä½“: {len(background_entities)} ä¸ª")
        else:
            background_text = "(No background entities)"

        # æ„å»ºå®ä½“ç±»å‹çš„æ ¼å¼åŒ–æ–‡æœ¬
        entity_types_text = "\n".join([
            f"- {t['type']}: {t['name']}" + (f" - {t['description']}" if t.get('description') else "")
            for t in entity_types
        ])

        try:
            # æ¸²æŸ“æç¤ºè¯æ¨¡æ¿
            prompt = self.prompt_manager.render(
                "rewrite_and_extract_entities",
                query=query,
                current_time=current_time,
                candidates=candidates_text,
                background_entities=background_text,  # ğŸ†• ä¼ å…¥èƒŒæ™¯å®ä½“
                entity_types=entity_types_text,
                max_entities=config.recall.max_entities
            )

            # è°ƒç”¨LLM
            messages = [
                LLMMessage(role=LLMRole.USER, content=prompt)
            ]

            schema = self._build_rewrite_and_extract_schema()

            response = await self.llm_client.chat_with_schema(
                messages,
                response_schema=schema,
                temperature=0.1  # ä½ temperature ä¿æŒç¨³å®šæ€§ï¼Œå…è®¸å°‘é‡çµæ´»æ€§
            )

            # è§£æé‡å†™åçš„æŸ¥è¯¢
            rewritten_query = response.get("rewritten_query", "").strip()
            if not rewritten_query:
                rewritten_query = query  # å¦‚æœä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢

            # ğŸ†• è§£æèšç„¦å®ä½“ç±»å‹ï¼ˆçº¿ç´¢ç»´åº¦ â†’ ç”¨äºRecall/Expandè¿‡æ»¤ï¼‰
            focus_types = response.get("focus_entity_types", [])
            if focus_types:
                # ä¿å­˜åˆ°configä¾›åç»­é˜¶æ®µä½¿ç”¨
                config.focus_entity_types = focus_types
                self.logger.info(f"ğŸ¯ LLMè¯†åˆ«çš„çº¿ç´¢ç»´åº¦(focus): {focus_types}")

            # ğŸ†• è§£æç›®æ ‡å®ä½“ç±»å‹ï¼ˆç›®æ ‡ç»´åº¦ â†’ ç”¨äºRerankåŠ æƒï¼‰
            target_types = response.get("target_entity_types", [])
            if target_types:
                # ä¿å­˜åˆ°configä¾›Reranké˜¶æ®µä½¿ç”¨
                config.target_entity_types = target_types
                self.logger.info(f"ğŸ¯ LLMè¯†åˆ«çš„ç›®æ ‡ç»´åº¦(target): {target_types}")

            # è·å–å¹¶å»é‡å®ä½“åˆ—è¡¨
            entity_names = response.get("entities", [])

            # å»é‡ï¼šä¿æŒåŸå§‹é¡ºåºï¼Œå»é™¤é‡å¤é¡¹
            seen = set()
            unique_entity_names = []
            for entity_name in entity_names:
                if entity_name and entity_name not in seen:
                    seen.add(entity_name)
                    unique_entity_names.append(entity_name)

            # è®°å½•å»é‡æ•ˆæœ
            if len(entity_names) != len(unique_entity_names):
                self.logger.info(f"ğŸ”„ å®ä½“å»é‡: åŸå§‹{len(entity_names)}ä¸ª â†’ å»é‡å{len(unique_entity_names)}ä¸ª")

            # é™åˆ¶å®ä½“æ•°é‡ï¼ˆä½¿ç”¨ max_entities é™åˆ¶ï¼‰
            entity_names = unique_entity_names[:config.recall.max_entities]

            self.logger.info(
                f"ğŸ”„ LLMåˆå¹¶è°ƒç”¨å®Œæˆ: "
                f"é‡å†™='{query}' â†’ '{rewritten_query}', "
                f"è¯†åˆ«å‡º {len(entity_names)} ä¸ªå®ä½“"
            )

            # æ‰“å°è¯†åˆ«å‡ºçš„å®ä½“
            if entity_names:
                self.logger.info(f"ğŸ“‹ LLMè¯†åˆ«çš„å®ä½“: {entity_names}")

            return rewritten_query, entity_names, focus_types

        except Exception as e:
            self.logger.error(f"LLMåˆå¹¶è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
            # å¤±è´¥æ—¶è¿”å›åŸå§‹æŸ¥è¯¢å’Œç©ºåˆ—è¡¨
            return query, [], []

    async def _es_exact_search_entities(
        self,
        expanded_entities: List[str],
        source_config_ids: List[str],
        limit_per_name: int,
    ) -> List[Dict[str, Any]]:
        """
        ESç²¾ç¡®æœç´¢å®ä½“ï¼ˆä½¿ç”¨å…¨ç­‰åŒ¹é…ï¼‰

        Args:
            expanded_entities: LLMæ‰©å±•ç”Ÿæˆçš„å®ä½“åç§°åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²æ•°ç»„ï¼‰
            source_config_ids: ä¿¡æ¯æºIDåˆ—è¡¨
            limit_per_name: æ¯ä¸ªå®ä½“åçš„æœ€å¤§æœç´¢ç»“æœæ•°

        Returns:
            æœç´¢åˆ°çš„å®ä½“åˆ—è¡¨
        """
        if not expanded_entities:
            return []

        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        names = [name.strip() for name in expanded_entities if name.strip()]

        if not names:
            return []

        # è·å–å®ä½“ç±»å‹æƒé‡ä¿¡æ¯
        type_info = await self.entity_repo._get_entity_type_info(source_config_ids)

        # ä½¿ç”¨ ES æ‰¹é‡ç²¾ç¡®æœç´¢
        es_results = await self.entity_repo.search_by_names_exact(
            names=names,
            source_config_ids=source_config_ids,
            entity_types=None,  # ä¸ä½¿ç”¨ç±»å‹è¿‡æ»¤
            size_per_name=limit_per_name,
        )

        # å¤„ç†ç»“æœï¼Œå»é‡
        results = []
        seen_ids = set()

        for entity in es_results:
            entity_id = entity.get("entity_id")
            entity_name = entity.get("name", "")
            entity_type = entity.get("type", "")

            if entity_id in seen_ids:
                continue

            seen_ids.add(entity_id)

            # ä½¿ç”¨ç±»å‹æƒé‡ä½œä¸º similarity
            type_weight = type_info.get(entity_type, {}).get("weight", 1.0)

            results.append({
                "entity_id": entity_id,
                "name": entity_name,
                "type": entity_type,
                "similarity": type_weight,
                "type_weight": type_weight,
                "source_attribute": entity_name,  # æ¥æºäºå“ªä¸ªLLMç”Ÿæˆçš„åç§°
                "match_method": "es_exact"
            })

        self.logger.info(
            f"ESç²¾ç¡®æœç´¢å®Œæˆ: "
            f"è¾“å…¥={len(names)}ä¸ªåç§°, "
            f"æ‰¾åˆ°={len(results)}ä¸ªå®ä½“"
        )

        # æ‰“å°ESç²¾ç¡®æœç´¢æ‰¾åˆ°çš„å®ä½“
        if results:
            es_names = [f"[{e['type']}]{e['name']}" for e in results]
            self.logger.info(f"ğŸ“‹ ESç²¾ç¡®æœç´¢å®ä½“: {es_names}")

        return results

    async def _tokenizer_match_entities(
        self,
        query: str,
        source_config_ids: List[str],
        top_k: int = 15,
        exclude_types: Optional[List[str]] = None,
        focus_types: Optional[List[str]] = None,
        existing_entity_ids: Optional[set] = None
    ) -> Tuple[List[Dict[str, Any]], int]:
        """
        åˆ†è¯æå–å…³é”®è¯ â†’ æ•°æ®åº“å‰ç¼€åŒ¹é…å®ä½“

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            source_config_ids: æ•°æ®æºIDåˆ—è¡¨
            top_k: æœ€å¤šæå–çš„å…³é”®è¯æ•°é‡
            exclude_types: éœ€è¦æ’é™¤çš„å®ä½“ç±»å‹ï¼ˆé»‘åå•ï¼‰
            focus_types: èšç„¦çš„å®ä½“ç±»å‹ï¼ˆç™½åå•ï¼Œä¼˜å…ˆçº§é«˜äº exclude_typesï¼‰
            existing_entity_ids: å·²å­˜åœ¨çš„å®ä½“IDé›†åˆï¼ˆç”¨äºå»é‡ï¼‰

        Returns:
            (æ–°å¢çš„å®ä½“åˆ—è¡¨, æ–°å¢æ•°é‡)
        """
        from dataflow.core.ai.tokensize import extract_keywords

        # 1. åˆ†è¯æå–å…³é”®è¯
        keywords = extract_keywords(query, top_k=top_k, mode="tokenizer")
        if not keywords:
            self.logger.debug("åˆ†è¯å™¨æœªæå–åˆ°å…³é”®è¯")
            return [], 0

        self.logger.info(f"ğŸ”¤ åˆ†è¯æå–: {keywords}")

        # 2. æ•°æ®åº“åŒ¹é…ï¼ˆä»…ç²¾ç¡®åŒ¹é…ï¼Œç¦ç”¨å‰ç¼€åŒ¹é…é¿å…å™ªéŸ³ï¼‰
        # åˆ†è¯å™¨è¡¥å……å¬å›ä½¿ç”¨ç±»å‹æƒé‡ä½œä¸º similarity
        matched_entities = await self._mysql_exact_search_entities(
            expanded_entities=keywords,
            source_config_ids=source_config_ids,
            limit_per_name=2,  # æ¯ä¸ªå…³é”®è¯æœ€å¤šåŒ¹é…2ä¸ªå®ä½“
            exclude_types=exclude_types,
            focus_types=focus_types,
            use_prefix_match=False  # ğŸ†• ç¦ç”¨å‰ç¼€åŒ¹é…ï¼Œé¿å… "New" åŒ¹é…åˆ° "New York" ç­‰å™ªéŸ³
        )

        if not matched_entities:
            self.logger.debug("åˆ†è¯å…³é”®è¯æœªåŒ¹é…åˆ°æ•°æ®åº“å®ä½“")
            return [], 0

        # 3. å»é‡ï¼ˆæ’é™¤å·²å­˜åœ¨çš„å®ä½“ï¼‰
        existing_ids = existing_entity_ids or set()
        new_entities = []
        for entity in matched_entities:
            if entity["entity_id"] not in existing_ids:
                entity["match_method"] = "tokenizer"  # æ ‡è®°æ¥æº
                new_entities.append(entity)

        # ğŸ”§ é™åˆ¶åˆ†è¯è¡¥å……çš„æ€»æ•°é‡ï¼ˆé¿å…å•ä¸ªæ³›è¯å¦‚"è´¨é‡"è¡¥å……è¿‡å¤šå®ä½“ï¼‰
        # æŒ‰ type_weight æ’åºï¼Œä¼˜å…ˆä¿ç•™é«˜æƒé‡ç±»å‹
        max_tokenizer_entities = 20  # æœ€å¤šè¡¥å…… 20 ä¸ªå®ä½“
        if len(new_entities) > max_tokenizer_entities:
            new_entities.sort(key=lambda x: x.get("type_weight", 1.0), reverse=True)
            truncated_count = len(new_entities) - max_tokenizer_entities
            new_entities = new_entities[:max_tokenizer_entities]
            self.logger.info(f"âš ï¸ åˆ†è¯è¡¥å……æˆªæ–­: è£å‰ª {truncated_count} ä¸ªï¼ˆä¿ç•™Top {max_tokenizer_entities}ï¼‰")

        new_count = len(new_entities)
        if new_count > 0:
            self.logger.info(
                f"âœ… åˆ†è¯åŒ¹é…: æ‰¾åˆ° {len(matched_entities)} ä¸ª, "
                f"å»é‡åæ–°å¢ {new_count} ä¸ª"
            )

        return new_entities, new_count

    async def _mysql_exact_search_entities(
        self,
        expanded_entities: List[str],
        source_config_ids: List[str],
        limit_per_name: int,
        exclude_types: Optional[List[str]] = None,
        focus_types: Optional[List[str]] = None,
        use_prefix_match: bool = True  # ğŸ†• æ§åˆ¶æ˜¯å¦ä½¿ç”¨å‰ç¼€åŒ¹é…
    ) -> List[Dict[str, Any]]:
        """
        SQLæ¨¡ç³Šæœç´¢å®ä½“ï¼ˆä¼˜åŒ–ç‰ˆï¼šå‰ç¼€åŒ¹é… + æ‰¹é‡æŸ¥è¯¢ï¼‰

        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. ä½¿ç”¨ normalized_name å­—æ®µï¼ˆå·²æœ‰ç´¢å¼• idx_normalized_nameï¼‰
        2. ä½¿ç”¨å‰ç¼€åŒ¹é… LIKE 'name%'ï¼ˆå¯ä»¥ä½¿ç”¨ç´¢å¼•ï¼Œæ€§èƒ½æå‡æ˜æ˜¾ï¼‰
        3. æ‰¹é‡æŸ¥è¯¢å‡å°‘æ•°æ®åº“å¾€è¿”æ¬¡æ•°
        4. å…ˆç²¾ç¡®åŒ¹é…ï¼Œå†å‰ç¼€åŒ¹é…ï¼ˆfallbackï¼‰
        5. æ”¯æŒç±»å‹è¿‡æ»¤ï¼šä¼˜å…ˆç™½åå•(focus_types)ï¼Œå›é€€é»‘åå•(exclude_types)
        6. ä½¿ç”¨å®ä½“ç±»å‹æƒé‡ä½œä¸º similarity

        Args:
            expanded_entities: LLMæ‰©å±•ç”Ÿæˆçš„å®ä½“åç§°åˆ—è¡¨ï¼ˆå­—ç¬¦ä¸²æ•°ç»„ï¼‰
            source_config_ids: ä¿¡æ¯æºIDåˆ—è¡¨
            limit_per_name: æ¯ä¸ªå®ä½“åçš„æœ€å¤§æœç´¢ç»“æœæ•°
            exclude_types: éœ€è¦æ’é™¤çš„å®ä½“ç±»å‹åˆ—è¡¨ï¼ˆé»‘åå•ï¼‰
            focus_types: èšç„¦çš„å®ä½“ç±»å‹åˆ—è¡¨ï¼ˆç™½åå•ï¼Œä¼˜å…ˆçº§é«˜äº exclude_typesï¼‰
            use_prefix_match: æ˜¯å¦å¯ç”¨å‰ç¼€åŒ¹é…ï¼ˆé»˜è®¤Trueï¼Œåˆ†è¯å™¨è¡¥å……æ—¶åº”è®¾ä¸ºFalseé¿å…å™ªéŸ³ï¼‰

        Returns:
            æœç´¢åˆ°çš„å®ä½“åˆ—è¡¨
        """
        if not expanded_entities:
            return []

        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
        raw_names = [
            name.strip()
            for name in expanded_entities
            if isinstance(name, str) and name.strip()
        ]
        if not raw_names:
            return []

        import time
        sql_start = time.perf_counter()

        def normalize_name(name: str) -> str:
            """æ ‡å‡†åŒ–åç§°ï¼šè½¬å°å†™ã€å»ç©ºæ ¼"""
            return name.lower().strip().replace(" ", "").replace("ã€€", "")

        # normalized_name -> {name, type}
        name_to_entity: Dict[str, Dict[str, Any]] = {}
        normalized_names: List[str] = []
        for raw_name in raw_names:
            normalized = normalize_name(raw_name)
            normalized_names.append(normalized)
            name_to_entity[normalized] = {
                "name": raw_name,
                "type": "",
            }

        if not normalized_names:
            return []

        results: List[Dict[str, Any]] = []
        seen_ids: set = set()
        
        # è·å–ç±»å‹æƒé‡ä¿¡æ¯
        type_info = await self.entity_repo._get_entity_type_info(source_config_ids)

        async with self.session_factory() as session:
            # ç­–ç•¥ 1ï¼šæ‰¹é‡ç²¾ç¡®åŒ¹é… normalized_name
            exact_query = (
                select(Entity)
                .where(Entity.source_config_id.in_(source_config_ids))
                .where(Entity.normalized_name.in_(normalized_names))
            )
            # ğŸ†• ç±»å‹è¿‡æ»¤ï¼šä¼˜å…ˆç™½åå•ï¼Œå›é€€é»‘åå•
            if focus_types:
                exact_query = exact_query.where(Entity.type.in_(focus_types))
            elif exclude_types:
                exact_query = exact_query.where(~Entity.type.in_(exclude_types))
            exact_result = await session.execute(exact_query)
            exact_entities = exact_result.scalars().all()

            exact_count = 0
            for entity in exact_entities:
                if entity.id in seen_ids:
                    continue
                seen_ids.add(entity.id)
                exact_count += 1

                mapped = name_to_entity.get(entity.normalized_name, {})
                source_name = mapped.get("name", entity.name)

                # è·å–ç±»å‹æƒé‡
                entity_type_weight = type_info.get(entity.type, {}).get("weight", 1.0)
                results.append({
                    "entity_id": entity.id,
                    "name": entity.name,
                    "type": entity.type,
                    "similarity": entity_type_weight,  # ä½¿ç”¨ç±»å‹æƒé‡ä½œä¸º similarity
                    "type_weight": entity_type_weight,
                    "source_attribute": source_name,
                    "match_method": "sql_exact_match",
                })

            if exact_count > 0:
                self.logger.info(f"âœ… SQLç²¾ç¡®åŒ¹é…æ‰¾åˆ° {exact_count} ä¸ªå®ä½“")

            # ç­–ç•¥ 2ï¼šå‰ç¼€åŒ¹é…ï¼ˆæ’é™¤å·²ç²¾ç¡®åŒ¹é…çš„åç§°ï¼‰
            # ğŸ†• ä»…åœ¨ use_prefix_match=True æ—¶æ‰§è¡Œå‰ç¼€åŒ¹é…
            remaining_norms = {
                n for n in normalized_names
                if n not in {e.normalized_name for e in exact_entities}
            }

            prefix_count = 0

            if use_prefix_match and remaining_norms:
                from collections import defaultdict
                # å½“å‰ä¸æŒ‰ç±»å‹åˆ†ç»„ï¼Œå…¨éƒ¨æ”¾åœ¨ä¸€ä¸ªç»„ä¸­ï¼Œé¢„ç•™æ‰©å±•
                entities_by_type: Dict[str, List[str]] = defaultdict(list)
                for norm in remaining_norms:
                    entities_by_type[""].append(norm)

                for entity_type, norms in entities_by_type.items():
                    if not norms:
                        continue

                    batch_size = 20
                    for i in range(0, len(norms), batch_size):
                        batch_norms = norms[i:i + batch_size]

                        # æ„å»ºæ‰¹é‡å‰ç¼€åŒ¹é…æŸ¥è¯¢
                        conditions = [
                            Entity.normalized_name.like(f"{n}%")
                            for n in batch_norms
                        ]

                        query = (
                            select(Entity)
                            .where(Entity.source_config_id.in_(source_config_ids))
                            .where(or_(*conditions))
                        )
                        if entity_type:
                            query = query.where(Entity.type == entity_type)
                        # ç±»å‹è¿‡æ»¤ï¼šä¼˜å…ˆç™½åå•ï¼Œå›é€€é»‘åå•
                        if focus_types:
                            query = query.where(Entity.type.in_(focus_types))
                        elif exclude_types:
                            query = query.where(~Entity.type.in_(exclude_types))

                        query = query.limit(limit_per_name * len(batch_norms))

                        result = await session.execute(query)
                        entities = result.scalars().all()

                        for entity in entities:
                            if entity.id in seen_ids:
                                continue
                            seen_ids.add(entity.id)
                            prefix_count += 1

                            matched_name = None
                            for n in batch_norms:
                                if entity.normalized_name.startswith(n):
                                    mapped = name_to_entity.get(n, {})
                                    matched_name = mapped.get("name", entity.name)
                                    break

                            # è·å–ç±»å‹æƒé‡
                            entity_type_weight = type_info.get(entity.type, {}).get("weight", 1.0)
                            results.append({
                                "entity_id": entity.id,
                                "name": entity.name,
                                "type": entity.type,
                                "similarity": entity_type_weight,  # ä½¿ç”¨ç±»å‹æƒé‡ä½œä¸º similarity
                                "type_weight": entity_type_weight,
                                "source_attribute": matched_name or entity.name,
                                "match_method": "sql_prefix_match",
                            })

        sql_time = time.perf_counter() - sql_start
        self.logger.info(
            f"âœ… SQLæœç´¢å®Œæˆï¼ˆä¼˜åŒ–ç‰ˆï¼‰: "
            f"è¾“å…¥={len(raw_names)}ä¸ªåç§°, "
            f"æ‰¾åˆ°={len(results)}ä¸ªå®ä½“ "
            f"(ç²¾ç¡®åŒ¹é…={exact_count}, å‰ç¼€åŒ¹é…={prefix_count}), "
            f"è€—æ—¶={sql_time:.3f}ç§’"
        )

        if results:
            sample = [f"[{e['type']}]{e['name']}" for e in results[:10]]
            self.logger.info(f"ğŸ“‹ SQLæœç´¢å®ä½“ç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰: {sample}")

        return results

    async def _reverse_find_entities_by_events(
        self,
        event_ids: List[str],
        source_config_ids: List[str],
        min_name_length: int = 2,
        max_count: int = 20,
        focus_types: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        ä»äº‹é¡¹åå‘æŸ¥æ‰¾å…³è”çš„å®ä½“ï¼ˆæŒ‰ç±»å‹æƒé‡æ’åºï¼‰
        
        Args:
            event_ids: äº‹é¡¹IDåˆ—è¡¨
            source_config_ids: ä¿¡æ¯æºIDåˆ—è¡¨
            min_name_length: å®ä½“åç§°æœ€å°é•¿åº¦ï¼ˆè¿‡æ»¤æ³›åŒ–å®ä½“ï¼‰
            max_count: æœ€å¤§è¿”å›æ•°é‡
            focus_types: èšç„¦çš„å®ä½“ç±»å‹ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            å®ä½“åˆ—è¡¨ï¼ŒæŒ‰ç±»å‹æƒé‡é™åºæ’åº
        """
        if not event_ids:
            return []
        
        from collections import Counter
        from dataflow.db import EventEntity, Entity
        
        # è·å–å®ä½“ç±»å‹æƒé‡ä¿¡æ¯
        type_info = await self.entity_repo._get_entity_type_info(source_config_ids)
        
        async with self.session_factory() as session:
            # 1. æŸ¥æ‰¾è¿™äº›äº‹é¡¹å…³è”çš„å®ä½“ID
            query = (
                select(EventEntity.entity_id)
                .where(EventEntity.event_id.in_(event_ids))
            )
            result = await session.execute(query)
            entity_ids = [row[0] for row in result.fetchall()]
            
            if not entity_ids:
                return []
            
            # 2. ç»Ÿè®¡æ¯ä¸ªå®ä½“çš„çƒ­åº¦ï¼ˆå‡ºç°æ¬¡æ•°ï¼‰
            entity_counter = Counter(entity_ids)
            unique_entity_ids = list(entity_counter.keys())
            
            # 3. æŸ¥è¯¢å®ä½“è¯¦ç»†ä¿¡æ¯
            entity_query = (
                select(Entity)
                .where(
                    Entity.id.in_(unique_entity_ids),
                    Entity.source_config_id.in_(source_config_ids)
                )
            )
            entity_result = await session.execute(entity_query)
            entities = entity_result.scalars().all()
            
            # 4. æ„å»ºç»“æœåˆ—è¡¨
            background_entities = []
            for entity in entities:
                # è¿‡æ»¤ï¼šåç§°é•¿åº¦
                if len(entity.name) < min_name_length:
                    continue
                
                # è¿‡æ»¤ï¼šå®ä½“ç±»å‹
                if focus_types and entity.type not in focus_types:
                    continue
                
                event_count = entity_counter.get(entity.id, 0)
                
                # è·å–ç±»å‹æƒé‡ï¼ˆé»˜è®¤1.0ï¼‰
                type_weight = type_info.get(entity.type, {}).get("weight", 1.0)
                
                background_entities.append({
                    "entity_id": entity.id,
                    "name": entity.name,
                    "type": entity.type,
                    "event_count": event_count,  # çƒ­åº¦
                    "type_weight": type_weight,  # ç±»å‹æƒé‡
                    "source": "background"
                })
            
            # 5. æŒ‰ç±»å‹æƒé‡é™åºæ’åºï¼ˆæƒé‡ç›¸åŒåˆ™æŒ‰çƒ­åº¦æ’åºï¼‰
            background_entities.sort(key=lambda x: (x["type_weight"], x["event_count"]), reverse=True)
            
            # æ—¥å¿—ï¼šæ˜¾ç¤ºæ’åºç»“æœ
            if background_entities:
                top_preview = [
                    f"[{e['type']}]{e['name']}(æƒé‡={e['type_weight']:.2f}, çƒ­åº¦={e['event_count']})"
                    for e in background_entities[:5]
                ]
                self.logger.info(f"ğŸ“Š èƒŒæ™¯å®ä½“(æŒ‰ç±»å‹æƒé‡æ’åºTop5): {top_preview}")
            
            # 6. é™åˆ¶æ•°é‡
            return background_entities[:max_count]

    async def _vector_search_entities(
        self, config: SearchConfig
    ) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
        """
        ä½¿ç”¨å‘é‡æœç´¢å¬å›å®ä½“ï¼ˆå¤ç”¨å¿«é€Ÿæ¨¡å¼é€»è¾‘ï¼‰

        Args:
            config: æœç´¢é…ç½®

        Returns:
            Tuple[candidate_entities, k1_weights]:
                - candidate_entities: å¬å›çš„å®ä½“åˆ—è¡¨
                - k1_weights: å®ä½“IDåˆ°ç›¸ä¼¼åº¦çš„æ˜ å°„
        """
        self.logger.info(f"ğŸ” å‘é‡æœç´¢å®ä½“: query='{config.query}'")

        try:
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„query_embedding
            if config.has_query_embedding and config.query_embedding:
                query_embedding = config.query_embedding
                self.logger.debug(f"ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„queryå‘é‡ï¼Œç»´åº¦: {len(query_embedding)}")
            else:
                # ç”ŸæˆåŸå§‹queryçš„embedding
                query_embedding = await self.processor.generate_embedding(config.query)
                # ç¼“å­˜query_embeddingåˆ°config
                config.query_embedding = query_embedding
                config.has_query_embedding = True
                self.logger.debug(f"âœ… Queryå‘é‡ç”ŸæˆæˆåŠŸï¼Œç»´åº¦: {len(query_embedding)}")

            # å‘é‡æœç´¢entity
            similar_entities = await self.entity_repo.search_similar(
                query_vector=query_embedding,
                k=config.recall.vector_top_k,
                source_config_ids=config.get_source_config_ids(),
                entity_type=None,  # ä¸é™åˆ¶ç±»å‹
                include_type_threshold=True,
            )

            self.logger.info(f"ğŸ“Š å‘é‡æœç´¢åˆ° {len(similar_entities)} ä¸ªå€™é€‰å®ä½“")

            # ğŸ†• è¿‡æ»¤æŒ‡å®šç±»å‹çš„å®ä½“ï¼ˆstart_time, end_time ç­‰ï¼‰
            exclude_types = set(config.exclude_entity_types)
            filtered_by_type_count = 0
            if exclude_types:
                original_count = len(similar_entities)
                similar_entities = [
                    e for e in similar_entities
                    if e.get("type") not in exclude_types
                ]
                filtered_by_type_count = original_count - len(similar_entities)
                if filtered_by_type_count > 0:
                    self.logger.info(
                        f"ğŸš« ç±»å‹è¿‡æ»¤: è¿‡æ»¤æ‰ {filtered_by_type_count} ä¸ªå®ä½“ (ç±»å‹: {exclude_types})"
                    )

            # è¿‡æ»¤é˜ˆå€¼
            candidate_entities = []
            k1_weights = {}

            for entity in similar_entities:
                similarity = float(entity.get("_score", 0.0))

                if similarity >= config.recall.entity_similarity_threshold:
                    # è·å–ç±»å‹é˜ˆå€¼å’Œæƒé‡
                    type_threshold = entity.get("type_threshold", 0.800)
                    type_weight = entity.get("type_weight", 1.0)
                    final_threshold = config.recall.entity_similarity_threshold
                    # è®¡ç®—åŠ æƒåˆ†æ•°
                    effective_score = similarity * type_weight
                    candidate_entities.append({
                        "entity_id": entity["entity_id"],
                        "name": entity["name"],
                        "type": entity["type"],
                        "similarity": similarity,
                        "type_weight": type_weight,
                        "effective_score": effective_score,
                        "source_attribute": config.query,
                        "type_threshold": type_threshold,
                        "final_threshold": final_threshold,
                        "match_method": "vector_search"
                    })
                    # æ™®é€šæ¨¡å¼ï¼šk1_weights ç›´æ¥ä½¿ç”¨ type_weightï¼Œä¸è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
                    k1_weights[entity["entity_id"]] = type_weight

            self.logger.info(
                f"ğŸ“ˆ é˜ˆå€¼è¿‡æ»¤ç»“æœ: é€šè¿‡ {len(candidate_entities)}/{len(similar_entities)}"
                f"{f' (å·²è¿‡æ»¤{filtered_by_type_count}ä¸ªæ—¶é—´ç±»å‹)' if filtered_by_type_count > 0 else ''}"
            )

            # æ‰“å°å‘é‡å¬å›çš„å®ä½“è¯¦æƒ…
            if candidate_entities:
                entity_names = [f"[{e['type']}]{e['name']}" for e in candidate_entities]
                self.logger.info(f"ğŸ“‹ å‘é‡å¬å›å®ä½“: {entity_names}")

            return candidate_entities, k1_weights

        except Exception as e:
            self.logger.error(f"âŒ å‘é‡æœç´¢å¤±è´¥: {e}", exc_info=True)
            raise

    def _merge_exact_search_results(
        self,
        exact_matched_entities: List[Dict[str, Any]],
        max_count: int,
        entity_types: Optional[List[Dict[str, Any]]] = None
    ) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
        """
        åˆå¹¶ç²¾ç¡®æœç´¢ç»“æœ

        Args:
            exact_matched_entities: ç²¾ç¡®æœç´¢åˆ°çš„å®ä½“åˆ—è¡¨
            max_count: æœ€å¤§è¿”å›æ•°é‡
            entity_types: å®ä½“ç±»å‹åˆ—è¡¨ï¼ˆåŒ…å« name å’Œ weightï¼‰

        Returns:
            Tuple[merged_entities, k1_weights]:
                - merged_entities: åˆå¹¶å»é‡åçš„å®ä½“åˆ—è¡¨
                - k1_weights: å®ä½“IDåˆ°effective_scoreçš„æ˜ å°„
        """
        seen_ids = set()
        merged = []
        k1_weights = {}

        # æ„å»ºç±»å‹æƒé‡æ˜ å°„
        type_weight_map: Dict[str, float] = {}
        if entity_types:
            for et in entity_types:
                type_name = et.get("name", "")
                type_weight = et.get("weight", 1.0)
                type_weight_map[type_name] = type_weight

        # æ·»åŠ ç²¾ç¡®æœç´¢çš„å®ä½“ï¼ˆéƒ½å·²é€šè¿‡æœç´¢éªŒè¯ï¼‰
        for entity in exact_matched_entities:
            entity_id = entity["entity_id"]
            if entity_id not in seen_ids:
                seen_ids.add(entity_id)

                # è·å– similarity å’Œ type_weight
                similarity = entity.get("similarity", 0.5)
                entity_type = entity.get("type", "")
                type_weight = type_weight_map.get(entity_type, 1.0)

                # è®¡ç®— effective_score = similarity Ã— type_weightï¼ˆç”¨äºæ’åºï¼‰
                effective_score = similarity * type_weight

                # è®¾ç½®åˆ°å®ä½“ä¸Š
                entity["type_weight"] = type_weight
                entity["effective_score"] = effective_score

                merged.append(entity)
                # æ™®é€šæ¨¡å¼ï¼šk1_weights ç›´æ¥ä½¿ç”¨ type_weightï¼Œä¸ä½¿ç”¨ effective_score
                k1_weights[entity_id] = type_weight

        # æŒ‰ effective_score æ’åºå¹¶é™åˆ¶æ•°é‡
        merged.sort(key=lambda x: x.get("effective_score", 0), reverse=True)
        merged = merged[:max_count]

        self.logger.info(
            f"æœç´¢ç»“æœåˆå¹¶å®Œæˆ: "
            f"ç²¾ç¡®åŒ¹é…={len(exact_matched_entities)}, "
            f"æœ€ç»ˆ={len(merged)}"
        )

        # æ‰“å°åˆå¹¶åçš„æœ€ç»ˆå®ä½“
        if merged:
            merged_names = [f"[{e['type']}]{e['name']}" for e in merged]
            self.logger.info(f"ğŸ“‹ åˆå¹¶åå®ä½“: {merged_names}")

        return merged, k1_weights
