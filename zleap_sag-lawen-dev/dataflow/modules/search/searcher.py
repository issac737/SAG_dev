"""
æœç´¢å™¨ - ç»Ÿä¸€å…¥å£

åªä¿ç•™SAGå¼•æ“ï¼Œå®ç°ä¸‰é˜¶æ®µæœç´¢ï¼šrecall â†’ expand â†’ rerank
"""

import time
from typing import Dict, List, Any, Optional

from dataflow.core.ai.base import BaseLLMClient
from dataflow.core.prompt.manager import PromptManager
from dataflow.db import SourceEvent
from dataflow.exceptions import SearchError
from dataflow.modules.search.config import SearchConfig, RerankStrategy, ReturnType
from dataflow.modules.search.recall import RecallSearcher, RecallResult
from dataflow.modules.search.expand import ExpandSearcher, ExpandResult
from dataflow.modules.search.ranking.pagerank import RerankPageRankSearcher as EventPageRankSearcher
from dataflow.modules.search.ranking.pagerank_section import RerankPageRankSearcher as SectionPageRankSearcher
from dataflow.modules.search.ranking.rrf import RerankRRFSearcher
from dataflow.utils import get_logger

logger = get_logger("search.searcher")


class SAGSearcher:
    """
    SAGæœç´¢å™¨ï¼ˆStructured + Attributes + Graphï¼‰

    ä¸‰é˜¶æ®µæœç´¢æµç¨‹ï¼š
    1. Recall - å®ä½“å¬å›ï¼ˆä»queryå¬å›ç›¸å…³å®ä½“ï¼‰
    2. Expand - å®ä½“æ‰©å±•ï¼ˆé€šè¿‡å¤šè·³å…³ç³»å‘ç°æ›´å¤šå®ä½“ï¼‰
    3. Rerank - é‡æ’åºï¼ˆåŸºäºå®ä½“æ£€ç´¢å’Œæ’åºäº‹é¡¹æˆ–æ®µè½ï¼‰

    è¿”å›ç»“æœï¼ˆæ ¹æ® return_type é…ç½®ï¼‰ï¼š

    EVENTæ¨¡å¼ï¼ˆè¿”å›äº‹é¡¹ï¼Œé»˜è®¤ï¼‰:
    {
        "events": List[SourceEvent],  # äº‹é¡¹åˆ—è¡¨
        "clues": List[Dict],           # çº¿ç´¢åˆ—è¡¨ï¼ˆæ”¯æŒå‰ç«¯å›¾è°±ï¼‰
        "stats": Dict,                 # ç»Ÿè®¡ä¿¡æ¯
        "query": Dict                  # æŸ¥è¯¢ä¿¡æ¯
    }

    PARAGRAPHæ¨¡å¼ï¼ˆè¿”å›æ®µè½ï¼‰:
    {
        "sections": List[Dict],        # æ®µè½åˆ—è¡¨
        "clues": List[Dict],           # çº¿ç´¢åˆ—è¡¨ï¼ˆæ”¯æŒå‰ç«¯å›¾è°±ï¼‰
        "stats": Dict,                 # ç»Ÿè®¡ä¿¡æ¯
        "query": Dict                  # æŸ¥è¯¢ä¿¡æ¯
    }
    """
    
    def __init__(
        self,
        prompt_manager: PromptManager,
        model_config: Optional[Dict] = None,
    ):
        """
        åˆå§‹åŒ–æœç´¢å™¨
        
        Args:
            prompt_manager: æç¤ºè¯ç®¡ç†å™¨
            model_config: LLMé…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
                - å¦‚æœä¼ å…¥ï¼šä½¿ç”¨è¯¥é…ç½®
                - å¦‚æœä¸ä¼ ï¼šè‡ªåŠ¨ä»é…ç½®ç®¡ç†å™¨è·å– 'search' åœºæ™¯é…ç½®
        """
        self.prompt_manager = prompt_manager
        self.model_config = model_config
        self._llm_client = None  # å»¶è¿Ÿåˆå§‹åŒ–
        self.logger = get_logger("search.sag")
        
        self.logger.info("SAGæœç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def _get_llm_client(self) -> BaseLLMClient:
        """è·å–LLMå®¢æˆ·ç«¯ï¼ˆæ‡’åŠ è½½ï¼‰"""
        if self._llm_client is None:
            from dataflow.core.ai.factory import create_llm_client
            
            self._llm_client = await create_llm_client(
                scenario='search',
                model_config=self.model_config
            )
        
        # åˆå§‹åŒ–ä¸‰é˜¶æ®µæœç´¢å™¨
        self.recall_searcher = RecallSearcher(llm_client=self._llm_client, prompt_manager=self.prompt_manager)
        self.expand_searcher = ExpandSearcher(
                self._llm_client,
                self.prompt_manager,
            self.recall_searcher
        )

        # åˆå§‹åŒ–é‡æ’ç­–ç•¥ - äº‹é¡¹çº§
        self.rerank_event_pagerank = EventPageRankSearcher(self._llm_client)
        self.rerank_rrf = RerankRRFSearcher(llm_client=self._llm_client)

        # åˆå§‹åŒ–é‡æ’ç­–ç•¥ - æ®µè½çº§
        self.rerank_section_pagerank = SectionPageRankSearcher(self._llm_client)
        
        return self._llm_client
    
    async def search(self, config: SearchConfig) -> Dict[str, Any]:
        """
        æ‰§è¡Œæœç´¢

        Args:
            config: æœç´¢é…ç½®

        Returns:
            æ ¹æ® config.return_type è¿”å›ä¸åŒæ ¼å¼ï¼š

            EVENTæ¨¡å¼ï¼ˆè¿”å›äº‹é¡¹ï¼‰:
            {
                "events": List[SourceEvent],  # äº‹é¡¹åˆ—è¡¨
                "clues": List[Dict],           # å®Œæ•´çº¿ç´¢é“¾
                "stats": Dict,                 # ç»Ÿè®¡ä¿¡æ¯
                "query": Dict                  # æŸ¥è¯¢ä¿¡æ¯
            }

            PARAGRAPHæ¨¡å¼ï¼ˆè¿”å›æ®µè½ï¼‰:
            {
                "sections": List[Dict],        # æ®µè½åˆ—è¡¨
                "clues": List[Dict],           # å®Œæ•´çº¿ç´¢é“¾
                "stats": Dict,                 # ç»Ÿè®¡ä¿¡æ¯
                "query": Dict                  # æŸ¥è¯¢ä¿¡æ¯
            }
        """
        try:
            # ç¡®ä¿LLMå®¢æˆ·ç«¯å·²åˆå§‹åŒ–
            await self._get_llm_client()

            total_start = time.perf_counter()

            # ğŸ†• æ‰“å°å®Œæ•´çš„é…ç½®å‚æ•°ï¼Œæ–¹ä¾¿éªŒè¯å‰ç«¯ä¼ å‚
            self.logger.info("=" * 100)
            self.logger.info("ğŸ“‹ SAGæœç´¢é…ç½®å‚æ•°è¯¦æƒ…:")
            self.logger.info("=" * 100)

            # åŸºç¡€å‚æ•°
            self.logger.info("ğŸ”¹ åŸºç¡€å‚æ•°:")
            self.logger.info(f"  query: '{config.query}'")
            self.logger.info(f"  original_query: '{config.original_query}'")
            self.logger.info(f"  source_config_id: {config.source_config_id}")
            self.logger.info(f"  source_config_ids: {config.source_config_ids[:5]}")
            self.logger.info(f"  enable_query_rewrite: {config.enable_query_rewrite}")
            self.logger.info(f"  return_type: {config.return_type}")

            # Recall é…ç½®
            self.logger.info("")
            self.logger.info("ğŸ”¹ Recall (å®ä½“å¬å›) é…ç½®:")
            self.logger.info(f"  recall_mode: {config.recall.recall_mode}")
            self.logger.info(f"  use_fast_mode: {config.recall.use_fast_mode}")
            self.logger.info(f"  vector_top_k: {config.recall.vector_top_k}")
            self.logger.info(f"  vector_candidates: {config.recall.vector_candidates}")
            self.logger.info(f"  entity_similarity_threshold: {config.recall.entity_similarity_threshold}")
            self.logger.info(f"  event_similarity_threshold: {config.recall.event_similarity_threshold}")
            self.logger.info(f"  max_entities: {config.recall.max_entities}")
            self.logger.info(f"  max_events: {config.recall.max_events}")
            self.logger.info(f"  entity_weight_threshold: {config.recall.entity_weight_threshold}")
            self.logger.info(f"  final_entity_count: {config.recall.final_entity_count}")
            self.logger.info(
                f"  use_tokenizer: {config.recall.use_tokenizer}")

            # Expand é…ç½®
            self.logger.info("")
            self.logger.info("ğŸ”¹ Expand (å®ä½“æ‰©å±•) é…ç½®:")
            self.logger.info(f"  enabled: {config.expand.enabled}")
            self.logger.info(f"  max_hops: {config.expand.max_hops}")
            self.logger.info(f"  entities_per_hop: {config.expand.entities_per_hop}")
            self.logger.info(f"  weight_change_threshold: {config.expand.weight_change_threshold}")
            self.logger.info(f"  event_similarity_threshold: {config.expand.event_similarity_threshold}")
            self.logger.info(f"  min_events_per_hop: {config.expand.min_events_per_hop}")
            self.logger.info(f"  max_events_per_hop: {config.expand.max_events_per_hop}")

            # Rerank é…ç½®
            self.logger.info("")
            self.logger.info("ğŸ”¹ Rerank (äº‹é¡¹é‡æ’) é…ç½®:")
            self.logger.info(f"  strategy: {config.rerank.strategy}")
            self.logger.info(f"  score_threshold: {config.rerank.score_threshold}")
            self.logger.info(f"  max_results: {config.rerank.max_results}")
            self.logger.info(f"  max_key_recall_results: {config.rerank.max_key_recall_results}")
            self.logger.info(f"  max_query_recall_results: {config.rerank.max_query_recall_results}")
            self.logger.info(f"  pagerank_damping_factor: {config.rerank.pagerank_damping_factor}")
            self.logger.info(f"  pagerank_max_iterations: {config.rerank.pagerank_max_iterations}")
            self.logger.info(f"  rrf_k: {config.rerank.rrf_k}")
            self.logger.info("=" * 100)

            self.logger.info(
                f"ğŸ” å¼€å§‹æœç´¢ï¼šquery='{config.query}', source_config_id={config.source_config_id}"
            )

            # Recall: å®ä½“å¬å›
            recall_start = time.perf_counter()
            recall_result = await self._recall(config)
            recall_time = time.perf_counter() - recall_start
            
            # Expand: å®ä½“æ‰©å±•ï¼ˆå¯é€‰ï¼‰
            expand_start = time.perf_counter()
            if config.expand.enabled:
                expand_result = await self._expand(config, recall_result)
            else:
                expand_result = recall_result  # è·³è¿‡æ‰©å±•
            expand_time = time.perf_counter() - expand_start
            
            # Rerank: äº‹é¡¹é‡æ’
            rerank_start = time.perf_counter()
            rerank_result = await self._rerank(config, expand_result)
            rerank_time = time.perf_counter() - rerank_start
            
            total_time = time.perf_counter() - total_start

            # è¾“å‡ºè€—æ—¶ç»Ÿè®¡
            self._log_timing(
                recall_time,
                expand_time,
                rerank_time,
                total_time,
                recall_result,
                expand_result,
            )
            
            # æ„å»ºæœ€ç»ˆå“åº”
            response = self._build_response(
                config=config,
                recall_result=recall_result,
                expand_result=expand_result,
                rerank_result=rerank_result,
            )

            # æ ¹æ® return_type è¾“å‡ºä¸åŒçš„æ—¥å¿—
            if config.return_type == ReturnType.PARAGRAPH:
                self.logger.info(
                    f"âœ… æœç´¢å®Œæˆï¼šè¿”å› {len(response.get('sections', []))} ä¸ªæ®µè½ï¼Œ"
                    f"{len(response['clues'])} æ¡çº¿ç´¢"
                )
            else:
                self.logger.info(
                    f"âœ… æœç´¢å®Œæˆï¼šè¿”å› {len(response.get('events', []))} ä¸ªäº‹é¡¹ï¼Œ"
                    f"{len(response['clues'])} æ¡çº¿ç´¢"
                )

            return response

        except Exception as e:
            self.logger.error(f"âŒ æœç´¢å¤±è´¥: {e}", exc_info=True)
            raise SearchError(f"æœç´¢å¤±è´¥: {e}") from e
    
    async def _recall(self, config: SearchConfig) -> RecallResult:
        """
        Recall: å®ä½“å¬å›
        
        ä»queryå¬å›ç›¸å…³å®ä½“
        """
        self.logger.info("ğŸ“ Recall: å®ä½“å¬å›")
        result = await self.recall_searcher.search(config)
        self.logger.info(f"âœ“ Recallå®Œæˆï¼šå¬å› {len(result.key_final)} ä¸ªå®ä½“")
        return result
    
    async def _expand(
        self, 
        config: SearchConfig, 
        recall_result: RecallResult
    ) -> ExpandResult:
        """
        Expand: å®ä½“æ‰©å±•
        
        åŸºäºå¬å›çš„å®ä½“ï¼Œé€šè¿‡å¤šè·³å…³ç³»å‘ç°æ›´å¤šç›¸å…³å®ä½“
        """
        self.logger.info("ğŸ“ Expand: å®ä½“æ‰©å±•")
        result = await self.expand_searcher.search(config, recall_result)
        self.logger.info(
            f"âœ“ Expandå®Œæˆï¼šæ‰©å±•åˆ° {len(result.key_final)} ä¸ªå®ä½“ï¼Œ"
            f"è·³è·ƒ {result.total_jumps} æ¬¡"
        )
        return result
    
    async def _rerank(
        self,
        config: SearchConfig,
        expand_result: ExpandResult
    ) -> Dict[str, Any]:
        """
        Rerank: äº‹é¡¹/æ®µè½é‡æ’

        æ ¹æ® return_type é€‰æ‹©è¿”å›äº‹é¡¹æˆ–æ®µè½ï¼š
        - EVENT: è¿”å›äº‹é¡¹åˆ—è¡¨ï¼ˆä½¿ç”¨ pagerank.pyï¼‰
        - PARAGRAPH: è¿”å›æ®µè½åˆ—è¡¨ï¼ˆä½¿ç”¨ pagerank_section.pyï¼‰
        """
        strategy = config.rerank.strategy
        return_type = config.return_type

        self.logger.info(
            f"ğŸ“ Rerank: {'äº‹é¡¹' if return_type == ReturnType.EVENT else 'æ®µè½'}é‡æ’"
            f"ï¼ˆç­–ç•¥={strategy}, è¿”å›ç±»å‹={return_type}ï¼‰"
        )

        # æ ¹æ® return_type é€‰æ‹©ä¸åŒçš„ Rerank å®ç°
        if return_type == ReturnType.PARAGRAPH:
            # æ®µè½çº§ PageRankï¼ˆåªæ”¯æŒ PAGERANK ç­–ç•¥ï¼‰
            if strategy != RerankStrategy.PAGERANK:
                self.logger.warning(
                    f"æ®µè½è¿”å›æ¨¡å¼ä»…æ”¯æŒ PAGERANK ç­–ç•¥ï¼Œå½“å‰ç­–ç•¥ {strategy} å°†è¢«å¿½ç•¥"
                )
            reranker = self.rerank_section_pagerank
        else:
            # äº‹é¡¹çº§é‡æ’ï¼ˆæ”¯æŒ PAGERANK å’Œ RRFï¼‰
            if strategy == RerankStrategy.PAGERANK:
                reranker = self.rerank_event_pagerank
            elif strategy == RerankStrategy.RRF:
                reranker = self.rerank_rrf
            else:
                raise SearchError(f"ä¸æ”¯æŒçš„é‡æ’ç­–ç•¥: {strategy}")

        # æ‰§è¡Œé‡æ’
        result = await reranker.search(
            key_final=expand_result.key_final,
            config=config
        )

        # æ—¥å¿—è¾“å‡º
        if return_type == ReturnType.PARAGRAPH:
            self.logger.info(
                f"âœ“ Rerankå®Œæˆï¼šè¿”å› {len(result.get('sections', []))} ä¸ªæ®µè½"
            )
        else:
            self.logger.info(
                f"âœ“ Rerankå®Œæˆï¼šè¿”å› {len(result.get('events', []))} ä¸ªäº‹é¡¹"
            )

        return result
    
    def _log_timing(
        self,
        recall_time: float,
        expand_time: float,
        rerank_time: float,
        total_time: float,
        recall_result: Optional[Any] = None,
        expand_result: Optional[Any] = None
    ):
        """è¾“å‡ºè€—æ—¶ç»Ÿè®¡"""

        def format_recall_steps(timings: Dict[str, float], step1_substep_timings: Optional[Dict[str, float]], recall_time_total: float):
            """æ ¼å¼åŒ– recall å„æ­¥éª¤è€—æ—¶"""
            if not timings:
                return ""

            lines = []
            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨å¤–å±‚ä¼ å…¥çš„ recall_time_total ä½œä¸ºåŸºæ•°ï¼Œè€Œä¸æ˜¯å†…å±‚çš„ timings['total']
            total = recall_time_total

            # ğŸ†• æ·»åŠ ï¼šæ˜¾ç¤ºå¬å›ç­–ç•¥ï¼ˆåŒ…æ‹¬ Fast/Normal å’Œ FUZZY/EXACTï¼‰
            if step1_substep_timings:
                # åˆ¤æ–­å¿«é€Ÿæ¨¡å¼è¿˜æ˜¯æ™®é€šæ¨¡å¼
                if "fast_generate_embedding" in step1_substep_timings:
                    mode_type = "å¿«é€Ÿæ¨¡å¼ Fast"
                elif "normal_vector_search" in step1_substep_timings:
                    mode_type = "æ™®é€šæ¨¡å¼ Normal"
                else:
                    mode_type = "æœªçŸ¥"
                
                # åˆ¤æ–­ç²¾ç¡®æœç´¢æ–¹å¼ï¼ˆFUZZY=ES / EXACT=MySQLï¼‰
                if "normal_mysql_exact" in step1_substep_timings and step1_substep_timings.get("normal_mysql_exact", 0) > 0:
                    recall_mode_type = "EXACT(MySQL)"
                elif "normal_es_exact" in step1_substep_timings and step1_substep_timings.get("normal_es_exact", 0) > 0:
                    recall_mode_type = "FUZZY(ES)"
                else:
                    recall_mode_type = "N/A"
                
                lines.append(f"  å¬å›ç­–ç•¥: ã€{mode_type} | {recall_mode_type}ã€‘")
                lines.append("")

            step_names = {
                "step1": "æ­¥éª¤1 (queryæ‰¾key)",
                "step2": "æ­¥éª¤2 (keyæ‰¾event)",
                "step3": "æ­¥éª¤3 (queryå†æ‰¾event)",
                "step4": "æ­¥éª¤4 (è¿‡æ»¤Event)",
                "step5": "æ­¥éª¤5 (è®¡ç®—event-keyæƒé‡)",
                "step6": "æ­¥éª¤6 (è®¡ç®—event-key-query)",
                "step7": "æ­¥éª¤7 (åå‘è®¡ç®—keyæƒé‡)",
                "step8": "æ­¥éª¤8 (æå–é‡è¦key)"
            }

            for step, name in step_names.items():
                if step in timings:
                    time_val = timings[step]
                    percentage = (time_val / total * 100) if total > 0 else 0
                    lines.append(f"  â”œâ”€â”€ {name}: {time_val:.3f}s ({percentage:.1f}%)")

                    # å¦‚æœæ˜¯step1ï¼Œæ·»åŠ å­æ­¥éª¤è¯¦ç»†ä¿¡æ¯
                    if step == "step1" and step1_substep_timings:
                        # å¿«é€Ÿæ¨¡å¼å­æ­¥éª¤
                        if "fast_generate_embedding" in step1_substep_timings:
                            # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ åˆ†è¯å¬å›
                            fast_tokenizer = step1_substep_timings.get("fast_tokenizer", 0)
                            fast_total = (
                                step1_substep_timings.get("fast_generate_embedding", 0) +
                                step1_substep_timings.get("fast_vector_search", 0) +
                                step1_substep_timings.get("fast_filter_threshold", 0) +
                                step1_substep_timings.get("fast_deduplicate", 0) +
                                fast_tokenizer
                            )
                            if fast_total > 0:
                                gen_embedding = step1_substep_timings.get('fast_generate_embedding', 0)
                                vector_search = step1_substep_timings.get('fast_vector_search', 0)
                                filter_threshold = step1_substep_timings.get('fast_filter_threshold', 0)
                                deduplicate = step1_substep_timings.get('fast_deduplicate', 0)
                                lines.append(f"  â”‚   â”œâ”€â”€ ç”Ÿæˆå‘é‡: {gen_embedding:.3f}s ({gen_embedding/fast_total*100:.1f}%)")
                                lines.append(f"  â”‚   â”œâ”€â”€ å‘é‡æœç´¢: {vector_search:.3f}s ({vector_search/fast_total*100:.1f}%)")
                                lines.append(f"  â”‚   â”œâ”€â”€ é˜ˆå€¼è¿‡æ»¤: {filter_threshold:.3f}s ({filter_threshold/fast_total*100:.1f}%)")
                                lines.append(f"  â”‚   â”œâ”€â”€ å»é‡é™åˆ¶: {deduplicate:.3f}s ({deduplicate/fast_total*100:.1f}%)")
                                if fast_tokenizer > 0:
                                    lines.append(f"  â”‚   â”œâ”€â”€ åˆ†è¯å¬å›: {fast_tokenizer:.3f}s ({fast_tokenizer/fast_total*100:.1f}%)")
                                lines.append(f"  â”‚       â””â”€â”€ å¿«é€Ÿæ¨¡å¼æ€»è®¡: {fast_total:.3f}s")
                        # æ™®é€šæ¨¡å¼å­æ­¥éª¤
                        elif "normal_vector_search" in step1_substep_timings:
                            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ key åç§°
                            mysql_time = step1_substep_timings.get("normal_mysql_exact", 0)
                            es_time = step1_substep_timings.get("normal_es_exact", 0)
                            exact_search_time = mysql_time if mysql_time > 0 else es_time
                            exact_search_label = "MySQLç²¾ç¡®åŒ¹é…" if mysql_time > 0 else "ESæ¨¡ç³ŠåŒ¹é…"
                            
                            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„ key
                            vector_search = step1_substep_timings.get('normal_vector_search', 0)
                            get_entity_types = step1_substep_timings.get('normal_get_entity_types', 0)
                            llm_rewrite = step1_substep_timings.get('normal_llm_rewrite_extract', 0)  # ğŸ”§ ä¿®å¤ key
                            tokenizer_time = step1_substep_timings.get('normal_tokenizer', 0)         # ğŸ†• æ–°å¢
                            merge_results = step1_substep_timings.get('normal_merge_results', 0)      # ğŸ”§ ä¿®å¤ key
                            
                            normal_total = (
                                vector_search +
                                get_entity_types +
                                llm_rewrite +
                                exact_search_time +
                                tokenizer_time +
                                merge_results
                            )
                            
                            if normal_total > 0:
                                lines.append(f"  â”‚   â”œâ”€â”€ å‘é‡å¬å›: {vector_search:.3f}s ({vector_search/normal_total*100:.1f}%)")
                                lines.append(f"  â”‚   â”œâ”€â”€ è·å–ç±»å‹: {get_entity_types:.3f}s ({get_entity_types/normal_total*100:.1f}%)")
                                lines.append(f"  â”‚   â”œâ”€â”€ LLMé‡å†™+è¯†åˆ«: {llm_rewrite:.3f}s ({llm_rewrite/normal_total*100:.1f}%)")
                                lines.append(f"  â”‚   â”œâ”€â”€ {exact_search_label}: {exact_search_time:.3f}s ({exact_search_time/normal_total*100:.1f}%)")
                                if tokenizer_time > 0:
                                    lines.append(f"  â”‚   â”œâ”€â”€ åˆ†è¯å¬å›: {tokenizer_time:.3f}s ({tokenizer_time/normal_total*100:.1f}%)")
                                lines.append(f"  â”‚   â””â”€â”€ åˆå¹¶ç»“æœ: {merge_results:.3f}s ({merge_results/normal_total*100:.1f}%)")
                                lines.append(f"  â”‚       â””â”€â”€ æ™®é€šæ¨¡å¼æ€»è®¡: {normal_total:.3f}s")
                                
                                # ğŸ†• æ˜¾ç¤ºæœªç»Ÿè®¡çš„è€—æ—¶ï¼ˆå¦‚æœæœ‰ï¼‰
                                step1_time = timings.get("step1", 0)
                                if step1_time > normal_total + 0.01:
                                    untracked = step1_time - normal_total
                                    lines.append(f"  â”‚       âš ï¸  æœªç»Ÿè®¡: {untracked:.3f}s ({untracked/step1_time*100:.1f}%)")

            # ğŸ†• æ·»åŠ ï¼šæ˜¾ç¤ºæ­¥éª¤æ€»å’Œä¸æ€»æ—¶é—´çš„å¯¹æ¯”
            steps_sum = sum(timings.get(step, 0) for step in step_names.keys())
            overhead = total - steps_sum
            if overhead > 0.001:  # å¦‚æœå¼€é”€å¤§äº1ms
                lines.append("")
                lines.append(f"  æ­¥éª¤æ€»è®¡: {steps_sum:.3f}s ({steps_sum/total*100:.1f}%)")
                lines.append(f"  å…¶ä»–å¼€é”€: {overhead:.3f}s ({overhead/total*100:.1f}%)")

            return "\n".join(lines)

        self.logger.info("=" * 80)
        self.logger.info("â±ï¸  æœç´¢è€—æ—¶ç»Ÿè®¡ï¼š")
        self.logger.info("-" * 80)

        # Recall é˜¶æ®µè¯¦ç»†ä¿¡æ¯
        if recall_result and hasattr(recall_result, 'step_timings'):
            step1_substep_timings = getattr(recall_result, 'step1_substep_timings', None)
            # ğŸ”§ ä¿®å¤ï¼šä¼ å…¥ recall_time ä½œä¸ºåŸºæ•°
            self.logger.info(
                f"Recall é˜¶æ®µ (æ€»è®¡: {recall_time:.3f}s ({recall_time/total_time*100:.1f}%)):\n"
                f"{format_recall_steps(recall_result.step_timings, step1_substep_timings, recall_time)}"
            )
        else:
            self.logger.info(
                f"Recall (å®ä½“å¬å›): {recall_time:.3f}ç§’ "
                f"({recall_time/total_time*100:.1f}%)"
            )

        # Expand é˜¶æ®µè¯¦ç»†ä¿¡æ¯
        self.logger.info("")
        expand_details = []
        if expand_result and hasattr(expand_result, "step_average_timings"):
            step_average_timings = getattr(expand_result, "step_average_timings", {}) or {}
            total_jumps = getattr(expand_result, "total_jumps", 0)
            if step_average_timings:
                step_labels = {
                    "step1": "keyæ‰¾event",
                    "step2": "queryåŒ¹é…",
                    "step3": "event-keyæƒé‡",
                    "step4": "event-key-query",
                    "step5": "åå‘ç®—key",
                }
                for step in ["step1", "step2", "step3", "step4", "step5"]:
                    if step in step_average_timings:
                        expand_details.append(f"{step_labels.get(step, step)}: {step_average_timings[step]:.3f}s")
        
        if expand_details:
            self.logger.info(
                f"Expand (å®ä½“æ‰©å±•): {expand_time:.3f}ç§’ ({expand_time/total_time*100:.1f}%) "
                f"| è·³æ•°: {total_jumps}"
            )
            self.logger.info(f"  â””â”€â”€ æ¯è·³å¹³å‡: {', '.join(expand_details)}")
        else:
            self.logger.info(
                f"Expand (å®ä½“æ‰©å±•): {expand_time:.3f}ç§’ ({expand_time/total_time*100:.1f}%)"
            )
        
        # Rerank é˜¶æ®µ
        self.logger.info(
            f"Rerank (äº‹é¡¹é‡æ’): {rerank_time:.3f}ç§’ ({rerank_time/total_time*100:.1f}%)"
        )
        
        self.logger.info("-" * 80)
        self.logger.info(f"æ€»è€—æ—¶: {total_time:.3f}ç§’ (100%)")
        
        # ğŸ†• æ€§èƒ½ç“¶é¢ˆæç¤º
        bottleneck_threshold = 2.0  # 2ç§’ä»¥ä¸Šä¸ºç“¶é¢ˆ
        bottlenecks = []
        if recall_time > bottleneck_threshold:
            bottlenecks.append(f"Recall({recall_time:.1f}s)")
        if expand_time > bottleneck_threshold:
            bottlenecks.append(f"Expand({expand_time:.1f}s)")
        if rerank_time > bottleneck_threshold:
            bottlenecks.append(f"Rerank({rerank_time:.1f}s)")
        
        if bottlenecks:
            self.logger.info(f"âš ï¸  æ€§èƒ½ç“¶é¢ˆ: {', '.join(bottlenecks)}")
        
        self.logger.info("=" * 80)
    
    def _build_response(
        self,
        config: SearchConfig,
        recall_result: RecallResult,
        expand_result,  # Union[RecallResult, ExpandResult]
        rerank_result: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        æ„å»ºæœ€ç»ˆå“åº”

        æ•´åˆä¸‰é˜¶æ®µçš„çº¿ç´¢å’Œç»“æœï¼Œæ”¯æŒäº‹é¡¹å’Œæ®µè½ä¸¤ç§è¿”å›æ ¼å¼
        """
        from collections import Counter
        from dataflow.modules.search.path_analyzer import analyze_paths

        # ğŸ†• ä½¿ç”¨ç»Ÿä¸€çš„all_clueså­—æ®µï¼ˆæ–°ç‰ˆè¿½è¸ªå™¨ï¼‰
        all_clues = getattr(config, 'all_clues', [])

        # Fallback: å…¼å®¹æ—§ç‰ˆï¼ˆå¦‚æœall_cluesä¸ºç©ºï¼Œå°è¯•ä»æ—§å­—æ®µæ”¶é›†ï¼‰
        if not all_clues:
            all_clues.extend(getattr(config, 'recall_clues', []))
            all_clues.extend(getattr(config, 'expansion_clues', []))
            all_clues.extend(getattr(config, 'rerank_clues', []))

        # æ„å»ºç»Ÿè®¡ä¿¡æ¯
        recall_entities = recall_result.key_final
        expand_entities = [
            k for k in expand_result.key_final
            if k.get("steps", [0])[0] >= 2
        ]

        recall_by_type = Counter(
            e.get("type") for e in recall_entities if e.get("type")
        )

        # åˆ¤æ–­ expand æ˜¯å¦è¢«æ‰§è¡Œ
        from .expand import ExpandResult
        expand_was_executed = isinstance(expand_result, ExpandResult)

        # æ ¹æ® return_type æå–ç»“æœ
        return_type = config.return_type

        if return_type == ReturnType.PARAGRAPH:
            # æ®µè½æ¨¡å¼
            sections = rerank_result.get("sections", [])
            result_key = "sections"
            result_count = len(sections)
            self.logger.info(
                f"âœ¨ å“åº”æ„å»ºå®Œæˆï¼ˆæ®µè½æ¨¡å¼ï¼‰: "
                f"sections={result_count}, "
                f"clues={len(all_clues)}"
            )
        else:
            # äº‹é¡¹æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
            events = rerank_result.get("events", [])
            result_key = "events"
            result_count = len(events)
            self.logger.info(
                f"âœ¨ å“åº”æ„å»ºå®Œæˆï¼ˆäº‹é¡¹æ¨¡å¼ï¼‰: "
                f"events={result_count}, "
                f"clues={len(all_clues)}"
            )

        stats = {
            "recall": {
                "entities_count": len(recall_entities),
                "by_type": dict(recall_by_type),
                "step_timings": getattr(recall_result, 'step_timings', {}),
                "step1_substep_timings": getattr(recall_result, 'step1_substep_timings', None),
            },
            "expand": {
                "entities_count": len(expand_entities) if expand_was_executed else 0,
                "total_entities": len(expand_result.key_final),
                "hops": expand_result.total_jumps if expand_was_executed else 0,
                "converged": expand_result.convergence_reached if expand_was_executed else False,
            },
            "rerank": {
                f"{result_key}_count": result_count,  # events_count æˆ– sections_count
                "strategy": str(config.rerank.strategy),
                "return_type": str(return_type),  # æ–°å¢ï¼šè®°å½•è¿”å›ç±»å‹
            }
        }

        # æ„å»ºæŸ¥è¯¢ä¿¡æ¯
        query_info = {
            "original": config.original_query or config.query,
            "current": config.query,
            "rewritten": (
                config.original_query != config.query
                if config.original_query else False
            )
        }

        # æ„å»ºå“åº”ï¼ˆæ ¹æ® return_type ä½¿ç”¨ä¸åŒçš„é”®ï¼‰
        response = {
            result_key: rerank_result.get(result_key, []),  # "events" æˆ– "sections"
            "clues": all_clues,
            "stats": stats,
            "query": query_info,
        }

        # ğŸ†• è·¯å¾„åˆ†æï¼ˆä»…äº‹é¡¹æ¨¡å¼éœ€è¦ï¼Œæ®µè½æ¨¡å¼è·³è¿‡ï¼‰
        if return_type == ReturnType.EVENT:
            try:
                # ğŸ”§ åªå¯¹æœ€ç»ˆè¿”å›çš„äº‹é¡¹è¿›è¡Œè·¯å¾„åˆ†æ
                events = rerank_result.get("events", [])
                target_event_ids = [event.id for event in events]

                self.logger.info(
                    f"ğŸ” è·¯å¾„åˆ†ï¿½ï¿½ç›®æ ‡: {len(events)} ä¸ªäº‹é¡¹, "
                    f"äº‹é¡¹IDç¤ºä¾‹: {target_event_ids[:3] if len(target_event_ids) >= 3 else target_event_ids}"
                )

                path_result = analyze_paths(
                    all_clues,
                    target_event_ids=target_event_ids
                )

                # è·å–è·¯å¾„åˆ†æç»“æœ
                min_lines = path_result.get("min_lines", {})
                max_lines = path_result.get("max_lines", {})
                entitys = path_result.get("entitys", {})
                rerank_lines = path_result.get("rerank_lines", {})

                self.logger.info(
                    f"ğŸ“Š è·¯å¾„åˆ†æå®Œæˆ: äº‹é¡¹æ•°={len(min_lines)}, "
                    f"æœ€çŸ­è·¯å¾„={len(min_lines)}, æœ€é•¿è·¯å¾„={len(max_lines)}"
                )

                # è¾“å‡º entitys å®Œæ•´ç»Ÿè®¡
                if entitys:
                    self.logger.debug("=" * 80)
                    self.logger.debug(f"ğŸ“Š å®ä½“æŒ‰è·³æ•°å®Œæ•´ç»Ÿè®¡:")
                    for hop in sorted(entitys.keys(), key=lambda x: int(x)):
                        entities = entitys[hop]
                        self.logger.debug(f"\n  ã€Hop {hop}ã€‘ {len(entities)} ä¸ªå®ä½“:")
                        for idx, entity in enumerate(entities, 1):
                            self.logger.debug(
                                f"    {idx}. {entity.get('name', 'N/A')[:30]} "
                                f"| ç±»å‹: {entity.get('type', 'N/A')}"
                            )

                # è¾“å‡º rerank_lines å®Œæ•´æ•°æ®ï¼ˆåªæ˜¾ç¤ºå‰3ä¸ªäº‹é¡¹çš„æ‰€æœ‰è·¯å¾„ï¼‰
                if rerank_lines:
                    self.logger.debug("=" * 80)
                    self.logger.debug(f"ğŸ“‹ Top 3 äº‹é¡¹çš„æ‰€æœ‰è·¯å¾„ (å…± {len(rerank_lines)} ä¸ªäº‹é¡¹):")

                    # åªæ˜¾ç¤ºå‰3ä¸ªäº‹é¡¹
                    for idx, (event_id, all_paths) in enumerate(list(rerank_lines.items())[:3], 1):
                        # è·å–äº‹é¡¹æ ‡é¢˜ï¼ˆä»ç¬¬ä¸€æ¡è·¯å¾„çš„æœ€åä¸€ä¸ªèŠ‚ç‚¹ï¼‰
                        event_title = "æœªçŸ¥äº‹é¡¹"
                        if all_paths and len(all_paths) > 0:
                            for item in all_paths[0]:  # ç¬¬ä¸€æ¡è·¯å¾„
                                if "event" in item:
                                    event_title = item["event"].get("title", "æœªçŸ¥äº‹é¡¹")[:40]
                                    break

                        self.logger.debug(f"\n  ã€äº‹é¡¹ {idx}ã€‘ {event_title}")
                        self.logger.debug(f"  äº‹é¡¹ID: {event_id[:50]}...")
                        self.logger.debug(f"  è·¯å¾„æ€»æ•°: {len(all_paths)} æ¡")

                        # è¾“å‡ºæ¯æ¡è·¯å¾„
                        for path_idx, path_items in enumerate(all_paths, 1):
                            self.logger.debug(f"\n    â”Œâ”€ è·¯å¾„ {path_idx} (é•¿åº¦: {len(path_items)} ä¸ªèŠ‚ç‚¹)")

                            for step_idx, item in enumerate(path_items, 1):
                                if "query" in item:
                                    q = item["query"]
                                    self.logger.debug(
                                        f"    â”‚ æ­¥éª¤{step_idx} [Query]: {q.get('content', 'N/A')[:40]}"
                                    )
                                elif "entity" in item:
                                    e = item["entity"]
                                    self.logger.debug(
                                        f"    â”‚ æ­¥éª¤{step_idx} [Entity]: {e.get('name', 'N/A')[:30]} "
                                        f"| Hop={e.get('hop', 0)} "
                                        f"| ç±»å‹={e.get('type', 'N/A')}"
                                    )
                                elif "event" in item:
                                    ev = item["event"]
                                    self.logger.debug(
                                        f"    â”‚ æ­¥éª¤{step_idx} [Event]: {ev.get('title', 'N/A')[:40]}"
                                    )

                            self.logger.debug(f"    â””â”€ è·¯å¾„ {path_idx} ç»“æŸ")

                    if len(rerank_lines) > 3:
                        self.logger.debug(f"\n  ... è¿˜æœ‰ {len(rerank_lines) - 3} ä¸ªäº‹é¡¹æœªæ˜¾ç¤º")

                    self.logger.debug("=" * 80)

                # è¿”å›ç»™å‰ç«¯
                response["min_lines"] = path_result.get("min_lines", {})
                response["max_lines"] = path_result.get("max_lines", {})
                response["entitys"] = path_result.get("entitys", {})
                response["rerank_lines"] = path_result.get("rerank_lines", {})

            except Exception as e:
                self.logger.warning(f"âš ï¸ è·¯å¾„åˆ†æå¤±è´¥: {e}", exc_info=True)

        return response


# å‘åå…¼å®¹åˆ«å
EventSearcher = SAGSearcher

__all__ = [
    "SAGSearcher",
    "EventSearcher",
]
