"""
OpenAI LLMå®¢æˆ·ç«¯å®ç°

æ³¨æ„:
- æ”¯æŒæ ‡å‡†OpenAIæ¨¡å‹ (sophnet/Qwen3-30B-A3B-Thinking-2507, gpt-3.5-turboç­‰)
- æ”¯æŒæ€è€ƒæ¨¡å‹ (Thinking Models): æŸäº›æ¨¡å‹(å¦‚Qwen3-30B-A3B-Thinking)ä¼šå°†æ¨ç†è¿‡ç¨‹
  æ”¾åœ¨reasoning_contentå­—æ®µä¸­è€Œä¸æ˜¯contentå­—æ®µã€‚æœ¬å®ç°ä¼šè‡ªåŠ¨æ£€æµ‹å¹¶å¤„ç†è¿™ç§æƒ…å†µã€‚
"""

from typing import Any, AsyncIterator, Iterable, List, Optional, cast

from openai import (
    APIConnectionError,
    APIError,
    APITimeoutError,
    AsyncOpenAI,
    RateLimitError,
)
from openai.types.chat import ChatCompletionMessageParam

from dataflow.core.ai.base import BaseLLMClient
from dataflow.core.ai.models import ModelConfig, LLMMessage, LLMProvider, LLMResponse, LLMUsage
from dataflow.core.cache import llm_cache
from dataflow.exceptions import LLMError, LLMRateLimitError, LLMTimeoutError
from dataflow.utils import get_logger

logger = get_logger("ai.openai")


class OpenAIClient(BaseLLMClient):
    """OpenAIå®¢æˆ·ç«¯å®ç°"""

    def __init__(self, config: ModelConfig) -> None:
        """
        åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯

        Args:
            config: LLMé…ç½®
        """
        super().__init__(config)

        # åˆ›å»ºAsyncOpenAIå®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            api_key=config.api_key,
            base_url=config.base_url,
            timeout=config.timeout,
        )

    async def chat(
        self,
        messages: List[LLMMessage],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs: Any,
    ) -> LLMResponse:
        """
        OpenAIèŠå¤©è¡¥å…¨ï¼ˆå¸¦ç¼“å­˜ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLMå“åº”

        Raises:
            LLMError: è°ƒç”¨å¤±è´¥
            LLMTimeoutError: è°ƒç”¨è¶…æ—¶
            LLMRateLimitError: é€Ÿç‡é™åˆ¶
        """
        response, is_cached = await self._chat_with_cache(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs,
        )
        return response

    @llm_cache
    async def _chat_with_cache(
        self,
        messages: List[LLMMessage],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        **kwargs: Any,
    ) -> LLMResponse:
        """
        OpenAIèŠå¤©è¡¥å…¨ï¼ˆå†…éƒ¨å®ç°ï¼Œåº”ç”¨ç¼“å­˜è£…é¥°å™¨ï¼‰

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLMå“åº”

        Raises:
            LLMError: è°ƒç”¨å¤±è´¥
            LLMTimeoutError: è°ƒç”¨è¶…æ—¶
            LLMRateLimitError: é€Ÿç‡é™åˆ¶
        """
        try:
            # å‡†å¤‡æ¶ˆæ¯
            api_messages = self._prepare_messages(messages)

            # è®°å½•ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
            logger.info(
                "ğŸ¤– è°ƒç”¨ LLM - æ¨¡å‹: %s, base_url: %s, temperature: %.2f, max_tokens: %s, timeout: %s",
                self.config.model,
                self.config.base_url,
                temperature or self.config.temperature,
                max_tokens or self.config.max_tokens or "æœªè®¾ç½®",
                self.config.timeout,
            )

            # è®°å½•æ¶ˆæ¯å†…å®¹ï¼ˆè°ƒè¯•ç”¨ï¼‰
            # logger.info(
            #     "ğŸ“¤ LLM è¯·æ±‚æ¶ˆæ¯ (%dæ¡): %s",
            #     len(messages),
            #     [
            #         {
            #             "role": m.role,
            #             "content": m.content[:5000] + "..." if len(m.content) > 5000 else m.content,
            #         }
            #         for m in messages
            #     ],
            # )

            # ğŸ†• è¿‡æ»¤ä¸æ”¯æŒçš„å‚æ•°ï¼ˆé’ˆå¯¹æŸäº›APIå¦‚302.aiï¼‰
            # è®°å½•åŸå§‹kwargsç”¨äºè°ƒè¯•
            if kwargs:
                logger.debug(f"ğŸ” åŸå§‹kwargs: {list(kwargs.keys())}")

            # ç™½åå•ï¼šOpenAI SDKæ”¯æŒçš„é€šç”¨å‚æ•°
            allowed_params = {
                'top_p', 'frequency_penalty', 'presence_penalty',
                'stop', 'n', 'stream', 'logprobs', 'top_logprobs',
                'response_format', 'seed', 'tools', 'tool_choice',
                'user', 'timeout', 'extra_headers', 'extra_query',
                'extra_body', 'stream_options'
            }

            # è¿‡æ»¤kwargs,åªä¿ç•™ç™½åå•å‚æ•°
            filtered_kwargs = {
                k: v for k, v in kwargs.items()
                if k in allowed_params
            }

            # å¦‚æœæœ‰å‚æ•°è¢«è¿‡æ»¤,è®°å½•è­¦å‘Š
            removed = set(kwargs.keys()) - set(filtered_kwargs.keys())
            if removed:
                logger.warning(f"âš ï¸ ç§»é™¤ä¸æ”¯æŒçš„å‚æ•°: {removed}")

            # è°ƒç”¨APIï¼ˆä½¿ç”¨ cast æ˜¾å¼ç±»å‹è½¬æ¢ï¼‰
            response = await self.client.chat.completions.create(
                model=self.config.model,
                messages=cast(Iterable[ChatCompletionMessageParam], api_messages),
                temperature=temperature or self.config.temperature,
                max_tokens=max_tokens or self.config.max_tokens,  # ä»é…ç½®è¯»å–ï¼Œä¸ç¡¬ç¼–ç 
                **filtered_kwargs,  # ğŸ†• ä½¿ç”¨è¿‡æ»¤åçš„kwargs
            )

            # è§£æå“åº”
            choice = response.choices[0]
            usage = response.usage

            # å¤„ç†å“åº”å†…å®¹
            content = choice.message.content
            reasoning = getattr(choice.message, "reasoning_content", None)

            # logger.info(
            #     "OpenAIå“åº”: content=%s, reasoning_content=%s, finish_reason=%s",
            #     choice.message.content,
            #     reasoning,
            #     choice.finish_reason,
            # )

            return LLMResponse(
                content=content or "",
                model=response.model,
                usage=LLMUsage(
                    prompt_tokens=usage.prompt_tokens if usage else 0,
                    completion_tokens=usage.completion_tokens if usage else 0,
                    total_tokens=usage.total_tokens if usage else 0,
                ),
                finish_reason=choice.finish_reason or "stop",
            )

        except APITimeoutError as e:
            logger.error(
                "âŒ OpenAIè°ƒç”¨è¶…æ—¶ - æ¨¡å‹: %s, base_url: %s, timeout: %s, é”™è¯¯: %s",
                self.config.model,
                self.config.base_url,
                self.config.timeout,
                e,
            )
            raise LLMTimeoutError(f"OpenAIè°ƒç”¨è¶…æ—¶: {e}") from e
        except RateLimitError as e:
            logger.error(
                "âŒ OpenAIé€Ÿç‡é™åˆ¶ - æ¨¡å‹: %s, é”™è¯¯: %s",
                self.config.model,
                e,
            )
            raise LLMRateLimitError(f"OpenAIé€Ÿç‡é™åˆ¶: {e}") from e
        except (APIError, APIConnectionError) as e:
            logger.error(
                "âŒ OpenAIè°ƒç”¨å¤±è´¥ - æ¨¡å‹: %s, base_url: %s, é”™è¯¯: %s",
                self.config.model,
                self.config.base_url,
                e,
                exc_info=True,
            )
            raise LLMError(f"OpenAIè°ƒç”¨å¤±è´¥: {e}") from e
        except Exception as e:
            logger.error("æœªçŸ¥é”™è¯¯: %s", e, exc_info=True)
            raise LLMError(f"OpenAIè°ƒç”¨å¤±è´¥: {e}") from e

    async def chat_stream(
        self,
        messages: List[LLMMessage],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        include_reasoning: bool = False,
        **kwargs: Any,
    ) -> AsyncIterator[tuple[str, Optional[str]]]:
        """
        OpenAIæµå¼èŠå¤©è¡¥å…¨

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
            include_reasoning: æ˜¯å¦è¿”å›æ¨ç†å†…å®¹ï¼ˆreasoning_contentï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            å…ƒç»„ (content, reasoning) - contentä¸ºå†…å®¹ç‰‡æ®µï¼Œreasoningä¸ºæ¨ç†ç‰‡æ®µï¼ˆå¦‚æœæœ‰ï¼‰

        Raises:
            LLMError: è°ƒç”¨å¤±è´¥
        """
        try:
            # è®°å½•ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯ï¼ˆæ·»åŠ max_tokensï¼‰
            logger.info(
                "ğŸ¤– è°ƒç”¨æµå¼LLM - æ¨¡å‹: %s, base_url: %s, temperature: %.2f, max_tokens: %s, timeout: %s",
                self.config.model,
                self.config.base_url,
                temperature or self.config.temperature,
                max_tokens or self.config.max_tokens or "æœªè®¾ç½®",
                self.config.timeout,
            )

            # å‡†å¤‡æ¶ˆæ¯
            api_messages = self._prepare_messages(messages)

            # ğŸ†• è¿‡æ»¤ä¸æ”¯æŒçš„å‚æ•°ï¼ˆä¸æ™®é€šè°ƒç”¨ä¿æŒä¸€è‡´ï¼‰
            if kwargs:
                logger.debug(f"ğŸ” æµå¼è°ƒç”¨åŸå§‹kwargs: {list(kwargs.keys())}")

            # ç™½åå•ï¼šOpenAI SDKæ”¯æŒçš„é€šç”¨å‚æ•°
            allowed_params = {
                'top_p', 'frequency_penalty', 'presence_penalty',
                'stop', 'n', 'stream', 'logprobs', 'top_logprobs',
                'response_format', 'seed', 'tools', 'tool_choice',
                'user', 'timeout', 'extra_headers', 'extra_query',
                'extra_body', 'stream_options'
            }

            # è¿‡æ»¤kwargs
            filtered_kwargs = {
                k: v for k, v in kwargs.items()
                if k in allowed_params
            }

            # å¦‚æœæœ‰å‚æ•°è¢«è¿‡æ»¤,è®°å½•è­¦å‘Š
            removed = set(kwargs.keys()) - set(filtered_kwargs.keys())
            if removed:
                logger.warning(f"âš ï¸ æµå¼è°ƒç”¨ç§»é™¤ä¸æ”¯æŒçš„å‚æ•°: {removed}")

            # è°ƒç”¨æµå¼APIï¼ˆä½¿ç”¨ cast æ˜¾å¼ç±»å‹è½¬æ¢ï¼‰
            stream = await self.client.chat.completions.create(
                model=self.config.model,
                messages=cast(Iterable[ChatCompletionMessageParam], api_messages),
                temperature=temperature or self.config.temperature,
                max_tokens=max_tokens or self.config.max_tokens,
                stream=True,
                **filtered_kwargs,  # ğŸ†• ä½¿ç”¨è¿‡æ»¤åçš„kwargs
            )

            # é€ä¸ªç”Ÿæˆå†…å®¹ç‰‡æ®µ
            async for chunk in stream:
                if chunk.choices:
                    delta = chunk.choices[0].delta
                    content = delta.content if delta.content else None
                    reasoning = None

                    # å¦‚æœéœ€è¦æ¨ç†å†…å®¹ï¼Œå°è¯•è·å–reasoning_content
                    if include_reasoning:
                        reasoning = getattr(delta, "reasoning_content", None)

                    # åªåœ¨æœ‰å†…å®¹æˆ–æ¨ç†æ—¶yield
                    if content or reasoning:
                        yield (content or "", reasoning)

        except APITimeoutError as e:
            logger.error("OpenAIæµå¼è°ƒç”¨è¶…æ—¶: %s", e)
            raise LLMTimeoutError(f"OpenAIæµå¼è°ƒç”¨è¶…æ—¶: {e}") from e
        except (APIError, APIConnectionError) as e:
            logger.error("OpenAIæµå¼è°ƒç”¨å¤±è´¥: %s", e, exc_info=True)
            raise LLMError(f"OpenAIæµå¼è°ƒç”¨å¤±è´¥: {e}") from e
        except Exception as e:
            logger.error("æœªçŸ¥é”™è¯¯: %s", e, exc_info=True)
            raise LLMError(f"OpenAIæµå¼è°ƒç”¨å¤±è´¥: {e}") from e


async def create_openai_client(  # pylint: disable=too-many-arguments,too-many-positional-arguments
    api_key: str,
    model: Optional[str] = None,
    base_url: Optional[str] = None,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    timeout: Optional[int] = None,
    max_retries: Optional[int] = None,
) -> OpenAIClient:
    """
    åˆ›å»ºOpenAIå®¢æˆ·ç«¯ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–é»˜è®¤å€¼ï¼‰

    Args:
        api_key: APIå¯†é’¥
        model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        base_url: åŸºç¡€URLï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        temperature: æ¸©åº¦å‚æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        max_tokens: æœ€å¤§è¾“å‡ºtokenæ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰

    Returns:
        OpenAIå®¢æˆ·ç«¯å®ä¾‹
    """
    from dataflow.core.config.settings import get_settings

    settings = get_settings()

    config = ModelConfig(
        provider=LLMProvider.OPENAI,
        model=model or settings.llm_model,
        api_key=api_key,
        base_url=base_url or settings.llm_base_url,
        temperature=temperature or settings.llm_temperature,
        max_tokens=max_tokens or settings.llm_max_tokens,
        top_p=settings.llm_top_p,
        frequency_penalty=settings.llm_frequency_penalty,
        presence_penalty=settings.llm_presence_penalty,
        timeout=timeout or settings.llm_timeout,
        max_retries=max_retries or settings.llm_max_retries,
    )

    return OpenAIClient(config)
