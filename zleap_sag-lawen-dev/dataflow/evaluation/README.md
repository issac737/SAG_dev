# DataFlow Evaluation æ¨¡å—

å®Œæ•´çš„è¯„ä¼°æ¡†æ¶ï¼Œæ”¯æŒæ£€ç´¢ç³»ç»Ÿå’ŒQAç³»ç»Ÿçš„æ€§èƒ½è¯„ä¼°ã€‚

## ğŸ“¦ æ¨¡å—ç»“æ„

```
dataflow/evaluation/
â”œâ”€â”€ benchmark.py                # â­ Evaluate ç±»ï¼ˆä¸»è¯„ä¼°æ¡†æ¶ï¼‰
â”œâ”€â”€ dataset/                    # æ•°æ®é›†ç›®å½•
â”œâ”€â”€ metrics/                    # è¯„ä¼°æŒ‡æ ‡
â”‚   â”œâ”€â”€ base.py                 # BaseMetricï¼ˆè¯„ä¼°æŒ‡æ ‡åŸºç±»ï¼‰
â”‚   â”œâ”€â”€ qa_eval.py              # QAè¯„ä¼°æŒ‡æ ‡ï¼ˆEM, F1ï¼‰
â”‚   â””â”€â”€ retrieval_eval.py       # æ£€ç´¢è¯„ä¼°æŒ‡æ ‡ï¼ˆRecall@kï¼‰
â”œâ”€â”€ utils/                      # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ load_utils.py          # â­ DatasetLoaderï¼ˆæ•°æ®é›†åŠ è½½ï¼‰
â”‚   â””â”€â”€ ...
â”œâ”€â”€ examples/                   # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ dataset_loader_example.py
â”‚   â””â”€â”€ evaluate_example.py
â”œâ”€â”€ test_evaluate.py            # é›†æˆæµ‹è¯•
â””â”€â”€ README.md                   # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ•°æ®é›†åŠ è½½ï¼ˆè°ƒç”¨ load_utilsï¼‰

```python
from dataflow.evaluation import DatasetLoader

# åˆ›å»ºåŠ è½½å™¨
loader = DatasetLoader('musique')

# è·å–æ•°æ®
docs = loader.get_docs()              # 11,656 ä¸ªæ–‡æ¡£
questions = loader.get_questions()    # 1,000 ä¸ªé—®é¢˜
gold_answers = loader.get_gold_answers()  # æ ‡å‡†ç­”æ¡ˆ
gold_docs = loader.get_gold_docs()    # æ”¯æŒæ–‡æ¡£

# è·å–ç»Ÿè®¡ä¿¡æ¯
stats = loader.get_stats()
print(stats)
```

**æ”¯æŒçš„æ•°æ®é›†**ï¼š
- âœ… MuSiQue (11,656 docs, 1,000 questions)
- âœ… HotpotQA (9,811 docs, 1,000 questions)
- âœ… 2WikiMultihopQA (6,119 docs, 1,000 questions)

### 2. å®Œæ•´è¯„ä¼°ï¼ˆEvaluate ç±»ï¼‰

```python
from dataflow.evaluation import Evaluate, EvaluationConfig

# é…ç½®è¯„ä¼°
config = EvaluationConfig(
    dataset_name='musique',
    max_samples=100,
    evaluate_retrieval=True,
    evaluate_qa=True,
    save_results=True
)

# åˆ›å»ºè¯„ä¼°å™¨
evaluator = Evaluate(config)

# åŠ è½½æ•°æ®é›†
evaluator.load_dataset()

# è¿è¡Œä½ çš„ç³»ç»Ÿï¼ˆç¤ºä¾‹ï¼‰
questions = evaluator.get_questions()
retrieved_docs_list = your_retrieval_system(questions)
predicted_answers = your_qa_system(questions, retrieved_docs_list)

# è¯„ä¼°
results = evaluator.evaluate_all(
    retrieved_docs_list=retrieved_docs_list,
    predicted_answers=predicted_answers
)

# æŸ¥çœ‹ç»“æœ
evaluator.print_summary(results)
```

### 3. ä¾¿æ·å‡½æ•°

```python
from dataflow.evaluation import quick_evaluate

# ä¸€è¡Œä»£ç å®Œæˆè¯„ä¼°
results = quick_evaluate(
    dataset_name='musique',
    retrieved_docs_list=my_results,
    predicted_answers=my_predictions
)
```

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

### æ£€ç´¢è¯„ä¼°

- **Recall@k** - å‰kä¸ªæ£€ç´¢ç»“æœä¸­åŒ…å«çš„ç›¸å…³æ–‡æ¡£æ¯”ä¾‹

### QAè¯„ä¼°

- **Exact Match (EM)** - ç²¾ç¡®åŒ¹é…ç‡
- **F1 Score** - Tokençº§åˆ«çš„F1åˆ†æ•°

## ğŸ“ˆ è¯„ä¼°ç»“æœç¤ºä¾‹

```json
{
  "dataset": "musique",
  "timestamp": "2025-12-19T14:53:28",
  "num_questions": 20,
  "retrieval": {
    "pooled": {
      "Recall@1": 0.0417,
      "Recall@5": 0.4375,
      "Recall@10": 0.9500,
      "Recall@20": 0.9500
    }
  },
  "qa": {
    "pooled": {
      "ExactMatch": 1.0000,
      "F1": 1.0000
    }
  }
}
```

## ğŸ§ª è¿è¡Œç¤ºä¾‹

```bash
# æ•°æ®é›†åŠ è½½ç¤ºä¾‹
python dataflow/evaluation/examples/dataset_loader_example.py

# è¯„ä¼°ç¤ºä¾‹
python dataflow/evaluation/examples/evaluate_example.py

# é›†æˆæµ‹è¯•
python dataflow/evaluation/test_evaluate.py
```

## ğŸ“š è¯¦ç»†æ–‡æ¡£

- **EVALè¯„ä¼°æ¡†æ¶** - å®Œæ•´çš„è¯„ä¼°ç±»æ–‡æ¡£ ([EVALUATE_README.md](./EVALUATE_README.md))
- **æ•°æ®é›†åŠ è½½å™¨** - DatasetLoader å®Œæ•´æ–‡æ¡£ ([utils/DATASET_LOADER_README.md](./utils/DATASET_LOADER_README.md))

## ğŸ“ å®Œæ•´æµç¨‹ç¤ºä¾‹

```python
from dataflow.evaluation import Evaluate, EvaluationConfig

# 1. é…ç½®
config = EvaluationConfig(
    dataset_name='musique',
    max_samples=100,
    evaluate_retrieval=True,
    evaluate_qa=True,
    retrieval_top_k_list=[1,5,10,20],
    save_results=True
)

# 2. åˆ›å»ºè¯„ä¼°å™¨
evaluator = Evaluate(config)

# 3. åŠ è½½æ•°æ®
dataset_info = evaluator.load_dataset()
print(f"Loaded {dataset_info['num_questions']} questions")

# 4. è·å–é—®é¢˜
questions = evaluator.get_questions()

# 5. è¿è¡Œä½ çš„ç³»ç»Ÿ
retrieved = my_system.retrieve(questions)
answers = my_system.answer(questions, retrieved)

# 6. è¯„ä¼°
results = evaluator.evaluate_all(
    retrieved_docs_list=retrieved,
    predicted_answers=answers
)

# 7. æŸ¥çœ‹æ‘˜è¦
evaluator.print_summary(results)
```

## ğŸ¯ ä¸»è¦ç‰¹æ€§

âœ… **å®Œæ•´çš„æ•°æ®é›†åŠ è½½** - è‡ªåŠ¨åŠ è½½ corpusã€questionsã€gold_answersã€gold_docs
âœ… **çµæ´»çš„è¯„ä¼°é…ç½®** - å¯é…ç½®è¯„ä¼°ç±»å‹ã€top-k åˆ—è¡¨ã€é‡‡æ ·ç­‰
âœ… **æ ‡å‡†åŒ–æŒ‡æ ‡** - Recall@k, EM, F1
âœ… **å¤šæ•°æ®é›†æ”¯æŒ** - MuSiQue, HotpotQA, 2WikiMultihopQA
âœ… **ç»“æœè‡ªåŠ¨ä¿å­˜** - JSONæ ¼å¼ï¼Œå¸¦æ—¶é—´æˆ³
âœ… **ç®€æ´çš„API** - æ˜“äºä½¿ç”¨å’Œé›†æˆ

## ğŸ”— å…³é”®æ–¹æ³•è¯´æ˜

### Evaluate ç±»æ ¸å¿ƒæ–¹æ³•

| æ–¹æ³• | ç”¨é€” |
|------|------|
| `load_dataset()` | åŠ è½½æ•°æ®é›† |
| `evaluate_retrieval()` | è¯„ä¼°æ£€ç´¢æ€§èƒ½ï¼ˆRecall@kï¼‰ |
| `evaluate_qa()` | è¯„ä¼°QAæ€§èƒ½ï¼ˆEM, F1ï¼‰ |
| `evaluate_all()` | è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹ |
| `print_summary()` | æ‰“å°è¯„ä¼°æ‘˜è¦ |

### DatasetLoader ç±»æ ¸å¿ƒæ–¹æ³•

| æ–¹æ³• | è¿”å› |
|------|------|
| `get_docs()` | æ ¼å¼åŒ–æ–‡æ¡£åˆ—è¡¨ ["title\ntext"] |
| `get_questions()` | é—®é¢˜åˆ—è¡¨ |
| `get_gold_answers()` | æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨ï¼ˆé›†åˆï¼‰ |
| `get_gold_docs()` | æ”¯æŒæ–‡æ¡£åˆ—è¡¨ |
| `get_stats()` | æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ |

## ğŸ‰ å¼€å§‹ä½¿ç”¨

æ‰€æœ‰ä»£ç å·²å®Œæˆå¹¶ç»è¿‡æµ‹è¯•ï¼Œå¯ä»¥ç«‹å³ä½¿ç”¨ï¼

1. æŸ¥çœ‹ç¤ºä¾‹ï¼š`examples/evaluate_example.py`
2. æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£ï¼š`EVALUATE_README.md`
3. è¿è¡Œæµ‹è¯•ï¼š`test_evaluate.py`
