"""
è¯„ä¼°åŸºå‡†æµ‹è¯•æ¨¡å—

æä¾›æ•°æ®é›†è¯„ä¼°ã€æ£€ç´¢è¯„ä¼°å’ŒQAè¯„ä¼°åŠŸèƒ½

ä½¿ç”¨æ–¹æ³•ï¼š

1. æŒ‡å®šåŠŸèƒ½æ¨¡å¼ï¼ˆå¿…éœ€ï¼‰:
   ä½¿ç”¨ --foundation å‚æ•°æŒ‡å®šæ ¸å¿ƒåŠŸèƒ½æ¨¡å¼:

   a) Upload æ¨¡å¼ - åŠ è½½æ•°æ®é›†å¹¶ä¸Šä¼ åˆ°ç³»ç»Ÿ:
      python dataflow/evaluation/benchmark.py \
          --foundation upload \
          --dataset test_hotpotqa

      å¯é€‰å‚æ•°:
      --chunks-per-file 500  # æ¯ä¸ªæ–‡ä»¶çš„ç‰‡æ®µæ•°ï¼ˆé»˜è®¤: 500ï¼‰

   b) Search æ¨¡å¼ - æ‰§è¡Œæ£€ç´¢å¹¶è¯„ä¼°å¬å›ç‡:
      python dataflow/evaluation/benchmark.py \
          --foundation search \
          --dataset test_hotpotqa \
          --info-limit 5 \
          --search-verbose

      å¯é€‰å‚æ•°:
      --info-limit N         # é™åˆ¶é—®é¢˜æ•°é‡ï¼ˆå¯é€‰ï¼‰
      --search-verbose       # æ˜¾ç¤ºè¯¦ç»†æ£€ç´¢è¿‡ç¨‹
      --enable-qa            # å¯ç”¨ QA è¯„ä¼°
      --no-paragraphs        # éšè—æ®µè½è¯¦ç»†ä¿¡æ¯

   c) Badcase Zero æ¨¡å¼ - é‡æµ‹ Zero Recall Badcase:
      python dataflow/evaluation/benchmark.py \
          --foundation badcase_zero \
          --dataset test_hotpotqa \
          --info-limit 5

   d) Badcase Partial æ¨¡å¼ - é‡æµ‹ Partial Recall Badcase:
      python dataflow/evaluation/benchmark.py \
          --foundation badcase_partial \
          --dataset test_hotpotqa \
          --info-limit 5

2. æ•°æ®é›†é€‰æ‹©ï¼ˆå¿…éœ€ï¼‰:
   --dataset DATASET_NAME   # æ•°æ®é›†åç§°ï¼ˆæ”¯æŒ: musique, hotpotqa, 2wikimultihopqa, test_hotpotqa, sampleï¼‰

3. æ¨¡å‹é…ç½®ï¼ˆé€šè¿‡ .env æ–‡ä»¶ï¼‰:
   ç³»ç»Ÿä¼šè‡ªåŠ¨è¯»å– .env æ–‡ä»¶ä¸­çš„ LLM_MODEL é…ç½®:
   - gpt-4o, gpt-4o-mini (OpenAI)
   - Qwen/qwen3, qwen-max (é˜¿é‡Œäº‘)
   - claude-3-5-sonnet-20241022 (Anthropic)

   æ¨¡å‹åç§°ä¼šè‡ªåŠ¨è¿‡æ»¤ä¸ºç®€å†™å½¢å¼ï¼ˆå¦‚ Qwen/qwen3 â†’ qwen3ï¼‰ç”¨äºç›®å½•å‘½åã€‚

4. è¾“å‡ºç»“æœ:
   - ä¿¡æ¯æºæ–‡ä»¶: dataflow/evaluation/source/SAG/{model_name}/{dataset_name}/{timestamp}/
   - Badcase æ–‡ä»¶: dataflow/evaluation/outputs/SAG/{model_name}/{dataset_name}/{timestamp}/
   - è¯„ä¼°ç»“æœ: outputs/evaluation/ï¼ˆå¦‚æœä½¿ç”¨ API æ–¹å¼ï¼‰

å®Œæ•´ç¤ºä¾‹:

# æ­¥éª¤1: ä¸Šä¼ æ•°æ®é›†ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
python dataflow/evaluation/benchmark.py \
    --foundation upload \
    --dataset test_hotpotqa

# æ­¥éª¤2: æµ‹è¯•æ£€ç´¢æ€§èƒ½
python dataflow/evaluation/benchmark.py \
    --foundation search \
    --dataset test_hotpotqa \
    --info-limit 10 \
    --search-verbose \
    --enable-qa

# æ­¥éª¤3: åˆ†æ badcaseï¼ˆå¦‚æœæœ‰ï¼‰
python dataflow/evaluation/benchmark.py \
    --foundation badcase_zero \
    --dataset test_hotpotqa \
    --info-limit 5

æ³¨æ„:
- Badcase æ–‡ä»¶ä» dataflow/evaluation/outputs/SAG/{model_name}/{dataset_name}/{timestamp}/ è‡ªåŠ¨åŠ è½½
- Badcase æ¨¡å¼ä¸‹ä¸æ”¯æŒ QA è¯„ä¼°ï¼ˆä¼šè‡ªåŠ¨ç¦ç”¨ï¼‰
- ä½¿ç”¨æœ€æ–°çš„æ—¶é—´æˆ³ç›®å½•
- å‘åå…¼å®¹æ—§çš„å‚æ•°è¯­æ³•
"""

import json
import sys
import time
import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Callable
from datetime import datetime
from dataclasses import dataclass, field, asdict

from dataflow.utils import get_logger
from dataflow.evaluation.utils import DatasetLoader
from dataflow.evaluation.metrics import (
    QAExactMatch,
    QAF1Score,
    RetrievalRecall,
)
from dataflow import DataFlowEngine, ExtractBaseConfig
from dataflow.modules.load.config import DocumentLoadConfig
from dataflow.db import close_database
from dataflow.engine.config import TaskConfig

# æœç´¢ç›¸å…³å¯¼å…¥
from dataflow.modules.search import SAGSearcher, SearchConfig
from dataflow.modules.search.config import (
    ReturnType, RecallConfig, ExpandConfig, RerankConfig, RecallMode
)
from dataflow.core.prompt.manager import PromptManager
from dataflow.core.storage.elasticsearch import close_es_client
from dataflow.core.ai.factory import create_llm_client
from dataflow.core.ai.base import BaseLLMClient
from dataflow.core.ai.models import LLMMessage, LLMRole

# Tokenè¿½è¸ªå™¨ï¼ˆæœ€å°åŒ–ç‰ˆæœ¬ï¼‰
class LLMTokenTracker:
    """è¿½è¸ªLLMè°ƒç”¨çš„tokenæ¶ˆè€—"""
    def __init__(self):
        self.total = {"prompt": 0, "completion": 0, "total": 0}
        self.stages = {}

    def record(self, stage: str, usage):
        """è®°å½•token"""
        if not usage:
            return
        prompt = getattr(usage, 'prompt_tokens', 0)
        completion = getattr(usage, 'completion_tokens', 0)
        total = getattr(usage, 'total_tokens', 0)

        self.total["prompt"] += prompt
        self.total["completion"] += completion
        self.total["total"] += total

        if stage not in self.stages:
            self.stages[stage] = {"calls": 0, "prompt": 0, "completion": 0, "total": 0}

        self.stages[stage]["calls"] += 1
        self.stages[stage]["prompt"] += prompt
        self.stages[stage]["completion"] += completion
        self.stages[stage]["total"] += total

    def get_summary(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡"""
        return {
            "total_prompt": self.total["prompt"],
            "total_completion": self.total["completion"],
            "total_tokens": self.total["total"],
            "total_calls": sum(s["calls"] for s in self.stages.values()),
            "stages": self.stages
        }


def enable_llm_tracking(token_tracker: LLMTokenTracker):
    """å¯ç”¨LLMè°ƒç”¨è¿½è¸ªï¼ˆå…¼å®¹æ‰€æœ‰é˜¶æ®µï¼šLOAD, EXTRACT, SEARCH, QAï¼‰"""
    from dataflow.core.ai import llm
    original_chat = llm.OpenAIClient.chat

    async def tracked_chat(self, messages, **kwargs):
        result = await original_chat(self, messages, **kwargs)
        import inspect
        frame = inspect.currentframe()
        stage = "UNKNOWN"

        try:
            # å‘ä¸ŠæŸ¥æ‰¾è°ƒç”¨æ ˆï¼Œæœ€å¤šæŸ¥æ‰¾30å±‚
            for _ in range(30):
                if not frame:
                    break
                frame = frame.f_back
                if not frame:
                    break

                # æ£€æŸ¥å‡½æ•°åï¼ˆç”¨äºQAé˜¶æ®µï¼‰
                func_name = frame.f_code.co_name if frame.f_code else ""
                if func_name == "show_retrieval_info":
                    stage = "QA"
                    break

                # æ£€æŸ¥ç±»åï¼ˆç”¨äºLOAD, EXTRACT, SEARCHé˜¶æ®µï¼‰
                if 'self' in frame.f_locals:
                    obj = frame.f_locals['self']
                    class_name = obj.__class__.__name__

                    # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„é˜¶æ®µ
                    if 'DocumentLoader' in class_name or 'DocumentProcessor' in class_name:
                        stage = "LOAD"
                        break
                    elif 'EventProcessor' in class_name or 'EventExtractor' in class_name:
                        stage = "EXTRACT"
                        break
                    elif 'RecallSearcher' in class_name:
                        stage = "SEARCH"
                        break
        except:
            pass
        finally:
            del frame

        if hasattr(result, 'usage') and result.usage:
            token_tracker.record(stage, result.usage)

        return result

    llm.OpenAIClient.chat = tracked_chat


logger = get_logger("evaluation.benchmark")


@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®"""

    # æ•°æ®é›†é…ç½®
    dataset_name: str = "musique"
    dataset_dir: Optional[str] = None

    # è¯„ä¼°ç±»å‹
    evaluate_retrieval: bool = True
    evaluate_qa: bool = True

    # æ£€ç´¢è¯„ä¼°é…ç½®
    retrieval_top_k_list: List[int] = field(default_factory=lambda: [1, 5, 10, 20])

    # QAè¯„ä¼°é…ç½®
    qa_aggregation: str = "max"  # max, mean, etc.

    # è¾“å‡ºé…ç½®
    save_results: bool = True
    output_dir: str = "./outputs/SAG"
    save_predictions: bool = True
    verbose: bool = True

    # é‡‡æ ·é…ç½®ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    max_samples: Optional[int] = None  # Noneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ ·æœ¬


class Evaluate:
    """
    è¯„ä¼°ç±»

    æä¾›å®Œæ•´çš„æ•°æ®é›†è¯„ä¼°åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ£€ç´¢è¯„ä¼°å’ŒQAè¯„ä¼°
    """

    # ç±»çº§åˆ«çš„ LLM å®¢æˆ·ç«¯ç¼“å­˜
    _llm_client: Optional[BaseLLMClient] = None
    _prompt_manager: Optional[PromptManager] = None

    def __init__(self, config: Optional[EvaluationConfig] = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            config: è¯„ä¼°é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or EvaluationConfig()

        # æ•°æ®é›†åŠ è½½å™¨
        self.dataset_loader: Optional[DatasetLoader] = None

        # æ•°æ®ç¼“å­˜
        self.docs: Optional[List[str]] = None
        self.questions: Optional[List[str]] = None
        self.gold_answers: Optional[List[Set[str]]] = None
        self.gold_docs: Optional[List[List[str]]] = None

        # è¯„ä¼°æŒ‡æ ‡
        self.qa_em_metric = QAExactMatch()
        self.qa_f1_metric = QAF1Score()
        self.retrieval_recall_metric = RetrievalRecall()

        # è¾“å‡ºç›®å½•
        self.output_dir = Path(self.config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"Initialized Evaluate with config: {asdict(self.config)}")

    @classmethod
    def load_badcase_questions(cls, dataset_name: str, badcase_type: str) -> Optional[List[str]]:
        """
        ä» badcase æ–‡ä»¶ä¸­åŠ è½½é—®é¢˜åˆ—è¡¨ï¼ˆç”¨äºé‡æµ‹ badcaseï¼‰

        Args:
            dataset_name: æ•°æ®é›†åç§°
            badcase_type: badcase ç±»å‹ ('zero' æˆ– 'partial')

        Returns:
            é—®é¢˜åˆ—è¡¨ï¼Œå¦‚æœåŠ è½½å¤±è´¥è¿”å› None
        """
        import json
        from pathlib import Path

        try:
            # è·å–å½“å‰æ¨¡å‹åç§°
            from dataflow.core.config import get_settings
            settings = get_settings()
            model_name = settings.llm_model
            if '/' in model_name:
                filtered_model_name = model_name.split('/')[-1]
            else:
                filtered_model_name = model_name

            # æ„å»º badcase æ–‡ä»¶è·¯å¾„
            # SAG/{model_name}/{dataset_name}/
            badcase_base_dir = Path(__file__).parent / "outputs" / "SAG" / filtered_model_name / dataset_name

            if not badcase_base_dir.exists():
                logger.warning(f"Badcase ç›®å½•ä¸å­˜åœ¨: {badcase_base_dir}")
                return None

            # æŸ¥æ‰¾æœ€æ–°çš„æ—¶é—´æˆ³ç›®å½•
            timestamp_dirs = [d for d in badcase_base_dir.iterdir() if d.is_dir()]
            if not timestamp_dirs:
                logger.warning(f"æ²¡æœ‰æ‰¾åˆ°æ—¶é—´æˆ³ç›®å½•: {badcase_base_dir}")
                return None

            # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè·å–æœ€æ–°çš„
            latest_timestamp_dir = max(timestamp_dirs, key=lambda d: d.name)

            # æ„å»º badcase æ–‡ä»¶å
            badcase_file = latest_timestamp_dir / f"{badcase_type}_{dataset_name}.json"

            if not badcase_file.exists():
                logger.warning(f"Badcase æ–‡ä»¶ä¸å­˜åœ¨: {badcase_file}")
                return None

            logger.info(f"åŠ è½½ {badcase_type} badcase æ–‡ä»¶: {badcase_file}")

            # åŠ è½½ badcase æ•°æ®
            with open(badcase_file, 'r', encoding='utf-8') as f:
                badcase_data = json.load(f)

            # æå–é—®é¢˜åˆ—è¡¨
            questions = [case['question'] for case in badcase_data]

            logger.info(f"æˆåŠŸåŠ è½½ {len(questions)} ä¸ª {badcase_type} badcase é—®é¢˜")

            return questions

        except Exception as e:
            logger.warning(f"åŠ è½½ badcase æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            logger.warning(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return None

    @classmethod
    def load_latest_source_info(cls, dataset_name: str) -> Dict[str, Any]:
        """
        ä» dataflow/evaluation/source/SAG/{model_name}/{dataset_name}/{timestamp}/ è·¯å¾„ä¸‹åŠ è½½æœ€æ–°æ—¶é—´æˆ³æ–‡ä»¶å¤¹çš„ source_info.json

        æ”¯æŒæ–°çš„è·¯å¾„ç»“æ„ï¼šSAG/{model_name}/{dataset_name}/{timestamp}/
        å‘ä¸‹å…¼å®¹æ—§çš„è·¯å¾„ç»“æ„ï¼šSAG/{dataset_name}/{timestamp}/

        è‡ªåŠ¨ä»é…ç½®ä¸­è·å–å½“å‰æ¨¡å‹åç§°ï¼Œä¼˜å…ˆä½¿ç”¨è¯¥æ¨¡å‹çš„æ•°æ®é›†ã€‚

        Args:
            dataset_name: æ•°æ®é›†åç§°

        Returns:
            åŒ…å« source_config_id å’Œ dataset_name çš„å­—å…¸
        """
        import json
        from pathlib import Path

        # ä»é…ç½®è·å–å½“å‰æ¨¡å‹åç§°ï¼ˆä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ä¸­ï¼‰
        try:
            from dataflow.core.config import get_settings
            settings = get_settings()

            # è·å–æ¨¡å‹åç§°ï¼ˆä» LLM é…ç½®ä¸­ï¼‰
            model_name = settings.llm_model

            # è¿‡æ»¤æ¨¡å‹åç§°ï¼Œåªä¿ç•™æœ€åä¸€å±‚ï¼ˆä¾‹å¦‚ï¼šQwen/qwen3 -> qwen3ï¼‰
            if '/' in model_name:
                filtered_model_name = model_name.split('/')[-1]
            else:
                filtered_model_name = model_name

            logger.info(f"ä»é…ç½®è·å–æ¨¡å‹åç§°: {model_name} -> è¿‡æ»¤å: {filtered_model_name}")
        except Exception as e:
            logger.warning(f"è·å–æ¨¡å‹åç§°å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨è‡ªåŠ¨æ£€æµ‹æ¨¡å¼")
            filtered_model_name = None

        # è·å– SAG ç›®å½•è·¯å¾„
        current_file = Path(__file__)
        sag_base_dir = current_file.parent / "source" / "SAG"

        if not sag_base_dir.exists():
            raise FileNotFoundError(f"SAG directory not found: {sag_base_dir}")

        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ source_info.json æ–‡ä»¶
        all_source_info_files = []

        # å¦‚æœè·å–åˆ°æ¨¡å‹åç§°ï¼Œä¼˜å…ˆä½¿ç”¨è¯¥æ¨¡å‹çš„æ•°æ®é›†
        if filtered_model_name:
            model_dataset_dir = sag_base_dir / filtered_model_name / dataset_name
            if model_dataset_dir.exists():
                timestamp_dirs = [d for d in model_dataset_dir.iterdir() if d.is_dir()]
                for timestamp_dir in timestamp_dirs:
                    source_info_path = timestamp_dir / "source_info.json"
                    if source_info_path.exists():
                        all_source_info_files.append(source_info_path)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼ˆæˆ–æ²¡æœ‰æŒ‡å®šæ¨¡å‹ï¼‰ï¼Œåˆ™éå†æ‰€æœ‰ model_name ç›®å½•
        if not all_source_info_files:
            for model_dir in sag_base_dir.iterdir():
                if model_dir.is_dir():
                    dataset_dir = model_dir / dataset_name
                    if dataset_dir.exists():
                        # è·å–è¯¥æ•°æ®é›†ä¸‹çš„æ‰€æœ‰æ—¶é—´æˆ³æ–‡ä»¶å¤¹
                        timestamp_dirs = [d for d in dataset_dir.iterdir() if d.is_dir()]
                        for timestamp_dir in timestamp_dirs:
                            source_info_path = timestamp_dir / "source_info.json"
                            if source_info_path.exists():
                                all_source_info_files.append(source_info_path)

        # å‘ä¸‹å…¼å®¹æ—§çš„è·¯å¾„ç»“æ„
        old_dataset_dir = sag_base_dir / dataset_name
        if old_dataset_dir.exists():
            timestamp_dirs = [d for d in old_dataset_dir.iterdir() if d.is_dir()]
            for timestamp_dir in timestamp_dirs:
                source_info_path = timestamp_dir / "source_info.json"
                if source_info_path.exists():
                    all_source_info_files.append(source_info_path)

        if not all_source_info_files:
            raise FileNotFoundError(f"No source_info.json found for dataset: {dataset_name}")

        # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
        latest_source_info_path = max(all_source_info_files, key=lambda p: p.stat().st_mtime)

        # æå–æ¨¡å‹åç§°ã€æ•°æ®é›†åç§°ç­‰ä¿¡æ¯
        path_parts = latest_source_info_path.parts
        model_part = None

        # æ£€æµ‹è·¯å¾„ç»“æ„ï¼šå°è¯•æ‰¾åˆ° model ä½ç½®
        try:
            # æŸ¥æ‰¾è·¯å¾„ä¸­çš„ model_name éƒ¨åˆ†
            if 'SAG' in path_parts:
                sag_index = path_parts.index('SAG')
                if len(path_parts) > sag_index + 3:
                    model_part = path_parts[sag_index + 1]  # model_name
                    dataset_part = path_parts[sag_index + 2]  # dataset_name
        except:
            pass

        logger.info(f"Loading source info from: {latest_source_info_path}")
        if model_part and model_part != dataset_name:
            logger.info(f"ä½¿ç”¨æ¨¡å‹: {model_part}")

        with open(latest_source_info_path, 'r', encoding='utf-8') as f:
            source_info = json.load(f)

        return {
            'source_config_id': source_info.get('source_config_id'),
            'dataset_name': source_info.get('dataset_name'),
            'timestamp': source_info.get('timestamp'),
            'source_name': source_info.get('source_name'),
            'model_name': model_part if model_part and model_part != dataset_name else None,
            'file_path': str(latest_source_info_path)
        }
    
    @classmethod
    def load_dataset_info(cls, dataset_name: str) -> Dict[str, Any]:
        """
        ä» dataflow/evaluation/dataset ç›®å½•åŠ è½½æŒ‡å®šæ•°æ®é›†çš„ä¿¡æ¯
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
        
        Returns:
            åŒ…å« questions, answers, paragraphs ä¿¡æ¯çš„å­—å…¸
        """
        from dataflow.evaluation.utils import DatasetLoader
        
        # ä½¿ç”¨ DatasetLoader åŠ è½½æ•°æ®é›†
        loader = DatasetLoader(dataset_name)
        
        # åŠ è½½åŸå§‹æ ·æœ¬æ•°æ®
        samples = loader.load_samples()
        
        questions = []
        answers = []
        all_paragraphs = []
        
        for sample in samples:
            questions.append(sample.get('question', ''))
            answers.append(sample.get('answer', []))
            all_paragraphs.append(sample.get('paragraphs', []))
        
        return {
            'dataset_name': dataset_name,
            'total_questions': len(questions),
            'questions': questions,
            'answers': answers,
            'paragraphs': all_paragraphs,
            'samples': samples
        }
    
    @classmethod
    async def search_questions(cls, source_config_id: str, questions: List[str], limit: Optional[int] = None, verbose: bool = False, bench_size: Optional[int] = None, callback: Optional[Callable] = None) -> List[Dict[str, Any]]:
        """
        å¯¹é—®é¢˜åˆ—è¡¨è¿›è¡Œæ£€ç´¢ï¼Œè¿”å›æ£€ç´¢ç»“æœ

        Args:
            source_config_id: æ•°æ®æºID
            questions: é—®é¢˜åˆ—è¡¨
            limit: é™åˆ¶å¤„ç†çš„é—®é¢˜æ•°é‡
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            bench_size: æ¯Nä¸ªé—®é¢˜æ‰§è¡Œä¸€æ¬¡å›è°ƒ
            callback: å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (current_idx, total, results_so_far)

        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        import logging

        # è·å– logger
        logger = get_logger('dataflow.evaluation.search')

        logger.info("Initializing searcher...")
        
        # é…ç½®æ—¥å¿—çº§åˆ«
        if verbose:
            logger.info("å¯ç”¨è¯¦ç»†æ—¥å¿—æ¨¡å¼...")

            # åˆ›å»ºæ§åˆ¶å°handler
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(formatter)

            # è®¾ç½®å„ä¸ªæ¨¡å—çš„æ—¥å¿—çº§åˆ«ï¼ˆæ’é™¤ evaluationï¼Œé¿å…å½±å“ benchmark è¾“å‡ºï¼‰
            loggers_to_configure = [
                'dataflow.modules.search',
                'dataflow.modules.search.recall',
                'dataflow.modules.search.expand',
                'dataflow.modules.search.rerank',
                'dataflow.search.rerank.pagerank',
                'dataflow.search.pagerank'
            ]

            for logger_name in loggers_to_configure:
                logger_obj = logging.getLogger(logger_name)
                logger_obj.setLevel(logging.INFO)
                # æ¸…é™¤ç°æœ‰handlersé¿å…é‡å¤
                logger_obj.handlers = []
                logger_obj.addHandler(console_handler)
                logger_obj.propagate = False
            # é…ç½®æ ¹loggerä»¥ç¡®ä¿æ‰€æœ‰æ—¥å¿—éƒ½èƒ½è¾“å‡º
            root_logger = logging.getLogger()
            root_logger.setLevel(logging.INFO)
            if not root_logger.handlers:
                root_logger.addHandler(console_handler)

        else:
            logger.info("ä½¿ç”¨é»˜è®¤æ—¥å¿—çº§åˆ«(ERROR)...")
            # åªæ˜¾ç¤ºWARNINGçº§åˆ«çš„æ—¥å¿—ï¼ˆæ’é™¤ evaluationï¼‰
            for logger_name in ['dataflow.modules.search', 'elasticsearch', 'urllib3']:
                logging.getLogger(logger_name).setLevel(logging.ERROR)
        
        # åˆå§‹åŒ–æœç´¢å™¨
        prompt_manager = PromptManager()
        searcher = SAGSearcher(prompt_manager=prompt_manager)
        
        # åº”ç”¨é™åˆ¶
        process_questions = questions[:limit] if limit else questions
        logger.info(f"Processing {len(process_questions)} questions for search")
        
        search_results = []

        for i, question in enumerate(process_questions, 1):
            if verbose:
                logger.info(f"\n[{i}/{len(process_questions)}] Searching: {question}")

            # é…ç½®æœç´¢å‚æ•°
            search_config = SearchConfig(
                query=question,
                source_config_id=source_config_id,
                return_type=ReturnType.PARAGRAPH,
                recall=RecallConfig(
                    use_fast_mode=False,
                    vector_top_k=50,
                    max_entities=50,
                    recall_mode=RecallMode.FUZZY,
                    entity_similarity_threshold=0.3,
                    entity_weight_threshold=0.2
                ),
                expand=ExpandConfig(max_hops=3),
                rerank=RerankConfig(
                    max_results=10,
                    score_threshold=0.45,
                    strategy="pagerank"
                )
            )
            
            try:
                # æ‰§è¡Œæœç´¢
                search_result = await searcher.search(search_config)
                sections = search_result.get("sections", [])
                
                # æ®µè½å»é‡
                seen_chunk_ids = set()
                unique_sections = []
                for section in sections:
                    chunk_id = section.get('chunk_id')
                    if chunk_id and chunk_id not in seen_chunk_ids:
                        seen_chunk_ids.add(chunk_id)
                        unique_sections.append(section)
                
                search_results.append({
                    'question_index': i,
                    'question': question,
                    'sections': unique_sections,
                    'total_sections': len(unique_sections),
                    'search_success': True
                })
                
                if verbose:
                    logger.info(f"   Found {len(unique_sections)} unique sections")
                    
            except Exception as e:
                logger.error(f"   Search failed: {e}")
                search_results.append({
                    'question_index': i,
                    'question': question,
                    'sections': [],
                    'total_sections': 0,
                    'search_success': False,
                    'error': str(e)
                })

            # ğŸ†• æ¯ bench_size ä¸ªé—®é¢˜æ‰§è¡Œå›è°ƒ
            if bench_size and callback and i % bench_size == 0:
                await callback(i, len(process_questions), search_results)

        # æ¸…ç†èµ„æº
        try:
            await close_es_client()
        except Exception as e:
            logger.warning(f"Error closing ES client: {e}")

        logger.info(f"Search completed for {len(search_results)} questions")
        return search_results
    
    @classmethod
    def _check_content_similarity(cls, gold_content: str, retrieved_content: str) -> bool:
        """
        æ£€æŸ¥å†…å®¹ç›¸ä¼¼æ€§
        
        Args:
            gold_content: æ ‡å‡†å†…å®¹
            retrieved_content: æ£€ç´¢åˆ°çš„å†…å®¹
            
        Returns:
            æ˜¯å¦åŒ¹é…
        """
        # ç®€å•çš„å†…å®¹åŒ¹é…é€»è¾‘ï¼šæ£€æŸ¥å…³é”®è¯æ˜¯å¦å­˜åœ¨
        gold_words = set(gold_content.lower().split())
        retrieved_words = set(retrieved_content.lower().split())
        
        # è®¡ç®—äº¤é›†çš„æ¯”ä¾‹
        if len(gold_words) == 0:
            return False
            
        intersection = gold_words & retrieved_words
        similarity_ratio = len(intersection) / len(gold_words)
        
        # å¦‚æœäº¤é›†è¶…è¿‡50%ï¼Œè®¤ä¸ºåŒ¹é…æˆåŠŸ
        return similarity_ratio >= 0.5
    
    @classmethod
    async def show_retrieval_info(cls, limit: Optional[int] = None, show_paragraphs: bool = True, enable_search: bool = False, search_verbose: bool = False, enable_qa: bool = False, dataset_name: Optional[str] = None, badcase: Optional[str] = None, bench_size: Optional[int] = None, enable_mlflow: bool = False, mlflow_uri: Optional[str] = None, mlflow_experiment: Optional[str] = None) -> Dict[str, Any]:
        """
        æ˜¾ç¤ºæ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼šæœ€æ–°çš„ source_config_idã€dataset_name å’Œæ•°æ®é›†å†…å®¹ï¼Œå¯é€‰è¿›è¡Œå®é™…æ£€ç´¢

        Args:
            limit: é™åˆ¶æ˜¾ç¤ºçš„é—®é¢˜æ•°é‡ï¼ŒNoneè¡¨ç¤ºæ˜¾ç¤ºå…¨éƒ¨
            show_paragraphs: æ˜¯å¦æ˜¾ç¤º paragraphs è¯¦ç»†ä¿¡æ¯
            enable_search: æ˜¯å¦å¯ç”¨å®é™…æ£€ç´¢åŠŸèƒ½
            search_verbose: æ£€ç´¢è¿‡ç¨‹æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            enable_qa: æ˜¯å¦å¯ç”¨ QA è¯„ä¼°åŠŸèƒ½
            dataset_name: æ•°æ®é›†åç§°ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™éå†æ‰€æœ‰æ•°æ®é›†ç›®å½•æ‰¾æœ€æ–°çš„
            badcase: åŠ è½½ç‰¹å®šçš„ badcase è¿›è¡Œæµ‹è¯• ('zero' æˆ– 'partial')

        Returns:
            å®Œæ•´çš„æ£€ç´¢ä¿¡æ¯å­—å…¸
        """
        logger.info("Loading latest source information...")
        
        # 1. åŠ è½½æœ€æ–°çš„ source_info
        try:
            source_info = cls.load_latest_source_info(dataset_name)
            logger.info("Successfully loaded source information")
        except Exception as e:
            logger.error(f"Failed to load source info: {e}")
            raise

        # 2. åŠ è½½å¯¹åº”çš„æ•°æ®é›†ä¿¡æ¯
        if dataset_name is None:
            dataset_name = source_info['dataset_name']
        logger.info(f"Loading dataset: {dataset_name}")

        # æ— è®ºæ˜¯å¦åŠ è½½ badcaseï¼Œéƒ½éœ€è¦åˆ›å»º dataset_loader
        try:
            from dataflow.evaluation.utils.load_utils import DatasetLoader
            dataset_loader = DatasetLoader(dataset_name)
        except Exception as e:
            logger.error(f"Failed to create dataset loader for {dataset_name}: {e}")
            raise

        # ğŸ†• æ£€æŸ¥æ˜¯å¦åŠ è½½ badcase
        if badcase:
            logger.info(f"\nğŸ¯ Badcase æ¨¡å¼: åŠ è½½ {badcase} recall çš„é—®é¢˜")
            badcase_questions = cls.load_badcase_questions(dataset_name, badcase)

            if badcase_questions is None or len(badcase_questions) == 0:
                logger.error(f"æ— æ³•åŠ è½½ {badcase} badcaseï¼Œè¯·ç¡®ä¿å·²ç»è¿è¡Œè¿‡æ£€ç´¢è¯„ä¼°å¹¶ä¿å­˜äº† badcase")
                sys.exit(1)

            # ä» badcase åŠ è½½é—®é¢˜
            if limit:
                questions = badcase_questions[:limit]
            else:
                questions = badcase_questions

            answers = [None] * len(questions)  # Badcase æ–‡ä»¶ä¸­ä¸åŒ…å«ç­”æ¡ˆ
            paragraphs = [None] * len(questions)

            # åŠ è½½æ•°æ®é›†ä¿¡æ¯ï¼ˆç”¨äº gold_docs å’Œè¿”å› valï¼‰
            try:
                dataset_info = dataset_loader.load_all()
            except Exception as e:
                logger.error(f"Failed to load dataset info: {e}")
                raise

            logger.info(f"æˆåŠŸåŠ è½½ {len(questions)} ä¸ª {badcase} badcase é—®é¢˜")
        else:
            # æ­£å¸¸åŠ è½½æ•°æ®é›†
            try:
                dataset_info = dataset_loader.load_all()
                logger.info(f"Successfully loaded dataset with {dataset_info['total_questions']} questions")
            except Exception as e:
                logger.error(f"Failed to load dataset {dataset_name}: {e}")
                raise

            # æ­£å¸¸è·å–æ•°æ®é›†ä¿¡æ¯
            questions = dataset_info['questions']
            answers = dataset_info['answers']
            paragraphs = dataset_info['paragraphs']

        # 3. æ‰“å°ä¿¡æ¯
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ” æ£€ç´¢ä¿¡æ¯æ¦‚è§ˆ")
        logger.info("=" * 80)
        logger.info(f"ğŸ“ ä¿¡æ¯æºæ–‡ä»¶: {source_info['file_path']}")
        logger.info(f"ğŸ†” ä¿¡æ¯æºID: {source_info['source_config_id']}")
        logger.info(f"ğŸ“Š æ•°æ®é›†åç§°: {source_info['dataset_name']}")
        logger.info(f"ğŸ“… æ—¶é—´æˆ³: {source_info['timestamp']}")
        logger.info(f"ğŸ“ ä¿¡æ¯æºåç§°: {source_info['source_name']}")
        logger.info(f"â“ é—®é¢˜æ€»æ•°: {len(questions)}")

        if badcase:
            logger.info(f"ğŸ¯ æµ‹è¯•æ¨¡å¼: {badcase} recall badcase")

        # 4. æ˜¾ç¤ºé—®é¢˜å’Œç­”æ¡ˆä¿¡æ¯

        # åº”ç”¨é™åˆ¶
        display_limit = min(len(questions), limit) if limit else len(questions)

        logger.info(f"\nğŸ“‹ æ˜¾ç¤ºå‰ {display_limit} ä¸ªé—®é¢˜:")
        logger.info("=" * 80)

        # åˆ¤æ–­æ˜¯å¦åœ¨æµ‹è¯•æ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼‰
        is_testing_mode = enable_search and badcase

        for i in range(display_limit):
            logger.info(f"\n[é—®é¢˜ {i+1}]")
            logger.info(f"é—®é¢˜: {questions[i]}")

            if not is_testing_mode:
                logger.info(f"ç­”æ¡ˆ: {answers[i] if i < len(answers) else 'N/A'}")

                if show_paragraphs and i < len(paragraphs) and paragraphs[i] is not None:
                    para_list = paragraphs[i]
                    logger.info(f"æ®µè½ä¿¡æ¯ ({len(para_list)} ä¸ª):")

                    for j, para in enumerate(para_list):
                        title = para.get('title', 'N/A')
                        text = para.get('text', 'N/A')
                        is_supporting = para.get('is_supporting', False)

                        logger.info(f"   [{j+1}] æ ‡é¢˜: {title}")
                        logger.info(f"       æ”¯æŒæ€§: {'æ˜¯' if is_supporting else 'å¦'}")
                        logger.info(f"       å†…å®¹: {text[:200]}..." if len(text) > 200 else f"       å†…å®¹: {text}")

            if i < display_limit - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ª
                logger.info("-" * 60)
        
        logger.info("\n" + "=" * 80)
        
        # 5. å¦‚æœå¯ç”¨äº†æœç´¢ï¼Œæ‰§è¡Œå®é™…æ£€ç´¢
        search_results = None
        recall_evaluation = None
        token_tracker = LLMTokenTracker()  # ğŸ†• åœ¨æ‰€æœ‰è·¯å¾„ä¸‹éƒ½ä¼šåˆ›å»ºï¼ˆç”¨äºè¿½è¸ªsearchå’ŒQAï¼‰
        search_time = 0.0  # ğŸ†• SEARCHé˜¶æ®µæ€»è€—æ—¶

        # ğŸ†• MLflow åˆå§‹åŒ–
        mlflow_run = None
        if enable_mlflow and enable_search and not badcase:  # åªåœ¨searchæ¨¡å¼ä¸”ébadcaseæ—¶å¯ç”¨
            import mlflow
            from datetime import datetime

            # è®¾ç½® tracking URI
            if mlflow_uri:
                mlflow.set_tracking_uri(mlflow_uri)

            # åˆ›å»ºæˆ–è·å–å®éªŒ
            try:
                mlflow.set_experiment(mlflow_experiment)
            except Exception as e:
                if "deleted experiment" in str(e):
                    # å¦‚æœå®éªŒè¢«åˆ é™¤ï¼Œåˆ›å»ºæ–°å®éªŒ
                    new_exp_name = f"{mlflow_experiment}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                    mlflow.set_experiment(new_exp_name)
                else:
                    raise

            # åˆ›å»ºè¿è¡Œ
            run_name = f"retrieval_{dataset_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            mlflow_run = mlflow.start_run(run_name=run_name)

            # è®°å½•å‚æ•°
            mlflow.log_params({
                "dataset": dataset_name,
                "bench_size": bench_size or 0,
                "total_questions": len(questions),
                "enable_qa": enable_qa,
                "vector_top_k": 50,
                "max_entities": 50,
                "max_hops": 3,
                "max_results": 10
            })

            logger.info(f"âœ… MLflow ç›‘æ§å·²å¯ç”¨ (å®éªŒ: {mlflow_experiment}, è¿è¡Œ: {run_name})")

        if enable_search:
            logger.info(f"\nå¯åŠ¨å®é™…æ£€ç´¢ (æ•°æ®æº: {source_info['source_config_id']})...")
            logger.info("=" * 80)

            # ğŸ†• å¯ç”¨ LLM token è¿½è¸ªï¼ˆåœ¨searchä¹‹å‰ï¼‰
            enable_llm_tracking(token_tracker)
            logger.info("âœ… æ£€ç´¢é˜¶æ®µçš„LLMè°ƒç”¨è¿½è¸ªå·²å¯ç”¨")

            try:
                # ğŸ†• å¼€å§‹è®¡æ—¶
                search_start = time.perf_counter()

                # é¢„è·å–æ ‡å‡†ç­”æ¡ˆæ–‡æ¡£ï¼Œç”¨äº bench_size å›è°ƒ
                gold_docs_for_recall = dataset_loader.get_gold_docs_for_recall(max_length=500, limit=display_limit)

                # ğŸ†• ä½¿ç”¨ç‹¬ç«‹çš„ logger ç”¨äº benchmark è¾“å‡ºï¼ˆä¸å— verbose å‚æ•°å½±å“ï¼‰
                bench_logger = logging.getLogger('dataflow.evaluation.benchmark')
                # æ˜¾å¼è®¾ç½®çº§åˆ«ä¸º INFOï¼Œç¡®ä¿ä¸å— root logger çº§åˆ«å½±å“
                bench_logger.setLevel(logging.INFO)
                # æ·»åŠ  handlerï¼Œç¡®ä¿æ—¥å¿—èƒ½å¤Ÿè¾“å‡º
                if not bench_logger.handlers:
                    console_handler = logging.StreamHandler()
                    console_handler.setLevel(logging.INFO)
                    formatter = logging.Formatter('%(message)s')
                    console_handler.setFormatter(formatter)
                    bench_logger.addHandler(console_handler)
                    bench_logger.propagate = False  # ä¸ä¼ æ’­åˆ°root logger

                # å®šä¹‰ bench_size å›è°ƒå‡½æ•°
                async def bench_callback(current_idx, total, results_so_far):
                    """æ¯ bench_size ä¸ªé—®é¢˜æ‰§è¡Œçš„å›è°ƒ"""
                    # è®¡ç®—æ‰¹æ¬¡ä¿¡æ¯ (æ‰¹æ¬¡ä»1å¼€å§‹ç¼–å·)
                    batch_index = (current_idx + bench_size - 1) // bench_size  # å‘ä¸Šå–æ•´
                    total_batches = (total + bench_size - 1) // bench_size  # å‘ä¸Šå–æ•´

                    bench_logger.info(f"\n{'='*80}")
                    bench_logger.info(f"ğŸ“ Bench è¿›åº¦: æ‰¹æ¬¡ {batch_index}/{total_batches} ({current_idx}/{total} é—®é¢˜)")
                    bench_logger.info(f"{'='*80}")

                    # è·å–å½“å‰å·²å¤„ç†é—®é¢˜çš„ gold_docs
                    current_gold_docs = gold_docs_for_recall[:current_idx]

                    # è®¡ç®—ç´¯ç§¯ç»Ÿè®¡ï¼ˆé‡æ–°è®¡ç®— matched_docsï¼Œå› ä¸ºæ­¤æ—¶è¿˜æ²¡æœ‰åŒ¹é…ä¿¡æ¯ï¼‰
                    full_count = 0
                    partial_count = 0
                    zero_count = 0

                    for i, result in enumerate(results_so_far):
                        gold_docs = current_gold_docs[i]
                        retrieved_docs = []

                        # è·å–æ£€ç´¢ç»“æœæ®µè½ï¼ˆæ ¼å¼ä¸å¬å›è¯„ä¼°ä¸€è‡´ï¼‰
                        for section in result['sections']:
                            heading = section.get('heading', '')
                            content = section.get('content', '')
                            clean_heading = heading.lstrip('#').strip()
                            clean_content = content.strip()
                            if clean_heading and clean_content:
                                retrieved_docs.append({
                                    'title': clean_heading,
                                    'content': clean_content
                                })

                        # é‡æ–°è®¡ç®— matched_docs
                        matched_docs = []
                        for gold_doc in gold_docs:
                            for retrieved_doc in retrieved_docs:
                                title_match = gold_doc['title'].strip().lower() == retrieved_doc['title'].strip().lower()
                                content_match = cls._check_content_similarity(
                                    gold_doc['content'],
                                    retrieved_doc['content']
                                )
                                if title_match and content_match:
                                    matched_docs.append(gold_doc['title'])
                                    break

                        if len(matched_docs) == 0:
                            zero_count += 1
                        elif len(matched_docs) < len(gold_docs):
                            partial_count += 1
                        else:
                            full_count += 1

                    # æ‰“å°ç´¯ç§¯ç»Ÿè®¡
                    bench_logger.info(f"\nğŸ“Š ç´¯ç§¯å¬å›æƒ…å†µç»Ÿè®¡ ({current_idx} ä¸ªé—®é¢˜):")
                    bench_logger.info("=" * 50)
                    bench_logger.info(f"âœ… å…¨éƒ¨å¬å›: {full_count} ä¸ª ({full_count/current_idx*100:.1f}%)")
                    bench_logger.info(f"âš ï¸  éƒ¨åˆ†å¬å›: {partial_count} ä¸ª ({partial_count/current_idx*100:.1f}%)")
                    bench_logger.info(f"âŒ é›¶å¬å›: {zero_count} ä¸ª ({zero_count/current_idx*100:.1f}%)")
                    bench_logger.info("=" * 50)

                    # ğŸ†• å‡†å¤‡ recall è¯„ä¼°æ•°æ®
                    gold_docs_list_for_recall = []
                    retrieved_docs_list_for_recall = []

                    for i, result in enumerate(results_so_far):
                        gold_docs = current_gold_docs[i]
                        retrieved_docs_for_recall = []

                        # è·å–æ£€ç´¢ç»“æœæ®µè½ï¼ˆæ ¼å¼ä¸å¬å›è¯„ä¼°ä¸€è‡´ï¼‰
                        for section in result['sections']:
                            heading = section.get('heading', '')
                            content = section.get('content', '')
                            clean_heading = heading.lstrip('#').strip()
                            clean_content = content.strip()
                            if clean_heading and clean_content:
                                retrieved_docs_for_recall.append({
                                    'title': clean_heading,
                                    'content': clean_content
                                })

                        # ä¸º recall è¯„ä¼°å‡†å¤‡æ•°æ®
                        gold_titles = [doc['title'] for doc in gold_docs]
                        gold_docs_list_for_recall.append(gold_titles)
                        retrieved_docs_list_for_recall.append([doc['title'] for doc in retrieved_docs_for_recall])

                    # è®¡ç®— recall@k
                    recall_metric = RetrievalRecall()
                    pooled_recall, _ = recall_metric.calculate_metric_scores(
                        gold_docs=gold_docs_list_for_recall,
                        retrieved_docs=retrieved_docs_list_for_recall,
                        k_list=[1, 2, 5, 10]
                    )

                    bench_logger.info(f"\nç´¯ç§¯Recall@K:")
                    for metric, score in pooled_recall.items():
                        bench_logger.info(f"  {metric}: {score:.4f} ({score*100:.2f}%)")

                    # ğŸ†• ä¸Šä¼ åˆ° MLflow
                    if enable_mlflow and mlflow_run:
                        import mlflow

                        # è®°å½•ç»Ÿè®¡æ•°é‡
                        mlflow.log_metrics({
                            "full_recall_count": full_count,
                            "partial_recall_count": partial_count,
                            "zero_recall_count": zero_count,
                            "questions_processed": current_idx
                        }, step=batch_index)

                        # è®°å½• Recall@K
                        for metric_name, score in pooled_recall.items():
                            k = int(metric_name.split('@')[1])
                            mlflow.log_metric(f"recall_at_{k}", score, step=batch_index)

                # æ‰§è¡Œæ£€ç´¢
                search_results = await cls.search_questions(
                    source_config_id=source_info['source_config_id'],
                    questions=questions,
                    limit=display_limit,
                    verbose=search_verbose,
                    bench_size=bench_size,
                    callback=bench_callback if bench_size else None
                )

                # ğŸ†• è®¡ç®—SEARCHé˜¶æ®µè€—æ—¶
                search_time = time.perf_counter() - search_start

                if gold_docs_for_recall and len(gold_docs_for_recall) == len(search_results):
                    bench_logger.info(f"\næ­£åœ¨è¿›è¡Œå¬å›ç‡è¯„ä¼°ï¼ˆæ•°æ®é›†: {dataset_name}ï¼‰...")
                    # å‡†å¤‡è¯„ä¼°æ•°æ®
                    gold_docs_list = []
                    retrieved_docs_list = []

                    for i, result in enumerate(search_results):
                        if i < len(gold_docs_for_recall):
                            # è·å–æ ‡å‡†ç­”æ¡ˆæ–‡æ¡£
                            gold_docs = gold_docs_for_recall[i]

                            # è·å–æ£€ç´¢ç»“æœæ®µè½ï¼ˆæ¸…ç†markdownæ ‡è®°ï¼‰
                            retrieved_docs = []
                            for section in result['sections']:
                                heading = section.get('heading', '')
                                content = section.get('content', '')
                                # æ¸…ç†markdownæ ‡è®°ï¼ˆ# å‰ç¼€ï¼‰å’Œé¦–å°¾ç©ºæ ¼
                                clean_heading = heading.lstrip('#').strip()
                                clean_content = content.strip()
                                if clean_heading and clean_content:
                                    retrieved_docs.append({
                                        'title': clean_heading,
                                        'content': clean_content
                                    })

                            # è¿›è¡Œæ ‡é¢˜+å†…å®¹çš„åŒé‡åŒ¹é…
                            matched_docs = []
                            for gold_doc in gold_docs:
                                for retrieved_doc in retrieved_docs:
                                    title_match = gold_doc['title'].strip().lower() == retrieved_doc['title'].strip().lower()
                                    # å†…å®¹åŒ¹é…ï¼šæ£€æŸ¥æ£€ç´¢å†…å®¹æ˜¯å¦åŒ…å«æ ‡å‡†å†…å®¹çš„å…³é”®ä¿¡æ¯
                                    content_match = cls._check_content_similarity(
                                        gold_doc['content'],
                                        retrieved_doc['content']
                                    )

                                    if title_match and content_match:
                                        matched_docs.append(gold_doc['title'])
                                        break  # æ‰¾åˆ°åŒ¹é…åé€€å‡ºå†…å¾ªç¯

                            # ä¸ºäº†å…¼å®¹RetrievalRecallï¼Œä¼ é€’æ ‡é¢˜åˆ—è¡¨
                            gold_titles = [doc['title'] for doc in gold_docs]
                            gold_docs_list.append(gold_titles)  # ä¼ é€’æ‰€æœ‰æ ‡å‡†ç­”æ¡ˆæ–‡æ¡£ï¼ˆä¿®å¤bugï¼‰
                            retrieved_docs_list.append([doc['title'] for doc in retrieved_docs])

                            # å°†åŒ¹é…ç»“æœä¿å­˜åˆ° search_resultsï¼Œç”¨äºåç»­æ˜¾ç¤º
                            result['matched_docs'] = matched_docs
                            result['gold_titles'] = gold_titles

                            # è°ƒè¯•è¾“å‡º
                            logger.debug(f"\n[DEBUG] é—®é¢˜ {i+1}:")
                            logger.debug(f"  æ ‡å‡†æ–‡æ¡£: {gold_titles}")
                            logger.debug(f"  æ£€ç´¢æ–‡æ¡£: {[doc['title'] for doc in retrieved_docs[:5]]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
                            logger.debug(f"  åŒ¹é…æˆåŠŸ: {matched_docs}")
                            logger.debug(f"  åŒ¹é…ç‡: {len(matched_docs)}/{len(gold_titles)}")
                    # ä½¿ç”¨RetrievalRecallè¿›è¡Œè¯„ä¼°
                    if gold_docs_list and retrieved_docs_list:
                        recall_metric = RetrievalRecall()

                        pooled_recall, example_recalls = recall_metric.calculate_metric_scores(
                            gold_docs=gold_docs_list,
                            retrieved_docs=retrieved_docs_list,
                            k_list=[1, 2, 5, 10]
                        )
                        
                        recall_evaluation = {
                            'pooled_results': pooled_recall,
                            'example_results': example_recalls,
                            'num_questions': len(gold_docs_list)
                        }
                        
                        bench_logger.info(f"\nå¬å›ç‡è¯„ä¼°ç»“æœ:")
                        bench_logger.info("=" * 50)
                        for metric, score in pooled_recall.items():
                            bench_logger.info(f"{metric}: {score:.4f} ({score*100:.2f}%)")
                        bench_logger.info("=" * 50)

                        # Debug info message to verify execution
                        logger.info("DEBUG: Starting statistics calculation...")

                        # Debug print to verify execution
                        logger.debug(f"DEBUG: About to calculate statistics. gold_docs_for_recall length: {len(gold_docs_for_recall)}")

                        # ğŸ†• ç»Ÿè®¡å„ä¸ªé—®é¢˜çš„å¬å›æƒ…å†µ
                        logger.info("DEBUG: Starting statistics calculation loop...")
                        total_questions = len(gold_docs_for_recall)
                        full_recall_count = 0  # å…¨éƒ¨å¬å›
                        partial_recall_count = 0  # éƒ¨åˆ†å¬å›
                        zero_recall_count = 0  # é›¶å¬å›

                        logger.info(f"DEBUG: total_questions={total_questions}, search_results length={len(search_results)}")

                        for i, result in enumerate(search_results):
                            if i < len(gold_docs_for_recall):
                                gold_docs = gold_docs_for_recall[i]
                                matched_docs = result.get('matched_docs', [])

                                if len(matched_docs) == 0:
                                    zero_recall_count += 1

                                elif len(matched_docs) < len(gold_docs):
                                    partial_recall_count += 1
   
                                else:
                                    full_recall_count += 1


                        # ğŸ†• ä¿å­˜ç»Ÿè®¡ç»“æœåˆ° recall_evaluation
                        statistics = {
                            'total_questions': total_questions,
                            'full_recall_count': full_recall_count,
                            'partial_recall_count': partial_recall_count,
                            'zero_recall_count': zero_recall_count
                        }

                        # ğŸ†• æ˜¾ç¤ºç»Ÿè®¡ç»“æœï¼ˆåœ¨è¯¦ç»†æ—¥å¿—ä¸­ï¼‰
                        logger.info(f"\nğŸ“Š é—®é¢˜å¬å›æƒ…å†µç»Ÿè®¡:")
                        logger.info("=" * 50)
                        logger.info(f"æ€»é—®é¢˜æ•°: {total_questions}")
                        logger.info(f"âœ… å…¨éƒ¨å¬å›: {full_recall_count} ä¸ª ({full_recall_count/total_questions*100:.1f}%)")
                        logger.info(f"âš ï¸  éƒ¨åˆ†å¬å›: {partial_recall_count} ä¸ª ({partial_recall_count/total_questions*100:.1f}%)")
                        logger.info(f"âŒ é›¶å¬å›: {zero_recall_count} ä¸ª ({zero_recall_count/total_questions*100:.1f}%)")
                        logger.info("=" * 50)

                        # ğŸ†• ä¿å­˜ badcase ä¿¡æ¯
                        try:
                            # è·å–å½“å‰æ¨¡å‹åç§°
                            from dataflow.core.config import get_settings
                            settings = get_settings()
                            model_name = settings.llm_model
                            if '/' in model_name:
                                filtered_model_name = model_name.split('/')[-1]
                            else:
                                filtered_model_name = model_name

                            # ä» source_info è·å–æ—¶é—´æˆ³
                            timestamp = source_info.get('timestamp', datetime.now().strftime("%Y%m%d_%H%M%S"))

                            # åˆ›å»ºè¾“å‡ºç›®å½•ï¼šSAG/{model_name}/{dataset_name}/{timestamp}
                            output_base_dir = Path(__file__).parent / "outputs" / "SAG" / filtered_model_name / dataset_name / timestamp
                            output_base_dir.mkdir(parents=True, exist_ok=True)

                            # åˆ†ç±» badcase
                            zero_recall_cases = []  # å®Œå…¨æ²¡æœ‰å¬å›
                            partial_recall_cases = []  # éƒ¨åˆ†å¬å›

                            for i, result in enumerate(search_results):
                                if i < len(gold_docs_for_recall):
                                    gold_docs = gold_docs_for_recall[i]
                                    matched_docs = result.get('matched_docs', [])

                                    # æ„å»ºä¸åŸæ•°æ®é›†æ ¼å¼å…¼å®¹çš„ç»“æ„
                                    # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯å¯JSONåºåˆ—åŒ–çš„åŸºæœ¬ç±»å‹
                                    badcase_data = {
                                        'question_index': i + 1,
                                        'question': str(result['question']) if result['question'] else "",
                                        'answer': str(answers[i]) if i < len(answers) and answers[i] else "",
                                        'gold_docs': [
                                            {
                                                'title': str(doc.get('title', '')),
                                                'content': str(doc.get('content', ''))
                                            }
                                            for doc in gold_docs
                                        ],
                                        'retrieved_docs': [  # æ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆåªåŒ…å«æ ‡é¢˜å’Œå†…å®¹ï¼‰
                                            {
                                                'title': str(section.get('heading', '')).lstrip('#').strip(),
                                                'content': str(section.get('content', '')),
                                                'cosine_score': float(section.get('original_score', 0.0)),
                                                'pagerank_score': float(section.get('weight') or section.get('pagerank', 0.0))
                                            }
                                            for section in result['sections']
                                        ],
                                        'matched_docs': [str(doc) for doc in matched_docs],  # æˆåŠŸåŒ¹é…çš„æ–‡æ¡£æ ‡é¢˜
                                        'recall_status': {
                                            'total_gold': int(len(gold_docs)),
                                            'matched': int(len(matched_docs)),
                                            'ratio': float(len(matched_docs) / len(gold_docs) if gold_docs else 0)
                                        }
                                    }

                                    # æ£€æŸ¥å¬å›æƒ…å†µ
                                    if len(matched_docs) == 0:
                                        # Zero recall - å®Œå…¨æ²¡æœ‰å¬å›
                                        zero_recall_cases.append(badcase_data)
                                    elif len(matched_docs) < len(gold_docs):
                                        # Partial recall - éƒ¨åˆ†å¬å›
                                        partial_recall_cases.append(badcase_data)

                            # ä¿å­˜ zero recall cases
                            if zero_recall_cases:
                                zero_output_file = output_base_dir / f"zero_{dataset_name}.json"
                                try:
                                    with open(zero_output_file, 'w', encoding='utf-8') as f:
                                        json.dump(zero_recall_cases, f, ensure_ascii=False, indent=2)
                                    logger.info(f"\nğŸ’¾ ä¿å­˜ Zero Recall Badcase: {zero_output_file}")
                                    logger.info(f"   å…± {len(zero_recall_cases)} ä¸ªé—®é¢˜å®Œå…¨æ²¡æœ‰å¬å›")
                                except Exception as e:
                                    logger.error(f"ä¿å­˜ Zero Recall Badcase å¤±è´¥: {e}")
                                    logger.error(f"æ•°æ®å†…å®¹: {zero_recall_cases[:2] if zero_recall_cases else 'ç©º'}")  # æ‰“å°å‰ä¸¤ä¸ªæ ·æœ¬

                            # ä¿å­˜ partial recall cases
                            if partial_recall_cases:
                                partial_output_file = output_base_dir / f"partial_{dataset_name}.json"
                                try:
                                    with open(partial_output_file, 'w', encoding='utf-8') as f:
                                        json.dump(partial_recall_cases, f, ensure_ascii=False, indent=2)
                                    logger.info(f"\nğŸ’¾ ä¿å­˜ Partial Recall Badcase: {partial_output_file}")
                                    logger.info(f"   å…± {len(partial_recall_cases)} ä¸ªé—®é¢˜éƒ¨åˆ†å¬å›")
                                except Exception as e:
                                    logger.error(f"ä¿å­˜ Partial Recall Badcase å¤±è´¥: {e}")
                                    logger.error(f"æ•°æ®å†…å®¹: {partial_recall_cases[:2] if partial_recall_cases else 'ç©º'}")  # æ‰“å°å‰ä¸¤ä¸ªæ ·æœ¬

                        except Exception as e:
                            logger.warning(f"ä¿å­˜ badcase å¤±è´¥: {e}")
                            import traceback
                            logger.warning(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
                
                # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
                logger.info(f"\næ£€ç´¢ç»“æœæ¦‚è¦:")
                logger.info("=" * 80)
                
                total_successful = sum(1 for r in search_results if r['search_success'])
                total_sections = sum(r['total_sections'] for r in search_results)
                
                logger.info(f"æˆåŠŸæ£€ç´¢: {total_successful}/{len(search_results)} ä¸ªé—®é¢˜")
                logger.info(f"æ€»æ£€ç´¢æ®µè½æ•°: {total_sections} ä¸ª")
                
                # æ˜¾ç¤ºæ¯ä¸ªé—®é¢˜çš„æ£€ç´¢ç»“æœ
                for result in search_results:
                    logger.info(f"\n[é—®é¢˜ {result['question_index']}] {result['question']}")

                    if result['search_success']:
                        sections = result['sections']
                        matched_docs = result.get('matched_docs', [])

                        logger.info(f"æ£€ç´¢åˆ° {len(sections)} ä¸ªç›¸å…³æ®µè½:")

                        for j, section in enumerate(sections[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                            heading = section.get('heading', 'N/A')
                            content = section.get('content', '').replace('\n', ' ')

                            # è·å–å¾—åˆ†ä¿¡æ¯
                            cosine_score = section.get('original_score', 0.0)
                            pagerank_score = section.get('weight') or section.get('pagerank', 0.0)

                            # æ£€æŸ¥æ˜¯å¦æ˜¯æ­£ç¡®ç­”æ¡ˆ
                            is_correct = heading.lstrip('#').strip() in matched_docs

                            logger.info(f"   [{j}] æ ‡é¢˜: {heading} {'âœ…' if is_correct else ''}")
                            logger.info(f"       ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_score:.4f} | PageRank: {pagerank_score:.4f}")
                            logger.info(f"       å†…å®¹: {content[:200]}..." if len(content) > 200 else f"       å†…å®¹: {content}")

                        if len(sections) > 5:
                            logger.info(f"   ... è¿˜æœ‰ {len(sections) - 5} ä¸ªæ®µè½")

                        # æ˜¾ç¤ºæ­£ç¡®ç­”æ¡ˆç»Ÿè®¡
                        total_matches = len(matched_docs) if matched_docs else 0
                        logger.info(f"\n   æ­£ç¡®ç­”æ¡ˆåŒ¹é…: {total_matches} ä¸ª")
                    else:
                        error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
                        logger.info(f"æ£€ç´¢å¤±è´¥: {error_msg}")

                    if result != search_results[-1]:  # ä¸æ˜¯æœ€åä¸€ä¸ª
                        logger.info("-" * 60)
                
                logger.info("\n" + "=" * 80)

                # ğŸ†• QA è¯„ä¼°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                qa_evaluation = None
                if enable_qa:
                    # æ£€æŸ¥æ˜¯å¦åœ¨ badcase æ¨¡å¼ä¸‹ï¼ˆbadcase ä¸éœ€è¦è¿›è¡Œ QA è¯„ä¼°ï¼‰
                    if badcase:
                        logger.warning(f"âš ï¸  Badcase æ¨¡å¼ä¸‹ä¸æ”¯æŒ QA è¯„ä¼°ï¼Œå·²è‡ªåŠ¨ç¦ç”¨")
                        logger.warning(f"   Badcase ä¸»è¦ç”¨äºåˆ†æå’Œä¼˜åŒ–æ£€ç´¢æ€§èƒ½")
                    else:
                        logger.info(f"\næ­£åœ¨è¿›è¡Œ QA è¯„ä¼°...")
                        logger.info("=" * 80)

                        # å‡†å¤‡ QA è¯„ä¼°æ•°æ®
                        gold_answers_list = []
                        predicted_answers_list = []

                        # æ£€æŸ¥æ•°æ®é›†å¯¹åº”çš„æç¤ºè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                        qa_prompt_path = f"prompts/benchmark/{dataset_name}.yaml"
                        import os
                        if not os.path.exists(qa_prompt_path):
                            # å¦‚æœæ•°æ®é›†ç‰¹å®šæç¤ºè¯ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤çš„ musique æç¤ºè¯
                            qa_prompt_path = "prompts/benchmark/musique.yaml"
                            logger.info(f"æ•°æ®é›† {dataset_name} çš„æç¤ºè¯ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æç¤ºè¯: {qa_prompt_path}")
                        else:
                            logger.info(f"åŠ è½½ QA æç¤ºè¯: {qa_prompt_path}")

                        # åˆå§‹åŒ– QA ç”Ÿæˆå™¨ï¼ˆä½¿ç”¨ç±»çº§åˆ«ç¼“å­˜ï¼Œé¿å…é‡å¤åˆ›å»ºï¼‰
                        llm_client = cls._llm_client
                        prompt_manager = cls._prompt_manager

                        # ğŸ†• QAé˜¶æ®µæ€»è€—æ—¶
                        qa_time = 0.0

                        # å¦‚æœç¼“å­˜ä¸ºç©ºï¼Œåˆ™åˆ›å»º LLM å®¢æˆ·ç«¯å’Œ PromptManager
                        if llm_client is None:
                            logger.info("é¦–æ¬¡ä½¿ç”¨ï¼Œåˆ›å»º LLM å®¢æˆ·ç«¯ï¼ˆåç»­å°†å¤ç”¨ï¼‰...")
                            llm_client = await create_llm_client(scenario='search')
                            prompt_manager = PromptManager()

                            # ä¿å­˜åˆ°ç±»çº§åˆ«ç¼“å­˜
                            cls._llm_client = llm_client
                            cls._prompt_manager = prompt_manager
                        else:
                            logger.info("å¤ç”¨å·²å­˜åœ¨çš„ LLM å®¢æˆ·ç«¯ï¼ˆé¿å…é‡å¤åˆ›å»ºï¼‰")

                        # ğŸ†• å¼€å§‹QAè®¡æ—¶
                        qa_start = time.perf_counter()

                        for i, result in enumerate(search_results):
                            if i >= len(answers):
                                break

                            question = result['question']
                            sections = result['sections']

                            # è·å–æ ‡å‡†ç­”æ¡ˆ
                            gold_ans = answers[i]
                            if isinstance(gold_ans, str):
                                gold_ans = [gold_ans]
                            gold_answers_list.append(gold_ans)

                            # ç”Ÿæˆé¢„æµ‹ç­”æ¡ˆï¼ˆä½¿ç”¨ LLMï¼‰
                            if sections:
                                # æ ¼å¼åŒ–æ–‡æ¡£
                                documents = ""
                                for section in sections[:5]:  # ä½¿ç”¨å‰5ä¸ªæ®µè½
                                    heading = section.get('heading', '').lstrip('#').strip()
                                    content = section.get('content', '').strip()
                                    if heading and content:
                                        documents += f"Wikipedia Title: {heading}\n{content}\n\n"

                                try:
                                    # ä½¿ç”¨ prompt_manager.render() æ¸²æŸ“æç¤ºè¯
                                    prompt_text = prompt_manager.render(
                                        "rag_qa_musique",  # æ¨¡æ¿åç§°
                                        document_title="",
                                        document_content=documents,
                                        question=question
                                    )

                                    # è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
                                    from dataflow.core.ai.models import LLMMessage, LLMRole
                                    response = await llm_client.chat(
                                        messages=[
                                            LLMMessage(role=LLMRole.USER, content=prompt_text)
                                        ],
                                        max_tokens=200,
                                        temperature=0.1
                                    )

                                    # æå–ç­”æ¡ˆ
                                    response_text = response.content
                                    predicted_answer = response_text.split("Answer:")[-1].strip() if "Answer:" in response_text else response_text.strip()
                                    logger.debug(f"é—®é¢˜ {i+1} - LLM ç”Ÿæˆç­”æ¡ˆ: {predicted_answer[:50]}...")

                                except Exception as e:
                                    logger.warning(f"LLM ç”Ÿæˆç­”æ¡ˆå¤±è´¥ (é—®é¢˜ {i+1}): {e}")
                                    predicted_answer = ""
                            else:
                                predicted_answer = ""

                            predicted_answers_list.append(predicted_answer)

                        # ğŸ†• è®¡ç®—QAé˜¶æ®µæ€»è€—æ—¶ï¼ˆåœ¨QAå¾ªç¯ç»“æŸåç«‹å³è®¡ç®—ï¼‰
                        qa_time = time.perf_counter() - qa_start

                        # ä½¿ç”¨ç°æœ‰çš„ QA è¯„ä¼°æŒ‡æ ‡
                        from dataflow.evaluation.metrics import QAExactMatch, QAF1Score

                        qa_em_metric = QAExactMatch()
                        qa_f1_metric = QAF1Score()

                        # è®¡ç®— Exact Match
                        em_pooled, em_examples = qa_em_metric.calculate_metric_scores(
                            gold_answers=gold_answers_list,
                            predicted_answers=predicted_answers_list
                        )

                        # è®¡ç®— F1 Score
                        f1_pooled, f1_examples = qa_f1_metric.calculate_metric_scores(
                            gold_answers=gold_answers_list,
                            predicted_answers=predicted_answers_list
                        )

                        # åˆå¹¶ç»“æœ
                        qa_pooled_results = {**em_pooled, **f1_pooled}
                        qa_example_results = []
                        for em_ex, f1_ex in zip(em_examples, f1_examples):
                            qa_example_results.append({**em_ex, **f1_ex})

                        qa_evaluation = {
                            'pooled_results': qa_pooled_results,
                            'example_results': qa_example_results,
                            'num_questions': len(gold_answers_list)
                        }

                        # æ˜¾ç¤º QA è¯„ä¼°ç»“æœ
                        logger.info(f"\nQAè¯„ä¼°ç»“æœ:")
                        logger.info("=" * 50)
                        for metric, score in qa_pooled_results.items():
                            logger.info(f"{metric}: {score:.4f} ({score*100:.2f}%)")
                        logger.info("=" * 50)

                        # æ˜¾ç¤ºæ¯ä¸ªé—®é¢˜çš„ QA ç»“æœ
                        logger.info(f"\næ¯ä¸ªé—®é¢˜çš„ QA è¯„ä¼°:")
                        for i, (question, result) in enumerate(zip(questions[:display_limit], search_results)):
                            if i >= len(qa_example_results):
                                break
                            qa_result = qa_example_results[i]
                            logger.info(f"\n[é—®é¢˜ {i+1}] {question}")
                            logger.info(f"  é¢„æµ‹ç­”æ¡ˆ: {predicted_answers_list[i]}")
                            logger.info(f"  æ ‡å‡†ç­”æ¡ˆ: {', '.join(gold_answers_list[i])}")
                            logger.info(f"  EM: {qa_result['ExactMatch']:.2f} | F1: {qa_result['F1']:.2f}")


                # ğŸ†• æ˜¾ç¤º LLM Token æ¶ˆè€—ç»Ÿè®¡
                if (enable_search or enable_qa) and 'token_tracker' in locals():
                    token_summary = token_tracker.get_summary()
                    if token_summary['total_tokens'] > 0:
                        logger.info("\n" + "=" * 80)
                        logger.info("LLM Token æ¶ˆè€—ç»Ÿè®¡")
                        logger.info("=" * 80)

                        # æŒ‰é˜¶æ®µæ˜¾ç¤ºï¼ˆä¸LOAD/EXTRACTä¿æŒä¸€è‡´ï¼‰
                        if token_summary.get('stages'):
                            for stage, stats in token_summary['stages'].items():
                                logger.info(f"\n{stage}:")
                                logger.info(f"  è°ƒç”¨æ¬¡æ•°: {stats['calls']}")
                                logger.info(f"  è¾“å…¥ Tokens: {stats['prompt']:,}")
                                logger.info(f"  è¾“å‡º Tokens: {stats['completion']:,}")
                                logger.info(f"  æ€»è®¡ Tokens: {stats['total']:,}")
                        logger.info("\n" + "=" * 80 + "\n")

                # æ˜¾ç¤ºæ£€ç´¢è¯„ä¼°ç»“æœ
                if recall_evaluation:
                    logger.info(f"æ£€ç´¢ç»Ÿè®¡:")
                    logger.info(f"  é—®é¢˜æ•°: {recall_evaluation['num_questions']}")
                    for metric, score in recall_evaluation['pooled_results'].items():
                        logger.info(f"  {metric}: {score:.4f} ({score*100:.2f}%)")

                # æ˜¾ç¤ºQAè¯„ä¼°ç»“æœ
                if qa_evaluation:
                    logger.info(f"QAç»Ÿè®¡:")
                    logger.info(f"  é—®é¢˜æ•°: {qa_evaluation['num_questions']}")
                    for metric, score in qa_evaluation['pooled_results'].items():
                        logger.info(f"  {metric}: {score:.4f} ({score*100:.2f}%)")

            except Exception as e:
                logger.error(f"æ£€ç´¢è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()

            # 6. è¿”å›å®Œæ•´ä¿¡æ¯
            # å‡†å¤‡è¿”å›å€¼
            result = {
                'source_info': source_info,
                'dataset_info': dataset_info,
                'display_limit': display_limit,
                'show_paragraphs': show_paragraphs,
                'enable_search': enable_search,
                'search_results': search_results,
                'recall_evaluation': recall_evaluation,
                'qa_evaluation': qa_evaluation if enable_qa else None,
                'llm_token_usage': token_tracker.get_summary() if (enable_search or enable_qa) and 'token_tracker' in locals() else None,
                'stage_times': {  # ğŸ†• æ·»åŠ å„é˜¶æ®µè€—æ—¶
                    'search': search_time
                }
            }

            # åªæœ‰åœ¨é badcase æ¨¡å¼ä¸‹æ‰æ·»åŠ  qa æ—¶é—´
            if enable_qa and not badcase:
                result['stage_times']['qa'] = qa_time

            # æ·»åŠ å¬å›ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'statistics' in locals():
                result['recall_statistics'] = statistics

            # ğŸ†• å…³é—­ MLflow è¿è¡Œ
            if enable_mlflow and mlflow_run:
                import mlflow
                mlflow.end_run()
                logger.info("âœ… MLflow è¿è¡Œå·²ç»“æŸ")

            return result

    @classmethod
    def evaluate(cls,
                 dataset_name: str,
                 load_and_generate_md: bool = False,
                 chunks_per_file: int = 500,
                 force_regenerate: bool = False) -> Dict[str, Any]:
        """
        ç±»æ–¹æ³•ï¼šä¸­å¿ƒè¯„ä¼°å‡½æ•°

        Args:
            dataset_name: æ•°æ®é›†åç§°
            load_and_generate_md: æ˜¯å¦åŠ è½½æ•°æ®é›†å¹¶ç”Ÿæˆ markdown æ–‡ä»¶
            chunks_per_file: æ¯ä¸ª markdown æ–‡ä»¶åŒ…å«çš„ chunk æ•°é‡
            force_regenerate: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆ markdown æ–‡ä»¶

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        logger.info(f"Starting evaluation for dataset: {dataset_name}")

        results = {
            'dataset': dataset_name,
            'timestamp': datetime.now().isoformat(),
        }

        # åŠ è½½æ•°æ®é›†
        loader = DatasetLoader(dataset_name)

        # ç”Ÿæˆ markdown æ–‡ä»¶
        if load_and_generate_md:
            logger.info(f"Loading dataset and generating markdown files...")
            load_start = time.perf_counter()  # ğŸ†• å¼€å§‹LOADè®¡æ—¶
            save_result = loader.save_as_markdown(
                chunks_per_file=chunks_per_file,
                force_regenerate=force_regenerate
            )
            load_time = time.perf_counter() - load_start  # ğŸ†• è®¡ç®—LOADè€—æ—¶

            results['markdown_generation'] = {
                'output_dir': str(save_result['output_dir']),
                'stats': save_result['stats'],
                'chunks_per_file': chunks_per_file,
                'status': 'completed',
                'load_time': load_time  # ğŸ†• æ·»åŠ LOADè€—æ—¶
            }

            logger.info(f"Markdown files generated successfully at: {save_result['output_dir']}")

        # TODO: åç»­å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šè¯„ä¼°åŠŸèƒ½
        # - æ£€ç´¢è¯„ä¼°
        # - QA è¯„ä¼°
        # - ç­‰ç­‰

        return results

    def load_dataset(self, force_reload: bool = False) -> Dict[str, Any]:
        """
        åŠ è½½æ•°æ®é›†

        Args:
            force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½

        Returns:
            åŒ…å«æ•°æ®é›†ä¿¡æ¯çš„å­—å…¸
        """
        if self.dataset_loader is None or force_reload:
            logger.info(f"Loading dataset: {self.config.dataset_name}")

            self.dataset_loader = DatasetLoader(
                dataset_name=self.config.dataset_name,
                dataset_dir=self.config.dataset_dir
            )

            # åŠ è½½æ•°æ®
            self.docs = self.dataset_loader.get_docs(force_reload)
            self.questions = self.dataset_loader.get_questions(force_reload)
            self.gold_answers = self.dataset_loader.get_gold_answers(force_reload)
            self.gold_docs = self.dataset_loader.get_gold_docs(force_reload)

            # é‡‡æ ·ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if self.config.max_samples is not None:
                logger.info(f"Sampling {self.config.max_samples} samples for quick testing")
                self.questions = self.questions[:self.config.max_samples]
                self.gold_answers = self.gold_answers[:self.config.max_samples]
                if self.gold_docs is not None:
                    self.gold_docs = self.gold_docs[:self.config.max_samples]

            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self.dataset_loader.get_stats()

            if self.config.max_samples is not None:
                stats['sampled'] = True
                stats['num_sampled_questions'] = len(self.questions)

            logger.info(f"Dataset loaded successfully: {stats}")

            return {
                'dataset_name': self.config.dataset_name,
                'num_docs': len(self.docs),
                'num_questions': len(self.questions),
                'num_gold_answers': len(self.gold_answers),
                'has_gold_docs': self.gold_docs is not None,
                'stats': stats
            }

        return {
            'dataset_name': self.config.dataset_name,
            'num_docs': len(self.docs) if self.docs else 0,
            'num_questions': len(self.questions) if self.questions else 0,
        }

    def evaluate_retrieval(
        self,
        retrieved_docs_list: List[List[str]],
        top_k_list: Optional[List[int]] = None
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°æ£€ç´¢æ€§èƒ½

        Args:
            retrieved_docs_list: æ¯ä¸ªé—®é¢˜çš„æ£€ç´¢ç»“æœåˆ—è¡¨
            top_k_list: è¦è¯„ä¼°çš„top-kåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if not self.config.evaluate_retrieval:
            logger.warning("Retrieval evaluation is disabled in config")
            return {}

        if self.gold_docs is None:
            logger.warning("Gold docs not available, skipping retrieval evaluation")
            return {}

        if len(retrieved_docs_list) != len(self.gold_docs):
            raise ValueError(
                f"Length mismatch: retrieved_docs_list ({len(retrieved_docs_list)}) "
                f"vs gold_docs ({len(self.gold_docs)})"
            )

        top_k_list = top_k_list or self.config.retrieval_top_k_list

        logger.info(f"Evaluating retrieval with top_k_list: {top_k_list}")

        start_time = time.time()

        # è®¡ç®— Recall@k
        pooled_results, example_results = self.retrieval_recall_metric.calculate_metric_scores(
            gold_docs=self.gold_docs,
            retrieved_docs=retrieved_docs_list,
            k_list=top_k_list
        )

        elapsed_time = time.time() - start_time

        logger.info(f"Retrieval evaluation completed in {elapsed_time:.2f}s")
        logger.info(f"Pooled results: {pooled_results}")

        return {
            'pooled': pooled_results,
            'examples': example_results,
            'metrics': ['Recall@k'],
            'top_k_list': top_k_list,
            'num_examples': len(example_results),
            'elapsed_time': elapsed_time
        }

    def evaluate_qa(
        self,
        predicted_answers: List[str],
        aggregation_fn: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°QAæ€§èƒ½

        Args:
            predicted_answers: é¢„æµ‹çš„ç­”æ¡ˆåˆ—è¡¨
            aggregation_fn: èšåˆå‡½æ•°ï¼ˆç”¨äºå¤šä¸ªgoldç­”æ¡ˆçš„æƒ…å†µï¼‰ï¼Œé»˜è®¤ä½¿ç”¨max

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if not self.config.evaluate_qa:
            logger.warning("QA evaluation is disabled in config")
            return {}

        if len(predicted_answers) != len(self.gold_answers):
            raise ValueError(
                f"Length mismatch: predicted_answers ({len(predicted_answers)}) "
                f"vs gold_answers ({len(self.gold_answers)})"
            )

        logger.info(f"Evaluating QA performance on {len(predicted_answers)} examples")

        import numpy as np
        aggregation_fn = aggregation_fn or (
            np.max if self.config.qa_aggregation == "max" else np.mean
        )

        # å°† Set[str] è½¬æ¢ä¸º List[List[str]]
        gold_answers_list = [list(ans_set) for ans_set in self.gold_answers]

        start_time = time.time()

        # è®¡ç®— Exact Match
        em_pooled, em_examples = self.qa_em_metric.calculate_metric_scores(
            gold_answers=gold_answers_list,
            predicted_answers=predicted_answers,
            aggregation_fn=aggregation_fn
        )

        # è®¡ç®— F1 Score
        f1_pooled, f1_examples = self.qa_f1_metric.calculate_metric_scores(
            gold_answers=gold_answers_list,
            predicted_answers=predicted_answers,
            aggregation_fn=aggregation_fn
        )

        elapsed_time = time.time() - start_time

        # åˆå¹¶ç»“æœ
        pooled_results = {**em_pooled, **f1_pooled}

        # åˆå¹¶æ¯ä¸ªæ ·æœ¬çš„ç»“æœ
        example_results = []
        for em_ex, f1_ex in zip(em_examples, f1_examples):
            example_results.append({**em_ex, **f1_ex})

        logger.info(f"QA evaluation completed in {elapsed_time:.2f}s")
        logger.info(f"Pooled results: {pooled_results}")

        return {
            'pooled': pooled_results,
            'examples': example_results,
            'metrics': ['ExactMatch', 'F1'],
            'aggregation': self.config.qa_aggregation,
            'num_examples': len(example_results),
            'elapsed_time': elapsed_time
        }

    def evaluate_all(
        self,
        retrieved_docs_list: Optional[List[List[str]]] = None,
        predicted_answers: Optional[List[str]] = None,
        top_k_list: Optional[List[int]] = None
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹

        Args:
            retrieved_docs_list: æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ˆç”¨äºretrievalè¯„ä¼°ï¼‰
            predicted_answers: é¢„æµ‹ç­”æ¡ˆåˆ—è¡¨ï¼ˆç”¨äºQAè¯„ä¼°ï¼‰
            top_k_list: Recall@kä¸­çš„kå€¼åˆ—è¡¨

        Returns:
            å®Œæ•´çš„è¯„ä¼°ç»“æœ
        """
        logger.info("=" * 60)
        logger.info("Starting comprehensive evaluation")
        logger.info("=" * 60)

        # ç¡®ä¿æ•°æ®é›†å·²åŠ è½½
        if self.docs is None:
            self.load_dataset()

        results = {
            'dataset': self.config.dataset_name,
            'timestamp': datetime.now().isoformat(),
            'config': asdict(self.config),
            'num_questions': len(self.questions),
        }

        # æ£€ç´¢è¯„ä¼°
        if retrieved_docs_list is not None and self.config.evaluate_retrieval:
            logger.info("\n--- Retrieval Evaluation ---")
            retrieval_results = self.evaluate_retrieval(
                retrieved_docs_list=retrieved_docs_list,
                top_k_list=top_k_list
            )
            results['retrieval'] = retrieval_results

        # QAè¯„ä¼°
        if predicted_answers is not None and self.config.evaluate_qa:
            logger.info("\n--- QA Evaluation ---")
            qa_results = self.evaluate_qa(predicted_answers=predicted_answers)
            results['qa'] = qa_results

        # ä¿å­˜ç»“æœ
        if self.config.save_results:
            self.save_results(results)

        logger.info("=" * 60)
        logger.info("Evaluation completed")
        logger.info("=" * 60)

        return results

    def save_results(self, results: Dict[str, Any], filename: Optional[str] = None):
        """
        ä¿å­˜è¯„ä¼°ç»“æœ

        Args:
            results: è¯„ä¼°ç»“æœå­—å…¸
            filename: è¾“å‡ºæ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"eval_{self.config.dataset_name}_{timestamp}.json"

        output_path = self.output_dir / filename

        logger.info(f"Saving results to {output_path}")

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"Results saved successfully")

        # åŒæ—¶ä¿å­˜ä¸€ä¸ªæœ€æ–°ç»“æœçš„è½¯é“¾æ¥
        latest_path = self.output_dir / f"eval_{self.config.dataset_name}_latest.json"
        with open(latest_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        return output_path

    def get_questions(self) -> List[str]:
        """è·å–é—®é¢˜åˆ—è¡¨"""
        if self.questions is None:
            self.load_dataset()
        return self.questions

    def get_docs(self) -> List[str]:
        """è·å–æ–‡æ¡£åˆ—è¡¨"""
        if self.docs is None:
            self.load_dataset()
        return self.docs

    def get_gold_answers(self) -> List[Set[str]]:
        """è·å–æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨"""
        if self.gold_answers is None:
            self.load_dataset()
        return self.gold_answers

    def get_gold_docs(self) -> Optional[List[List[str]]]:
        """è·å–æ ‡å‡†æ–‡æ¡£åˆ—è¡¨"""
        if self.gold_docs is None and self.docs is None:
            self.load_dataset()
        return self.gold_docs

    async def upload_corpus(
        self,
        enable_extraction: bool = True,
        source_name: Optional[str] = None,
        source_description: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä¸Šä¼ æ•°æ®é›†çš„ markdown æ–‡ä»¶åˆ°ç³»ç»Ÿ

        Args:
            enable_extraction: æ˜¯å¦æ‰§è¡Œæå–é˜¶æ®µï¼ˆFalse åˆ™åªåŠ è½½æ–‡æ¡£ï¼‰
            source_name: ä¿¡æ¯æºåç§°ï¼Œé»˜è®¤ä¸º "{dataset_name} Corpus"
            source_description: ä¿¡æ¯æºæè¿°

        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼ŒåŒ…å« source_config_id, article_ids, ç»Ÿè®¡ä¿¡æ¯ç­‰
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹ä¸Šä¼  corpus åˆ°ç³»ç»Ÿ")
        logger.info("=" * 60)

        # è·å–å½“å‰æ¨¡å‹åç§°
        try:
            # åˆ›å»º LLM å®¢æˆ·ç«¯æ¥è·å–æ¨¡å‹é…ç½®
            llm_client = await create_llm_client(scenario='extract')
            model_name = llm_client.client.config.model if hasattr(llm_client, 'client') else llm_client.config.model

            # è¿‡æ»¤æ¨¡å‹åç§°ï¼Œåªä¿ç•™æœ€åä¸€å±‚ï¼ˆä¾‹å¦‚ï¼šQwen/qwen3 -> qwen3ï¼‰
            if '/' in model_name:
                filtered_model_name = model_name.split('/')[-1]
            else:
                filtered_model_name = model_name

            logger.info(f"å½“å‰ä½¿ç”¨æ¨¡å‹: {model_name} -> è¿‡æ»¤å: {filtered_model_name}")
        except Exception as e:
            logger.warning(f"è·å–æ¨¡å‹åç§°å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹åç§° 'default'")
            filtered_model_name = "default"

        # æ£€æŸ¥ markdown æ–‡ä»¶ç›®å½•æ˜¯å¦å­˜åœ¨
        md_dir = Path(__file__).parent / "markdown_datasets" / self.config.dataset_name
        if not md_dir.exists():
            error_msg = f"é”™è¯¯ï¼šmarkdown ç›®å½•ä¸å­˜åœ¨: {md_dir}"
            logger.error(error_msg)
            return {'status': 'error', 'message': error_msg}

        # è·å–æ‰€æœ‰ md æ–‡ä»¶
        md_files = sorted(md_dir.glob("*.md"))
        if not md_files:
            error_msg = f"é”™è¯¯ï¼šåœ¨ {md_dir} ä¸­æœªæ‰¾åˆ° .md æ–‡ä»¶"
            logger.error(error_msg)
            return {'status': 'error', 'message': error_msg}

        logger.info(f"æ‰¾åˆ° {len(md_files)} ä¸ª markdown æ–‡ä»¶")

        # 1. ç”Ÿæˆä¿¡æ¯æº ID
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        source_config_id = f"{self.config.dataset_name}-{timestamp}"

        # è®¾ç½®é»˜è®¤åç§°å’Œæè¿°
        if source_name is None:
            source_name = f"{self.config.dataset_name}-{timestamp}"  # ä½¿ç”¨æ•°æ®é›†åç§°+æ—¶é—´æˆ³
        if source_description is None:
            source_description = f"Evaluation corpus for {self.config.dataset_name} dataset"

        logger.info(f"ä¿¡æ¯æº ID: {source_config_id}")
        logger.info(f"ä¿¡æ¯æºåç§°: {source_name}")
        logger.info(f"æè¿°: {source_description}\n")

        # 2. åˆ›å»º TaskConfigï¼ˆç”¨äºä¼ é€’ source_nameï¼‰
        task_config = TaskConfig(
            task_name=f"Upload {self.config.dataset_name} Corpus",
            source_config_id=source_config_id,
            source_name=source_name
        )

        # 3. åˆ›å»º DataFlowEngine
        engine = DataFlowEngine(task_config=task_config)

        # 4. åˆå§‹åŒ–ç»Ÿè®¡
        file_results = []
        total_sections = 0
        total_events = 0
        total_time_load = 0.0
        total_time_extract = 0.0
        total_files_processed = 0

        # Tokenè¿½è¸ªå™¨ï¼ˆç”¨äºè®°å½•æ‰€æœ‰LLMè°ƒç”¨çš„tokenæ¶ˆè€—ï¼‰
        token_tracker = LLMTokenTracker()

        # å¯ç”¨LLMè°ƒç”¨è¿½è¸ªï¼ˆä½¿ç”¨monkey patchè‡ªåŠ¨æ‹¦æˆªæ‰€æœ‰LLMè°ƒç”¨ï¼‰
        if enable_extraction:
            enable_llm_tracking(token_tracker)
            logger.info("âœ… LLMè°ƒç”¨è¿½è¸ªå·²å¯ç”¨ï¼Œå°†è‡ªåŠ¨ç»Ÿè®¡tokenæ¶ˆè€—")

        # ç”¨äºè®°å½•æ¯ä¸ªæ–‡æ¡£çš„LLM Tokenç»Ÿè®¡
        file_token_stats = []

        for idx, md_file in enumerate(md_files, 1):
            logger.info(f"[{idx}/{len(md_files)}] å¤„ç†æ–‡ä»¶: {md_file.name}")
            file_size_mb = md_file.stat().st_size / 1024 / 1024
            logger.info(f"  æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")

            # Load é˜¶æ®µ - åŠ è½½æ–‡æ¡£
            load_start = time.perf_counter()
            try:
                await engine.load_async(
                    DocumentLoadConfig(
                        path=str(md_file),
                        recursive=False,
                        source_config_id=source_config_id
                    )
                )
                load_time = time.perf_counter() - load_start
                total_time_load += load_time
                logger.info(f"  âœ“ æ–‡æ¡£åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.1f} ç§’")
            except Exception as e:
                error_msg = f"æ–‡æ¡£åŠ è½½å¤±è´¥ ({md_file.name}): {e}"
                logger.error(error_msg, exc_info=True)
                file_results.append({
                    'file': md_file.name,
                    'status': 'error',
                    'message': str(e)
                })
                continue

            # è·å– Load ç»“æœ
            engine_result = engine.get_result()
            if not engine_result or not engine_result.load_result:
                error_msg = f"Load é˜¶æ®µå¤±è´¥ï¼šæ— æ³•è·å–åŠ è½½ç»“æœ ({md_file.name})"
                logger.error(error_msg)
                file_results.append({
                    'file': md_file.name,
                    'status': 'error',
                    'message': error_msg
                })
                continue

            # ä» engine_result è·å–æ•°æ®
            try:
                article_id = engine_result.article_id
                load_result = engine_result.load_result
                sections_count = load_result.stats.get("chunk_count", 0) if load_result.stats else 0
                total_sections += sections_count

                logger.info(f"  Article ID: {article_id}")
                logger.info(f"  æ–‡æ¡£ç‰‡æ®µæ•°: {sections_count}")
            except Exception as e:
                error_msg = f"è¯»å– Load ç»“æœå¤±è´¥ ({md_file.name}): {e}"
                logger.error(error_msg, exc_info=True)
                file_results.append({
                    'file': md_file.name,
                    'status': 'error',
                    'message': str(e)
                })
                continue

            events_count = 0

            # Extract é˜¶æ®µ - æå–äº‹é¡¹ï¼ˆå¯é€‰ï¼‰
            if enable_extraction:
                logger.info(f"  å¼€å§‹æå–äº‹é¡¹...")
                extract_start = time.perf_counter()

                try:
                    await engine.extract_async(
                        ExtractBaseConfig(
                            parallel=True,
                            max_concurrency=50
                        )
                    )
                    extract_time = time.perf_counter() - extract_start
                    total_time_extract += extract_time
                    logger.info(f"  âœ“ äº‹é¡¹æå–å®Œæˆï¼Œè€—æ—¶: {extract_time:.1f} ç§’")

                    # è·å– Extract ç»“æœ
                    engine_result = engine.get_result()
                    if engine_result and engine_result.extract_result:
                        extract_result = engine_result.extract_result
                        events_count = len(extract_result.data_ids) if extract_result.data_ids else 0
                        total_events += events_count
                        logger.info(f"  ç”Ÿæˆäº‹é¡¹æ•°: {events_count}")
                    else:
                        logger.warning(f"  âš ï¸  Extract ç»“æœä¸ºç©º")
                except Exception as e:
                    error_msg = f"äº‹é¡¹æå–å¤±è´¥ ({md_file.name}): {e}"
                    logger.error(error_msg, exc_info=True)
                    # æå–å¤±è´¥ä¸è¿”å›é”™è¯¯ï¼Œå› ä¸º Load å·²ç»æˆåŠŸ
            else:
                logger.info(f"  è·³è¿‡æå–é˜¶æ®µï¼ˆenable_extraction=Falseï¼‰")

            # è®°å½•æ–‡ä»¶å¤„ç†ç»“æœ
            file_results.append({
                'file': md_file.name,
                'article_id': article_id,
                'sections_count': sections_count,
                'events_count': events_count,
                'status': 'completed'
            })

            # å¦‚æœæ˜¯æå–æ¨¡å¼ï¼Œè®°å½•è¯¥æ–‡ä»¶çš„tokenç»Ÿè®¡
            if enable_extraction:
                # è®¡ç®—æœ¬æ–‡ä»¶å¤„ç†æœŸé—´çš„tokenå¢é‡
                current_stats = token_tracker.get_summary()

                # è·å–è¯¥æ–‡ä»¶å¤„ç†çš„tokenï¼ˆç®€å•çš„ç´¯åŠ æ–¹å¼ï¼‰
                # æ›´ç²¾ç¡®çš„æ–¹å¼éœ€è¦æŒ‰æ–‡ä»¶éš”ç¦»ç»Ÿè®¡ï¼Œè¿™é‡Œä½¿ç”¨ç®€åŒ–çš„ç´¯åŠ æ–¹å¼
                # å› ä¸ºæˆ‘ä»¬çš„è¿½è¸ªå™¨æ˜¯å…¨å±€çš„ï¼Œæ‰€ä»¥è¿™é‡Œåªæ˜¯è®°å½•å½“å‰æ€»æ•°
                file_tokens = {
                    'file': md_file.name,
                    'prompt_tokens': current_stats['total_prompt'],
                    'completion_tokens': current_stats['total_completion'],
                    'total_tokens': current_stats['total_tokens'],
                    'processing_time': {
                        'load_time': round(load_time, 1),
                        'extract_time': round(extract_time, 1) if 'extract_time' in locals() else 0
                    }
                }
                file_token_stats.append(file_tokens)

                # æ˜¾ç¤ºæœ¬æ–‡ä»¶çš„tokenç»Ÿè®¡ï¼ˆæ˜¾ç¤ºå¢é‡ï¼‰
                if len(file_token_stats) > 1:
                    # è®¡ç®—å¢é‡ï¼ˆç›¸å¯¹äºä¸Šä¸€ä¸ªæ–‡ä»¶ï¼‰
                    prev_file = file_token_stats[-2]
                    delta_prompt = file_tokens['prompt_tokens'] - prev_file['prompt_tokens']
                    delta_completion = file_tokens['completion_tokens'] - prev_file['completion_tokens']
                    delta_total = file_tokens['total_tokens'] - prev_file['total_tokens']
                else:
                    # ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œç›´æ¥æ˜¾ç¤ºæ€»æ•°
                    delta_prompt = file_tokens['prompt_tokens']
                    delta_completion = file_tokens['completion_tokens']
                    delta_total = file_tokens['total_tokens']

                # ä»…åœ¨æœ‰tokenæ¶ˆè€—æ—¶æ˜¾ç¤º
                if delta_total > 0:
                    logger.info(f"  ğŸ“Š LLM Tokens: è¾“å…¥={delta_prompt:,}, è¾“å‡º={delta_completion:,}, æ€»è®¡={delta_total:,}")

            total_files_processed += 1
            logger.info(f"  âœ“ æ–‡ä»¶å¤„ç†å®Œæˆ\n")

        # 5. ä¿å­˜ç»“æœåˆ° dataflow/evaluation/source/SAG/{filtered_model_name}/{dataset_name}/{timestamp}/
        source_dir = Path(__file__).parent / "source" / "SAG" / filtered_model_name / self.config.dataset_name / timestamp
        source_dir.mkdir(parents=True, exist_ok=True)

        # è·å–tokenç»Ÿè®¡
        token_summary = token_tracker.get_summary()

        result = {
            "source_config_id": source_config_id,
            "source_name": source_name,
            "source_description": source_description,
            "dataset_name": self.config.dataset_name,
            "model_name": model_name,  # æ·»åŠ åŸå§‹æ¨¡å‹åç§°
            "filtered_model_name": filtered_model_name,  # æ·»åŠ è¿‡æ»¤åçš„æ¨¡å‹åç§°
            "file_count": len(md_files),
            "successful_files": total_files_processed,
            "failed_files": len([r for r in file_results if r['status'] == 'error']),
            "total_sections_count": total_sections,
            "total_events_count": total_events,
            "processing_time": {
                "total_load_time": round(total_time_load, 1),
                "total_extract_time": round(total_time_extract, 1),
                "total_time": round(total_time_load + total_time_extract, 1)
            },
            "file_results": file_results,
            "timestamp": timestamp,
            "status": "completed",
            "extraction_enabled": enable_extraction,
            "llm_token_usage": token_summary,  # æ·»åŠ LLM tokenä½¿ç”¨ç»Ÿè®¡
            "file_token_stats": file_token_stats  # æ¯ä¸ªæ–‡ä»¶çš„tokenç»Ÿè®¡
        }

        # ä¿å­˜åˆ° source_info.json
        source_info_path = source_dir / "source_info.json"
        with open(source_info_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ä¿¡æ¯æºç»“æœå·²ä¿å­˜: {source_info_path}")

        # è¿”å›ç»“æœ
        logger.info("=" * 60)
        logger.info("âœ… Corpus ä¸Šä¼ å®Œæˆ")
        logger.info(f"  æ€»æ–‡ä»¶æ•°: {len(md_files)}")
        logger.info(f"  æˆåŠŸ: {result['successful_files']}")
        logger.info(f"  å¤±è´¥: {result['failed_files']}")
        logger.info(f"  æ€»ç‰‡æ®µæ•°: {total_sections}")
        logger.info(f"  æ€»äº‹é¡¹æ•°: {total_events}")
        logger.info(f"  ç»“æœä¿å­˜ä½ç½®: dataflow/evaluation/source/SAG/{filtered_model_name}/{self.config.dataset_name}/{timestamp}/")
        logger.info("=" * 60)

        # ä¸»åŠ¨å…³é—­æ•°æ®åº“è¿æ¥å’ŒAIå®¢æˆ·ç«¯ï¼Œé¿å… "Event loop is closed" è­¦å‘Š
        try:
            logger.info("å…³é—­æ•°æ®åº“è¿æ¥å’ŒAIå®¢æˆ·ç«¯...")
            # å…³é—­æ•°æ®åº“è¿æ¥
            await close_database()

            logger.info("âœ“ æ‰€æœ‰è¿æ¥å·²å…³é—­")
        except Exception as e:
            logger.warning(f"å…³é—­è¿æ¥æ—¶å‡ºç°è­¦å‘Š: {e}")

        return result

    def print_summary(self, results: Dict[str, Any]):
        """
        æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦

        Args:
            results: è¯„ä¼°ç»“æœå­—å…¸
        """
        logger.info("\n" + "=" * 60)
        logger.info(f"Evaluation Summary - {results['dataset']}")
        logger.info("=" * 60)
        logger.info(f"Timestamp: {results['timestamp']}")
        logger.info(f"Num Questions: {results['num_questions']}")

        if 'retrieval' in results:
            logger.info("\n--- Retrieval Results ---")
            pooled = results['retrieval']['pooled']
            for metric, score in pooled.items():
                logger.info(f"{metric}: {score:.4f}")

        if 'qa' in results:
            logger.info("\n--- QA Results ---")
            pooled = results['qa']['pooled']
            for metric, score in pooled.items():
                logger.info(f"{metric}: {score:.4f}")

        logger.info("=" * 60 + "\n")


# ä¾¿æ·å‡½æ•°
def quick_evaluate(
    dataset_name: str,
    retrieved_docs_list: Optional[List[List[str]]] = None,
    predicted_answers: Optional[List[str]] = None,
    max_samples: Optional[int] = None,
    output_dir: str = "outputs/evaluation"
) -> Dict[str, Any]:
    """
    å¿«é€Ÿè¯„ä¼°å‡½æ•°

    Args:
        dataset_name: æ•°æ®é›†åç§°
        retrieved_docs_list: æ£€ç´¢ç»“æœåˆ—è¡¨
        predicted_answers: é¢„æµ‹ç­”æ¡ˆåˆ—è¡¨
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        è¯„ä¼°ç»“æœ
    """
    config = EvaluationConfig(
        dataset_name=dataset_name,
        max_samples=max_samples,
        output_dir=output_dir
    )

    evaluator = Evaluate(config)
    results = evaluator.evaluate_all(
        retrieved_docs_list=retrieved_docs_list,
        predicted_answers=predicted_answers
    )

    evaluator.print_summary(results)

    return results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Evaluation benchmark for multi-hop QA datasets"
    )

    parser.add_argument(
        "--dataset",
        type=str,
        default="musique",
        choices=["musique", "hotpotqa", "2wikimultihopqa", "sample", "test_hotpotqa"],
        help="Dataset name to evaluate (default: musique)"
    )

    parser.add_argument(
        "--load",
        action="store_true",
        help="Load dataset and generate markdown files"
    )

    parser.add_argument(
        "--chunks-per-file",
        type=int,
        default=500,
        help="Number of chunks per markdown file (default: 500)"
    )


    parser.add_argument(
        "--upload",
        action="store_true",
        help="Upload markdown files to system (creates source in dataflow/evaluation/source/SAG/)"
    )

    parser.add_argument(
        "--show-retrieval-info",
        action="store_true",
        help="Show latest source_config_id, dataset info and retrieval information"
    )

    parser.add_argument(
        "--foundation",
        type=str,
        choices=["upload", "search", "badcase_zero", "badcase_partial"],
        help="Foundation mode: 'upload' to load and upload corpus, 'search' to perform retrieval, 'badcase_zero' or 'badcase_partial' to test badcases"
    )

    parser.add_argument(
        "--info-limit",
        type=int,
        default=None,
        help="Limit number of questions to display in --show-retrieval-info (default: show all)"
    )

    parser.add_argument(
        "--badcase",
        type=str,
        choices=["zero", "partial"],
        default=None,
        help="é‡æµ‹ç‰¹å®šçš„ badcase ç±»å‹ã€‚ä»å·²ä¿å­˜çš„ç»“æœä¸­åŠ è½½ badcase æ–‡ä»¶å¹¶åªæµ‹è¯•è¿™äº›é—®é¢˜ã€‚é€‰é¡¹ï¼š'zero'ï¼ˆé›¶å¬å›ï¼‰ï¼Œ'partial'ï¼ˆéƒ¨åˆ†å¬å›ï¼‰ã€‚ä½¿ç”¨æœ€æ–°çš„æ—¶é—´æˆ³ã€‚"
    )

    parser.add_argument(
        "--enable-search",
        action="store_true",
        help="Enable actual search functionality in --show-retrieval-info"
    )

    parser.add_argument(
        "--enable-qa",
        action="store_true",
        help="Enable QA evaluation after retrieval (requires --enable-search)"
    )

    parser.add_argument(
        "--search-verbose",
        action="store_true",
        help="Show detailed search process logs"
    )

    parser.add_argument(
        "--no-paragraphs",
        action="store_true",
        help="Hide paragraph details in --show-retrieval-info"
    )

    parser.add_argument(
        "--bench-size",
        type=int,
        default=None,
        help="Print cumulative statistics every N questions during search (default: None, print only at the end)"
    )

    parser.add_argument(
        "--enable-mlflow",
        action="store_true",
        help="Enable MLflow monitoring for recall metrics"
    )

    parser.add_argument(
        "--mlflow-uri",
        type=str,
        default="http://192.168.110.10:5050",
        help="MLflow tracking URI (default: http://192.168.110.10:5050)"
    )

    parser.add_argument(
        "--mlflow-experiment",
        type=str,
        default="benchmark_retrieval",
        help="MLflow experiment name (default: benchmark_retrieval)"
    )

    args = parser.parse_args()

    # ğŸ†• æ ¹æ® foundation æ¨¡å¼è‡ªåŠ¨è®¾ç½®å‚æ•°
    if args.foundation:
        logger.info(f"ğŸ¯ Foundation æ¨¡å¼: {args.foundation}")

        if args.foundation == "upload":
            # Upload æ¨¡å¼: è‡ªåŠ¨è®¾ç½® load å’Œ upload
            args.load = True
            args.upload = True
            args.chunks_per_file = 500  # ä½¿ç”¨é»˜è®¤å€¼
            logger.info("  è‡ªåŠ¨å¯ç”¨: --load --upload")

        elif args.foundation == "search":
            # Search æ¨¡å¼: è‡ªåŠ¨è®¾ç½® --show-retrieval-info å’Œ --enable-search
            args.show_retrieval_info = True
            args.enable_search = True
            logger.info("  è‡ªåŠ¨å¯ç”¨: --show-retrieval-info --enable-search")

        elif args.foundation == "badcase_zero":
            # Badcase Zero æ¨¡å¼: è‡ªåŠ¨è®¾ç½® search + badcase
            args.show_retrieval_info = True
            args.enable_search = True
            args.badcase = "zero"
            logger.info("  è‡ªåŠ¨å¯ç”¨: --show-retrieval-info --enable-search --badcase zero")

        elif args.foundation == "badcase_partial":
            # Badcase Partial æ¨¡å¼: è‡ªåŠ¨è®¾ç½® search + badcase
            args.show_retrieval_info = True
            args.enable_search = True
            args.badcase = "partial"
            logger.info("  è‡ªåŠ¨å¯ç”¨: --show-retrieval-info --enable-search --badcase partial")

    # å¦‚æœæŒ‡å®šäº† --show-retrieval-infoï¼Œæ˜¾ç¤ºæ£€ç´¢ä¿¡æ¯å¹¶é€€å‡º
    if args.show_retrieval_info:
        try:
            # æ£€æŸ¥å‚æ•°æœ‰æ•ˆæ€§
            if args.enable_qa and not args.enable_search:
                logger.error("é”™è¯¯: --enable-qa éœ€è¦ä¸ --enable-search ä¸€èµ·ä½¿ç”¨")
                sys.exit(1)

            # ä½¿ç”¨ asyncio.run è¿è¡Œå¼‚æ­¥å‡½æ•°
            # æ·»åŠ  badcase æ”¯æŒ
            badcase = getattr(args, 'badcase', None)

            retrieval_info = asyncio.run(Evaluate.show_retrieval_info(
                limit=args.info_limit,
                show_paragraphs=not args.no_paragraphs,
                enable_search=args.enable_search,
                search_verbose=args.search_verbose,
                enable_qa=args.enable_qa,
                dataset_name=args.dataset,
                badcase=badcase,
                bench_size=args.bench_size,
                enable_mlflow=args.enable_mlflow,
                mlflow_uri=args.mlflow_uri,
                mlflow_experiment=args.mlflow_experiment
            ))
            logger.info(f"\næ£€ç´¢ä¿¡æ¯æ˜¾ç¤ºå®Œæˆ")
            logger.info(f"å…±æ˜¾ç¤º {retrieval_info['display_limit']} ä¸ªé—®é¢˜")
            logger.info(f"ä¿¡æ¯æºID: {retrieval_info['source_info']['source_config_id']}")
            logger.info(f"æ•°æ®é›†: {retrieval_info['source_info']['dataset_name']}")
            if retrieval_info['enable_search'] and retrieval_info['search_results']:
                total_successful = sum(1 for r in retrieval_info['search_results'] if r['search_success'])
                logger.info(f"æ£€ç´¢ç»Ÿè®¡: {total_successful}/{len(retrieval_info['search_results'])} ä¸ªé—®é¢˜æ£€ç´¢æˆåŠŸ")

                # æ˜¾ç¤ºå¬å›ç‡è¯„ä¼°ç»“æœ
                if retrieval_info.get('recall_evaluation'):
                    recall_eval = retrieval_info['recall_evaluation']
                    logger.info(f"å¬å›ç‡è¯„ä¼°: åŸºäº {recall_eval['num_questions']} ä¸ªé—®é¢˜")
                    for metric, score in recall_eval['pooled_results'].items():
                        logger.info(f"  {metric}: {score:.4f} ({score*100:.2f}%)")

                # æ˜¾ç¤ºç»Ÿè®¡ç»“æœï¼ˆå…¨éƒ¨å¬å›ã€éƒ¨åˆ†å¬å›ã€é›¶å¬å›ï¼‰
                if retrieval_info.get('recall_statistics'):
                    stats = retrieval_info['recall_statistics']
                    logger.info(f"\nğŸ“Š é—®é¢˜å¬å›æƒ…å†µç»Ÿè®¡:")
                    logger.info("=" * 50)
                    logger.info(f"æ€»é—®é¢˜æ•°: {stats['total_questions']}")
                    logger.info(f"âœ… å…¨éƒ¨å¬å›: {stats['full_recall_count']} ä¸ª ({stats['full_recall_count']/stats['total_questions']*100:.1f}%)")
                    logger.info(f"âš ï¸  éƒ¨åˆ†å¬å›: {stats['partial_recall_count']} ä¸ª ({stats['partial_recall_count']/stats['total_questions']*100:.1f}%)")
                    logger.info(f"âŒ é›¶å¬å›: {stats['zero_recall_count']} ä¸ª ({stats['zero_recall_count']/stats['total_questions']*100:.1f}%)")
                    logger.info("=" * 50)

                # æ˜¾ç¤º QA è¯„ä¼°ç»“æœï¼ˆåªåœ¨é badcase æ¨¡å¼ä¸‹ï¼‰
                if not badcase and retrieval_info.get('qa_evaluation'):
                    qa_eval = retrieval_info['qa_evaluation']
                    logger.info(f"QAè¯„ä¼°: åŸºäº {qa_eval['num_questions']} ä¸ªé—®é¢˜")
                    for metric, score in qa_eval['pooled_results'].items():
                        logger.info(f"  {metric}: {score:.4f} ({score*100:.2f}%)")

                # ğŸ†• æ˜¾ç¤ºé˜¶æ®µè€—æ—¶ç»Ÿè®¡
                if retrieval_info.get('stage_times'):
                    stage_times = retrieval_info['stage_times']
                    # ä½¿ç”¨é»˜è®¤å€¼0æ¥é¿å…Noneæ¯”è¾ƒé”™è¯¯
                    search_time = stage_times.get('search', 0) or 0
                    qa_time = stage_times.get('qa', 0) or 0

                    if search_time > 0 or qa_time > 0:
                        logger.info("\né˜¶æ®µè€—æ—¶ç»Ÿè®¡:")
                        logger.info("=" * 50)
                        if search_time > 0:
                            logger.info(f"  SEARCHé˜¶æ®µ: {search_time:.1f} ç§’")
                        # åªåœ¨é badcase æ¨¡å¼ä¸‹æ˜¾ç¤º QA æ—¶é—´
                        if not badcase and qa_time > 0:
                            logger.info(f"  QAé˜¶æ®µ: {qa_time:.1f} ç§’")
                        total_time = search_time
                        if not badcase:
                            total_time += qa_time
                        if total_time > 0:
                            logger.info(f"  æ€»è®¡: {total_time:.1f} ç§’")
                        logger.info("=" * 50)
        except Exception as e:
            logger.error(f"\nè·å–æ£€ç´¢ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
        sys.exit(0)  # æ˜¾ç¤ºä¿¡æ¯åæ­£å¸¸é€€å‡º

    # è¿è¡Œè¯„ä¼°
    results = Evaluate.evaluate(
        dataset_name=args.dataset,
        load_and_generate_md=args.load,
        chunks_per_file=args.chunks_per_file,
        force_regenerate=True
    )

    # æ‰“å°ç»“æœæ‘˜è¦
    logger.info("\n" + "=" * 70)
    logger.info("Evaluation Results")
    logger.info("=" * 70)
    logger.info(f"Dataset: {results['dataset']}")
    logger.info(f"Timestamp: {results['timestamp']}")

    if 'markdown_generation' in results:
        logger.info("\n--- Markdown Generation ---")
        stats = results['markdown_generation'].get('stats', {})
        logger.info(f"Output Directory: {results['markdown_generation']['output_dir']}")
        logger.info(f"Total Chunks: {stats.get('total_chunks', 'N/A'):,}")
        logger.info(f"Number of MD Files: {stats.get('num_files', 'N/A')} ä¸ª")
        logger.info(f"Chunks Per File: {results['markdown_generation']['chunks_per_file']}")
        if stats.get('last_file_chunks') is not None:
            logger.info(f"Last File Chunks: {stats['last_file_chunks']} ä¸ª")
        logger.info(f"Status: {results['markdown_generation']['status']}")

        # ğŸ†• æ˜¾ç¤ºLOADé˜¶æ®µè€—æ—¶
        if 'load_time' in results['markdown_generation']:
            logger.info(f"LOADé˜¶æ®µè€—æ—¶: {results['markdown_generation']['load_time']:.1f} ç§’")

    logger.info("=" * 70 + "\n")

    # ä¸Šä¼ åˆ°ç³»ç»Ÿï¼ˆå¦‚æœæŒ‡å®šäº† --uploadï¼‰
    if args.upload:
        logger.info("\n" + "=" * 70)
        logger.info("å¼€å§‹ä¸Šä¼  corpus åˆ°ç³»ç»Ÿ")
        logger.info("=" * 70)

        config = EvaluationConfig(dataset_name=args.dataset)
        evaluator = Evaluate(config)

        async def upload_task():
            upload_result = await evaluator.upload_corpus(
                enable_extraction=True  # é»˜è®¤å¯ç”¨æå–
            )
            return upload_result

        # ä½¿ç”¨æ›´æ¸©å’Œçš„æ–¹å¼ç®¡ç† event loop
        try:
            upload_result = asyncio.run(upload_task())

            if upload_result['status'] == 'completed':
                logger.info("\n" + "=" * 70)
                logger.info("ä¸Šä¼ ç»“æœ")
                logger.info("=" * 70)
                logger.info(f"Source Config ID: {upload_result['source_config_id']}")
                logger.info(f"æ•°æ®é›†: {upload_result['dataset_name']}")
                logger.info(f"æ–‡ä»¶æ•°: {upload_result['file_count']}")
                logger.info(f"æˆåŠŸ: {upload_result['successful_files']}")
                logger.info(f"å¤±è´¥: {upload_result['failed_files']}")
                logger.info(f"æ€»ç‰‡æ®µæ•°: {upload_result['total_sections_count']:,}")
                logger.info(f"æ€»äº‹é¡¹æ•°: {upload_result['total_events_count']:,}")
                # è·å–æ¨¡å‹åç§°ï¼Œå¦‚æœå­˜åœ¨åˆ™ä½¿ç”¨å®Œæ•´è·¯å¾„
                filtered_model_name = upload_result.get('filtered_model_name', upload_result.get('model_name', ''))
                if filtered_model_name:
                    logger.info(f"ç»“æœä¿å­˜ä½ç½®: dataflow/evaluation/source/SAG/{filtered_model_name}/{upload_result['dataset_name']}/{upload_result['timestamp']}/")
                else:
                    logger.info(f"ç»“æœä¿å­˜ä½ç½®: dataflow/evaluation/source/SAG/{upload_result['dataset_name']}/{upload_result['timestamp']}/")

                # æ˜¾ç¤ºå¤„ç†æ—¶é—´
                if 'processing_time' in upload_result:
                    time_info = upload_result['processing_time']
                    logger.info(f"\nå¤„ç†æ—¶é—´:")
                    logger.info(f"  Load é˜¶æ®µ: {time_info['total_load_time']:.1f} ç§’")
                    logger.info(f"  Extract é˜¶æ®µ: {time_info['total_extract_time']:.1f} ç§’")
                    logger.info(f"  æ€»è®¡: {time_info['total_time']:.1f} ç§’")

                logger.info("=" * 70 + "\n")

                # æ˜¾ç¤º LLM Token æ¶ˆè€—ç»Ÿè®¡
                if 'llm_token_usage' in upload_result and upload_result['llm_token_usage']['total_tokens'] > 0:
                    token_summary = upload_result['llm_token_usage']
                    logger.info("=" * 70)
                    logger.info("LLM Token æ¶ˆè€—ç»Ÿè®¡")
                    logger.info("=" * 70)
                    logger.info(f"æ€»è°ƒç”¨æ¬¡æ•°: {token_summary['total_calls']:,}")
                    logger.info(f"æ€»è¾“å…¥ Tokens: {token_summary['total_prompt']:,}")
                    logger.info(f"æ€»è¾“å‡º Tokens: {token_summary['total_completion']:,}")
                    logger.info(f"æ€» Tokens: {token_summary['total_tokens']:,}")

                    # æŒ‰é˜¶æ®µæ˜¾ç¤º
                    if token_summary.get('stages'):
                        logger.info("\næŒ‰é˜¶æ®µç»Ÿè®¡:")
                        for stage, stats in token_summary['stages'].items():
                            logger.info(f"\n  {stage}:")
                            logger.info(f"    è°ƒç”¨æ¬¡æ•°: {stats['calls']:,}")
                            logger.info(f"    è¾“å…¥ Tokens: {stats['prompt']:,}")
                            logger.info(f"    è¾“å‡º Tokens: {stats['completion']:,}")
                            logger.info(f"    æ€»è®¡ Tokens: {stats['total']:,}")

                    logger.info("=" * 70 + "\n")
            else:
                logger.error(f"\nâŒ ä¸Šä¼ å¤±è´¥: {upload_result.get('message', 'Unknown error')}\n")
        except KeyboardInterrupt:
            logger.info("\n\nç”¨æˆ·ä¸­æ–­ä¸Šä¼ ")
            sys.exit(1)
        except Exception as e:
            logger.error(f"\nâŒ ä¸Šä¼ è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\n")
            import traceback
            traceback.print_exc()
            sys.exit(1)
