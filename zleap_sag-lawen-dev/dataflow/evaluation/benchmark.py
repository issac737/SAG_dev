"""
è¯„ä¼°åŸºå‡†æµ‹è¯•æ¨¡å—

æä¾›æ•°æ®é›†è¯„ä¼°ã€æ£€ç´¢è¯„ä¼°å’ŒQAè¯„ä¼°åŠŸèƒ½
"""

import json
import sys
import time
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Callable
from datetime import datetime
from dataclasses import dataclass, field, asdict

from dataflow.utils import get_logger
from dataflow.evaluation.utils import DatasetLoader
from dataflow.evaluation.metrics import (
    QAExactMatch,
    QAF1Score,
    RetrievalRecall,
)
from dataflow import DataFlowEngine, ExtractBaseConfig
from dataflow.modules.load.config import DocumentLoadConfig
from dataflow.db import close_database
from dataflow.engine.config import TaskConfig

# æœç´¢ç›¸å…³å¯¼å…¥
from dataflow.modules.search import SAGSearcher, SearchConfig
from dataflow.modules.search.config import (
    ReturnType, RecallConfig, ExpandConfig, RerankConfig, RecallMode
)
from dataflow.core.prompt.manager import PromptManager
from dataflow.core.storage.elasticsearch import close_es_client

logger = get_logger("evaluation.benchmark")


@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®"""

    # æ•°æ®é›†é…ç½®
    dataset_name: str = "musique"
    dataset_dir: Optional[str] = None

    # è¯„ä¼°ç±»å‹
    evaluate_retrieval: bool = True
    evaluate_qa: bool = True

    # æ£€ç´¢è¯„ä¼°é…ç½®
    retrieval_top_k_list: List[int] = field(default_factory=lambda: [1, 5, 10, 20])

    # QAè¯„ä¼°é…ç½®
    qa_aggregation: str = "max"  # max, mean, etc.

    # è¾“å‡ºé…ç½®
    save_results: bool = True
    output_dir: str = "./outputs/SAG"
    save_predictions: bool = True
    verbose: bool = True

    # é‡‡æ ·é…ç½®ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    max_samples: Optional[int] = None  # Noneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ ·æœ¬


class Evaluate:
    """
    è¯„ä¼°ç±»

    æä¾›å®Œæ•´çš„æ•°æ®é›†è¯„ä¼°åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ£€ç´¢è¯„ä¼°å’ŒQAè¯„ä¼°
    """

    def __init__(self, config: Optional[EvaluationConfig] = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            config: è¯„ä¼°é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or EvaluationConfig()

        # æ•°æ®é›†åŠ è½½å™¨
        self.dataset_loader: Optional[DatasetLoader] = None

        # æ•°æ®ç¼“å­˜
        self.docs: Optional[List[str]] = None
        self.questions: Optional[List[str]] = None
        self.gold_answers: Optional[List[Set[str]]] = None
        self.gold_docs: Optional[List[List[str]]] = None

        # è¯„ä¼°æŒ‡æ ‡
        self.qa_em_metric = QAExactMatch()
        self.qa_f1_metric = QAF1Score()
        self.retrieval_recall_metric = RetrievalRecall()

        # è¾“å‡ºç›®å½•
        self.output_dir = Path(self.config.output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"Initialized Evaluate with config: {asdict(self.config)}")

    @classmethod
    def load_latest_source_info(cls) -> Dict[str, Any]:
        """
        ä» dataflow/evaluation/source/SAG è·¯å¾„ä¸‹åŠ è½½æœ€æ–°æ—¶é—´æˆ³æ–‡ä»¶å¤¹çš„ source_info.json
        
        Returns:
            åŒ…å« source_config_id å’Œ dataset_name çš„å­—å…¸
        """
        import json
        from pathlib import Path
        
        # è·å– SAG ç›®å½•è·¯å¾„
        current_file = Path(__file__)
        sag_dir = current_file.parent / "source" / "SAG"
        
        if not sag_dir.exists():
            raise FileNotFoundError(f"SAG directory not found: {sag_dir}")
        
        # è·å–æ‰€æœ‰æ—¶é—´æˆ³æ–‡ä»¶å¤¹
        timestamp_dirs = [d for d in sag_dir.iterdir() if d.is_dir()]
        
        if not timestamp_dirs:
            raise FileNotFoundError(f"No timestamp directories found in: {sag_dir}")
        
        # æŒ‰æ—¶é—´æˆ³æ’åºï¼Œè·å–æœ€æ–°çš„
        latest_dir = max(timestamp_dirs, key=lambda d: d.name)
        
        # è¯»å– source_info.json
        source_info_path = latest_dir / "source_info.json"
        if not source_info_path.exists():
            raise FileNotFoundError(f"source_info.json not found in: {latest_dir}")
        
        logger.info(f"Loading source info from: {source_info_path}")
        
        with open(source_info_path, 'r', encoding='utf-8') as f:
            source_info = json.load(f)
        
        return {
            'source_config_id': source_info.get('source_config_id'),
            'dataset_name': source_info.get('dataset_name'),
            'timestamp': source_info.get('timestamp'),
            'source_name': source_info.get('source_name'),
            'file_path': str(source_info_path)
        }
    
    @classmethod
    def load_dataset_info(cls, dataset_name: str) -> Dict[str, Any]:
        """
        ä» dataflow/evaluation/dataset ç›®å½•åŠ è½½æŒ‡å®šæ•°æ®é›†çš„ä¿¡æ¯
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
        
        Returns:
            åŒ…å« questions, answers, paragraphs ä¿¡æ¯çš„å­—å…¸
        """
        from dataflow.evaluation.utils import DatasetLoader
        
        # ä½¿ç”¨ DatasetLoader åŠ è½½æ•°æ®é›†
        loader = DatasetLoader(dataset_name)
        
        # åŠ è½½åŸå§‹æ ·æœ¬æ•°æ®
        samples = loader.load_samples()
        
        questions = []
        answers = []
        all_paragraphs = []
        
        for sample in samples:
            questions.append(sample.get('question', ''))
            answers.append(sample.get('answer', []))
            all_paragraphs.append(sample.get('paragraphs', []))
        
        return {
            'dataset_name': dataset_name,
            'total_questions': len(questions),
            'questions': questions,
            'answers': answers,
            'paragraphs': all_paragraphs,
            'samples': samples
        }
    
    @classmethod
    async def search_questions(cls, source_config_id: str, questions: List[str], limit: Optional[int] = None, verbose: bool = False) -> List[Dict[str, Any]]:
        """
        å¯¹é—®é¢˜åˆ—è¡¨è¿›è¡Œæ£€ç´¢ï¼Œè¿”å›æ£€ç´¢ç»“æœ
        
        Args:
            source_config_id: æ•°æ®æºID
            questions: é—®é¢˜åˆ—è¡¨
            limit: é™åˆ¶å¤„ç†çš„é—®é¢˜æ•°é‡
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        import logging
        
        logger.info("Initializing searcher...")
        
        # é…ç½®æ—¥å¿—çº§åˆ«
        if verbose:
            logger.info("å¯ç”¨è¯¦ç»†æ—¥å¿—æ¨¡å¼...")
            
            # åˆ›å»ºæ§åˆ¶å°handler
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(formatter)
            
            # è®¾ç½®å„ä¸ªæ¨¡å—çš„æ—¥å¿—çº§åˆ«
            loggers_to_configure = [
                'dataflow',
                'dataflow.modules.search', 
                'dataflow.modules.search.recall',
                'dataflow.modules.search.expand', 
                'dataflow.modules.search.rerank',
                'dataflow.search.rerank.pagerank',
                'dataflow.search.pagerank'
            ]
            
            for logger_name in loggers_to_configure:
                logger_obj = logging.getLogger(logger_name)
                logger_obj.setLevel(logging.INFO)
                # æ¸…é™¤ç°æœ‰handlersé¿å…é‡å¤
                logger_obj.handlers = []
                logger_obj.addHandler(console_handler)
                logger_obj.propagate = False
            # é…ç½®æ ¹loggerä»¥ç¡®ä¿æ‰€æœ‰æ—¥å¿—éƒ½èƒ½è¾“å‡º
            root_logger = logging.getLogger()
            root_logger.setLevel(logging.INFO)
            if not root_logger.handlers:
                root_logger.addHandler(console_handler)
                
        else:
            logger.info("ä½¿ç”¨é»˜è®¤æ—¥å¿—çº§åˆ«(WARNING)...")
            # åªæ˜¾ç¤ºWARNINGçº§åˆ«çš„æ—¥å¿—
            for logger_name in ['dataflow', 'dataflow.modules.search', 'elasticsearch', 'urllib3']:
                logging.getLogger(logger_name).setLevel(logging.WARNING)
        
        # åˆå§‹åŒ–æœç´¢å™¨
        prompt_manager = PromptManager()
        searcher = SAGSearcher(prompt_manager=prompt_manager)
        
        # åº”ç”¨é™åˆ¶
        process_questions = questions[:limit] if limit else questions
        logger.info(f"Processing {len(process_questions)} questions for search")
        
        search_results = []
        
        for i, question in enumerate(process_questions, 1):
            if verbose:
                logger.info(f"\n[{i}/{len(process_questions)}] Searching: {question}")
            
            # é…ç½®æœç´¢å‚æ•°
            search_config = SearchConfig(
                query=question,
                source_config_id=source_config_id,
                return_type=ReturnType.PARAGRAPH,
                recall=RecallConfig(
                    use_fast_mode=False,
                    vector_top_k=50,
                    max_entities=50,
                    recall_mode=RecallMode.FUZZY,
                    entity_similarity_threshold=0.3,
                    entity_weight_threshold=0.2
                ),
                expand=ExpandConfig(max_hops=3),
                rerank=RerankConfig(
                    max_results=10,
                    score_threshold=0.45,
                    strategy="pagerank"
                )
            )
            
            try:
                # æ‰§è¡Œæœç´¢
                search_result = await searcher.search(search_config)
                sections = search_result.get("sections", [])
                
                # æ®µè½å»é‡
                seen_chunk_ids = set()
                unique_sections = []
                for section in sections:
                    chunk_id = section.get('chunk_id')
                    if chunk_id and chunk_id not in seen_chunk_ids:
                        seen_chunk_ids.add(chunk_id)
                        unique_sections.append(section)
                
                search_results.append({
                    'question_index': i,
                    'question': question,
                    'sections': unique_sections,
                    'total_sections': len(unique_sections),
                    'search_success': True
                })
                
                if verbose:
                    logger.info(f"   Found {len(unique_sections)} unique sections")
                    
            except Exception as e:
                logger.error(f"   Search failed: {e}")
                search_results.append({
                    'question_index': i,
                    'question': question,
                    'sections': [],
                    'total_sections': 0,
                    'search_success': False,
                    'error': str(e)
                })
        
        # æ¸…ç†èµ„æº
        try:
            await close_es_client()
        except Exception as e:
            logger.warning(f"Error closing ES client: {e}")
        
        logger.info(f"Search completed for {len(search_results)} questions")
        return search_results
    
    @classmethod
    def _check_content_similarity(cls, gold_content: str, retrieved_content: str) -> bool:
        """
        æ£€æŸ¥å†…å®¹ç›¸ä¼¼æ€§
        
        Args:
            gold_content: æ ‡å‡†å†…å®¹
            retrieved_content: æ£€ç´¢åˆ°çš„å†…å®¹
            
        Returns:
            æ˜¯å¦åŒ¹é…
        """
        # ç®€å•çš„å†…å®¹åŒ¹é…é€»è¾‘ï¼šæ£€æŸ¥å…³é”®è¯æ˜¯å¦å­˜åœ¨
        gold_words = set(gold_content.lower().split())
        retrieved_words = set(retrieved_content.lower().split())
        
        # è®¡ç®—äº¤é›†çš„æ¯”ä¾‹
        if len(gold_words) == 0:
            return False
            
        intersection = gold_words & retrieved_words
        similarity_ratio = len(intersection) / len(gold_words)
        
        # å¦‚æœäº¤é›†è¶…è¿‡50%ï¼Œè®¤ä¸ºåŒ¹é…æˆåŠŸ
        return similarity_ratio >= 0.5
    
    @classmethod
    async def show_retrieval_info(cls, limit: Optional[int] = None, show_paragraphs: bool = True, enable_search: bool = False, search_verbose: bool = False) -> Dict[str, Any]:
        """
        æ˜¾ç¤ºæ£€ç´¢ç›¸å…³ä¿¡æ¯ï¼šæœ€æ–°çš„ source_config_idã€dataset_name å’Œæ•°æ®é›†å†…å®¹ï¼Œå¯é€‰è¿›è¡Œå®é™…æ£€ç´¢
        
        Args:
            limit: é™åˆ¶æ˜¾ç¤ºçš„é—®é¢˜æ•°é‡ï¼ŒNoneè¡¨ç¤ºæ˜¾ç¤ºå…¨éƒ¨
            show_paragraphs: æ˜¯å¦æ˜¾ç¤º paragraphs è¯¦ç»†ä¿¡æ¯
            enable_search: æ˜¯å¦å¯ç”¨å®é™…æ£€ç´¢åŠŸèƒ½
            search_verbose: æ£€ç´¢è¿‡ç¨‹æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        
        Returns:
            å®Œæ•´çš„æ£€ç´¢ä¿¡æ¯å­—å…¸
        """
        logger.info("Loading latest source information...")
        
        # 1. åŠ è½½æœ€æ–°çš„ source_info
        try:
            source_info = cls.load_latest_source_info()
            logger.info("Successfully loaded source information")
        except Exception as e:
            logger.error(f"Failed to load source info: {e}")
            raise
        
        # 2. åŠ è½½å¯¹åº”çš„æ•°æ®é›†ä¿¡æ¯
        dataset_name = source_info['dataset_name']
        logger.info(f"Loading dataset: {dataset_name}")
        
        try:
            dataset_info = cls.load_dataset_info(dataset_name)
            logger.info(f"Successfully loaded dataset with {dataset_info['total_questions']} questions")
        except Exception as e:
            logger.error(f"Failed to load dataset {dataset_name}: {e}")
            raise
        
        # 3. æ‰“å°ä¿¡æ¯
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ” æ£€ç´¢ä¿¡æ¯æ¦‚è§ˆ")
        logger.info("=" * 80)
        logger.info(f"ğŸ“ ä¿¡æ¯æºæ–‡ä»¶: {source_info['file_path']}")
        logger.info(f"ğŸ†” ä¿¡æ¯æºID: {source_info['source_config_id']}")
        logger.info(f"ğŸ“Š æ•°æ®é›†åç§°: {source_info['dataset_name']}")
        logger.info(f"ğŸ“… æ—¶é—´æˆ³: {source_info['timestamp']}")
        logger.info(f"ğŸ“ ä¿¡æ¯æºåç§°: {source_info['source_name']}")
        logger.info(f"â“ é—®é¢˜æ€»æ•°: {dataset_info['total_questions']}")
        
        # 4. æ˜¾ç¤ºé—®é¢˜å’Œç­”æ¡ˆä¿¡æ¯
        questions = dataset_info['questions']
        answers = dataset_info['answers'] 
        paragraphs = dataset_info['paragraphs']
        
        # åº”ç”¨é™åˆ¶
        display_limit = min(len(questions), limit) if limit else len(questions)
        
        logger.info(f"\nğŸ“‹ æ˜¾ç¤ºå‰ {display_limit} ä¸ªé—®é¢˜:")
        logger.info("=" * 80)
        
        for i in range(display_limit):
            logger.info(f"\n[é—®é¢˜ {i+1}]")
            logger.info(f"é—®é¢˜: {questions[i]}")
            logger.info(f"ç­”æ¡ˆ: {answers[i]}")
            
            if show_paragraphs and i < len(paragraphs):
                para_list = paragraphs[i]
                logger.info(f"æ®µè½ä¿¡æ¯ ({len(para_list)} ä¸ª):")
                
                for j, para in enumerate(para_list):
                    title = para.get('title', 'N/A')
                    text = para.get('text', 'N/A')
                    is_supporting = para.get('is_supporting', False)
                    
                    logger.info(f"   [{j+1}] æ ‡é¢˜: {title}")
                    logger.info(f"       æ”¯æŒæ€§: {'æ˜¯' if is_supporting else 'å¦'}")
                    logger.info(f"       å†…å®¹: {text[:200]}..." if len(text) > 200 else f"       å†…å®¹: {text}")
            
            if i < display_limit - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ª
                logger.info("-" * 60)
        
        logger.info("\n" + "=" * 80)
        
        # 5. å¦‚æœå¯ç”¨äº†æœç´¢ï¼Œæ‰§è¡Œå®é™…æ£€ç´¢
        search_results = None
        recall_evaluation = None
        if enable_search:
            logger.info(f"\nå¯åŠ¨å®é™…æ£€ç´¢ (æ•°æ®æº: {source_info['source_config_id']})...")
            logger.info("=" * 80)
            
            try:
                # æ‰§è¡Œæ£€ç´¢
                search_results = await cls.search_questions(
                    source_config_id=source_info['source_config_id'],
                    questions=questions,
                    limit=display_limit,
                    verbose=search_verbose
                )
                
                # æ£€æŸ¥æ˜¯å¦æœ‰paragraphsæ•°æ®ï¼Œå¦‚æœæœ‰åˆ™è¿›è¡Œå¬å›ç‡è¯„ä¼°
                has_supporting_paragraphs = any(
                    paragraphs[i] and any(p.get('is_supporting', False) for p in paragraphs[i] if p)
                    for i in range(min(len(paragraphs), display_limit))
                    if i < len(paragraphs) and paragraphs[i]
                )
                
                if has_supporting_paragraphs:
                    logger.info(f"\næ­£åœ¨è¿›è¡Œå¬å›ç‡è¯„ä¼°...")
                    # å‡†å¤‡è¯„ä¼°æ•°æ®
                    gold_docs_list = []
                    retrieved_docs_list = []
                    
                    for i, result in enumerate(search_results):
                        if i < len(paragraphs) and paragraphs[i]:
                            # è·å–æ ‡å‡†ç­”æ¡ˆæ®µè½(æ”¯æŒæ€§æ®µè½çš„æ ‡é¢˜+å†…å®¹)
                            supporting_docs = []
                            for para in paragraphs[i]:
                                if para.get('is_supporting', False):
                                    supporting_docs.append({
                                        'title': para['title'],
                                        'content': para['text'][:500]  # å–å‰500å­—ç¬¦ç”¨äºåŒ¹é…
                                    })
                            
                            # è·å–æ£€ç´¢ç»“æœæ®µè½(æ¸…ç†markdownæ ‡è®°)
                            retrieved_docs = []
                            for section in result['sections']:
                                heading = section.get('heading', '')
                                content = section.get('content', '')
                                # æ¸…ç†markdownæ ‡è®° (# å‰ç¼€) å’Œé¦–å°¾ç©ºæ ¼
                                clean_heading = heading.lstrip('#').strip()
                                clean_content = content.strip()[:500]  # å–å‰500å­—ç¬¦ç”¨äºåŒ¹é…
                                if clean_heading and clean_content:
                                    retrieved_docs.append({
                                        'title': clean_heading,
                                        'content': clean_content
                                    })
                            
                            # è¿›è¡Œæ ‡é¢˜+å†…å®¹çš„åŒé‡åŒ¹é…
                            matched_docs = []
                            for gold_doc in supporting_docs:
                                for retrieved_doc in retrieved_docs:
                                    title_match = gold_doc['title'].strip().lower() == retrieved_doc['title'].strip().lower()
                                    # å†…å®¹åŒ¹é…ï¼šæ£€æŸ¥æ£€ç´¢å†…å®¹æ˜¯å¦åŒ…å«æ ‡å‡†å†…å®¹çš„å…³é”®ä¿¡æ¯
                                    content_match = cls._check_content_similarity(
                                        gold_doc['content'], 
                                        retrieved_doc['content']
                                    )
                                    
                                    if title_match and content_match:
                                        matched_docs.append(gold_doc['title'])
                                        break  # æ‰¾åˆ°åŒ¹é…åé€€å‡ºå†…å¾ªç¯
                            
                            # ä¸ºäº†å…¼å®¹RetrievalRecallï¼Œä»ç„¶ä¼ é€’æ ‡é¢˜åˆ—è¡¨
                            supporting_titles = [doc['title'] for doc in supporting_docs]
                            gold_docs_list.append(matched_docs)  # ä¼ é€’åŒ¹é…æˆåŠŸçš„æ–‡æ¡£
                            retrieved_docs_list.append([doc['title'] for doc in retrieved_docs])
                            
                            # è°ƒè¯•è¾“å‡º
                            logger.info(f"\n[DEBUG] é—®é¢˜ {i+1}:")
                            logger.info(f"  æ ‡å‡†æ–‡æ¡£: {supporting_titles}")
                            logger.info(f"  æ£€ç´¢æ–‡æ¡£: {[doc['title'] for doc in retrieved_docs[:5]]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
                            logger.info(f"  åŒ¹é…æˆåŠŸ: {matched_docs}")
                            logger.info(f"  åŒ¹é…ç‡: {len(matched_docs)}/{len(supporting_titles)}")
                    
                    # ä½¿ç”¨RetrievalRecallè¿›è¡Œè¯„ä¼°
                    if gold_docs_list and retrieved_docs_list:
                        from dataflow.evaluation.metrics import RetrievalRecall
                        recall_metric = RetrievalRecall()
                        
                        pooled_recall, example_recalls = recall_metric.calculate_metric_scores(
                            gold_docs=gold_docs_list,
                            retrieved_docs=retrieved_docs_list,
                            k_list=[1, 3, 5, 10]
                        )
                        
                        recall_evaluation = {
                            'pooled_results': pooled_recall,
                            'example_results': example_recalls,
                            'num_questions': len(gold_docs_list)
                        }
                        
                        logger.info(f"\nå¬å›ç‡è¯„ä¼°ç»“æœ:")
                        logger.info("=" * 50)
                        for metric, score in pooled_recall.items():
                            logger.info(f"{metric}: {score:.4f} ({score*100:.2f}%)")
                        logger.info("=" * 50)
                
                # æ˜¾ç¤ºæ£€ç´¢ç»“æœ
                logger.info(f"\næ£€ç´¢ç»“æœæ¦‚è¦:")
                logger.info("=" * 80)
                
                total_successful = sum(1 for r in search_results if r['search_success'])
                total_sections = sum(r['total_sections'] for r in search_results)
                
                logger.info(f"æˆåŠŸæ£€ç´¢: {total_successful}/{len(search_results)} ä¸ªé—®é¢˜")
                logger.info(f"æ€»æ£€ç´¢æ®µè½æ•°: {total_sections} ä¸ª")
                
                # æ˜¾ç¤ºæ¯ä¸ªé—®é¢˜çš„æ£€ç´¢ç»“æœ
                for result in search_results:
                    logger.info(f"\n[é—®é¢˜ {result['question_index']}] {result['question']}")
                    
                    if result['search_success']:
                        sections = result['sections']
                        logger.info(f"æ£€ç´¢åˆ° {len(sections)} ä¸ªç›¸å…³æ®µè½:")
                        
                        for j, section in enumerate(sections[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                            heading = section.get('heading', 'N/A')
                            content = section.get('content', '').replace('\n', ' ')
                            
                            # è·å–å¾—åˆ†ä¿¡æ¯
                            cosine_score = section.get('original_score', 0.0)
                            pagerank_score = section.get('weight') or section.get('pagerank', 0.0)
                            
                            logger.info(f"   [{j}] æ ‡é¢˜: {heading}")
                            logger.info(f"       ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_score:.4f} | PageRank: {pagerank_score:.4f}")
                            logger.info(f"       å†…å®¹: {content[:200]}..." if len(content) > 200 else f"       å†…å®¹: {content}")
                            
                        if len(sections) > 5:
                            logger.info(f"   ... è¿˜æœ‰ {len(sections) - 5} ä¸ªæ®µè½")
                    else:
                        error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
                        logger.info(f"æ£€ç´¢å¤±è´¥: {error_msg}")
                    
                    if result != search_results[-1]:  # ä¸æ˜¯æœ€åä¸€ä¸ª
                        logger.info("-" * 60)
                
                logger.info("\n" + "=" * 80)
                
            except Exception as e:
                logger.error(f"æ£€ç´¢è¿‡ç¨‹å‡ºç°é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
        
        # 6. è¿”å›å®Œæ•´ä¿¡æ¯
        return {
            'source_info': source_info,
            'dataset_info': dataset_info,
            'display_limit': display_limit,
            'show_paragraphs': show_paragraphs,
            'enable_search': enable_search,
            'search_results': search_results,
            'recall_evaluation': recall_evaluation
        }

    @classmethod
    def evaluate(cls,
                 dataset_name: str,
                 load_and_generate_md: bool = False,
                 chunks_per_file: int = 500,
                 force_regenerate: bool = False) -> Dict[str, Any]:
        """
        ç±»æ–¹æ³•ï¼šä¸­å¿ƒè¯„ä¼°å‡½æ•°

        Args:
            dataset_name: æ•°æ®é›†åç§°
            load_and_generate_md: æ˜¯å¦åŠ è½½æ•°æ®é›†å¹¶ç”Ÿæˆ markdown æ–‡ä»¶
            chunks_per_file: æ¯ä¸ª markdown æ–‡ä»¶åŒ…å«çš„ chunk æ•°é‡
            force_regenerate: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆ markdown æ–‡ä»¶

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        logger.info(f"Starting evaluation for dataset: {dataset_name}")

        results = {
            'dataset': dataset_name,
            'timestamp': datetime.now().isoformat(),
        }

        # åŠ è½½æ•°æ®é›†
        loader = DatasetLoader(dataset_name)

        # ç”Ÿæˆ markdown æ–‡ä»¶
        if load_and_generate_md:
            logger.info(f"Loading dataset and generating markdown files...")
            save_result = loader.save_as_markdown(
                chunks_per_file=chunks_per_file,
                force_regenerate=force_regenerate
            )

            results['markdown_generation'] = {
                'output_dir': str(save_result['output_dir']),
                'stats': save_result['stats'],
                'chunks_per_file': chunks_per_file,
                'status': 'completed'
            }

            logger.info(f"Markdown files generated successfully at: {save_result['output_dir']}")

        # TODO: åç»­å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šè¯„ä¼°åŠŸèƒ½
        # - æ£€ç´¢è¯„ä¼°
        # - QA è¯„ä¼°
        # - ç­‰ç­‰

        return results

    def load_dataset(self, force_reload: bool = False) -> Dict[str, Any]:
        """
        åŠ è½½æ•°æ®é›†

        Args:
            force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½

        Returns:
            åŒ…å«æ•°æ®é›†ä¿¡æ¯çš„å­—å…¸
        """
        if self.dataset_loader is None or force_reload:
            logger.info(f"Loading dataset: {self.config.dataset_name}")

            self.dataset_loader = DatasetLoader(
                dataset_name=self.config.dataset_name,
                dataset_dir=self.config.dataset_dir
            )

            # åŠ è½½æ•°æ®
            self.docs = self.dataset_loader.get_docs(force_reload)
            self.questions = self.dataset_loader.get_questions(force_reload)
            self.gold_answers = self.dataset_loader.get_gold_answers(force_reload)
            self.gold_docs = self.dataset_loader.get_gold_docs(force_reload)

            # é‡‡æ ·ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
            if self.config.max_samples is not None:
                logger.info(f"Sampling {self.config.max_samples} samples for quick testing")
                self.questions = self.questions[:self.config.max_samples]
                self.gold_answers = self.gold_answers[:self.config.max_samples]
                if self.gold_docs is not None:
                    self.gold_docs = self.gold_docs[:self.config.max_samples]

            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self.dataset_loader.get_stats()

            if self.config.max_samples is not None:
                stats['sampled'] = True
                stats['num_sampled_questions'] = len(self.questions)

            logger.info(f"Dataset loaded successfully: {stats}")

            return {
                'dataset_name': self.config.dataset_name,
                'num_docs': len(self.docs),
                'num_questions': len(self.questions),
                'num_gold_answers': len(self.gold_answers),
                'has_gold_docs': self.gold_docs is not None,
                'stats': stats
            }

        return {
            'dataset_name': self.config.dataset_name,
            'num_docs': len(self.docs) if self.docs else 0,
            'num_questions': len(self.questions) if self.questions else 0,
        }

    def evaluate_retrieval(
        self,
        retrieved_docs_list: List[List[str]],
        top_k_list: Optional[List[int]] = None
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°æ£€ç´¢æ€§èƒ½

        Args:
            retrieved_docs_list: æ¯ä¸ªé—®é¢˜çš„æ£€ç´¢ç»“æœåˆ—è¡¨
            top_k_list: è¦è¯„ä¼°çš„top-kåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if not self.config.evaluate_retrieval:
            logger.warning("Retrieval evaluation is disabled in config")
            return {}

        if self.gold_docs is None:
            logger.warning("Gold docs not available, skipping retrieval evaluation")
            return {}

        if len(retrieved_docs_list) != len(self.gold_docs):
            raise ValueError(
                f"Length mismatch: retrieved_docs_list ({len(retrieved_docs_list)}) "
                f"vs gold_docs ({len(self.gold_docs)})"
            )

        top_k_list = top_k_list or self.config.retrieval_top_k_list

        logger.info(f"Evaluating retrieval with top_k_list: {top_k_list}")

        start_time = time.time()

        # è®¡ç®— Recall@k
        pooled_results, example_results = self.retrieval_recall_metric.calculate_metric_scores(
            gold_docs=self.gold_docs,
            retrieved_docs=retrieved_docs_list,
            k_list=top_k_list
        )

        elapsed_time = time.time() - start_time

        logger.info(f"Retrieval evaluation completed in {elapsed_time:.2f}s")
        logger.info(f"Pooled results: {pooled_results}")

        return {
            'pooled': pooled_results,
            'examples': example_results,
            'metrics': ['Recall@k'],
            'top_k_list': top_k_list,
            'num_examples': len(example_results),
            'elapsed_time': elapsed_time
        }

    def evaluate_qa(
        self,
        predicted_answers: List[str],
        aggregation_fn: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°QAæ€§èƒ½

        Args:
            predicted_answers: é¢„æµ‹çš„ç­”æ¡ˆåˆ—è¡¨
            aggregation_fn: èšåˆå‡½æ•°ï¼ˆç”¨äºå¤šä¸ªgoldç­”æ¡ˆçš„æƒ…å†µï¼‰ï¼Œé»˜è®¤ä½¿ç”¨max

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if not self.config.evaluate_qa:
            logger.warning("QA evaluation is disabled in config")
            return {}

        if len(predicted_answers) != len(self.gold_answers):
            raise ValueError(
                f"Length mismatch: predicted_answers ({len(predicted_answers)}) "
                f"vs gold_answers ({len(self.gold_answers)})"
            )

        logger.info(f"Evaluating QA performance on {len(predicted_answers)} examples")

        import numpy as np
        aggregation_fn = aggregation_fn or (
            np.max if self.config.qa_aggregation == "max" else np.mean
        )

        # å°† Set[str] è½¬æ¢ä¸º List[List[str]]
        gold_answers_list = [list(ans_set) for ans_set in self.gold_answers]

        start_time = time.time()

        # è®¡ç®— Exact Match
        em_pooled, em_examples = self.qa_em_metric.calculate_metric_scores(
            gold_answers=gold_answers_list,
            predicted_answers=predicted_answers,
            aggregation_fn=aggregation_fn
        )

        # è®¡ç®— F1 Score
        f1_pooled, f1_examples = self.qa_f1_metric.calculate_metric_scores(
            gold_answers=gold_answers_list,
            predicted_answers=predicted_answers,
            aggregation_fn=aggregation_fn
        )

        elapsed_time = time.time() - start_time

        # åˆå¹¶ç»“æœ
        pooled_results = {**em_pooled, **f1_pooled}

        # åˆå¹¶æ¯ä¸ªæ ·æœ¬çš„ç»“æœ
        example_results = []
        for em_ex, f1_ex in zip(em_examples, f1_examples):
            example_results.append({**em_ex, **f1_ex})

        logger.info(f"QA evaluation completed in {elapsed_time:.2f}s")
        logger.info(f"Pooled results: {pooled_results}")

        return {
            'pooled': pooled_results,
            'examples': example_results,
            'metrics': ['ExactMatch', 'F1'],
            'aggregation': self.config.qa_aggregation,
            'num_examples': len(example_results),
            'elapsed_time': elapsed_time
        }

    def evaluate_all(
        self,
        retrieved_docs_list: Optional[List[List[str]]] = None,
        predicted_answers: Optional[List[str]] = None,
        top_k_list: Optional[List[int]] = None
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹

        Args:
            retrieved_docs_list: æ£€ç´¢ç»“æœåˆ—è¡¨ï¼ˆç”¨äºretrievalè¯„ä¼°ï¼‰
            predicted_answers: é¢„æµ‹ç­”æ¡ˆåˆ—è¡¨ï¼ˆç”¨äºQAè¯„ä¼°ï¼‰
            top_k_list: Recall@kä¸­çš„kå€¼åˆ—è¡¨

        Returns:
            å®Œæ•´çš„è¯„ä¼°ç»“æœ
        """
        logger.info("=" * 60)
        logger.info("Starting comprehensive evaluation")
        logger.info("=" * 60)

        # ç¡®ä¿æ•°æ®é›†å·²åŠ è½½
        if self.docs is None:
            self.load_dataset()

        results = {
            'dataset': self.config.dataset_name,
            'timestamp': datetime.now().isoformat(),
            'config': asdict(self.config),
            'num_questions': len(self.questions),
        }

        # æ£€ç´¢è¯„ä¼°
        if retrieved_docs_list is not None and self.config.evaluate_retrieval:
            logger.info("\n--- Retrieval Evaluation ---")
            retrieval_results = self.evaluate_retrieval(
                retrieved_docs_list=retrieved_docs_list,
                top_k_list=top_k_list
            )
            results['retrieval'] = retrieval_results

        # QAè¯„ä¼°
        if predicted_answers is not None and self.config.evaluate_qa:
            logger.info("\n--- QA Evaluation ---")
            qa_results = self.evaluate_qa(predicted_answers=predicted_answers)
            results['qa'] = qa_results

        # ä¿å­˜ç»“æœ
        if self.config.save_results:
            self.save_results(results)

        logger.info("=" * 60)
        logger.info("Evaluation completed")
        logger.info("=" * 60)

        return results

    def save_results(self, results: Dict[str, Any], filename: Optional[str] = None):
        """
        ä¿å­˜è¯„ä¼°ç»“æœ

        Args:
            results: è¯„ä¼°ç»“æœå­—å…¸
            filename: è¾“å‡ºæ–‡ä»¶åï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"eval_{self.config.dataset_name}_{timestamp}.json"

        output_path = self.output_dir / filename

        logger.info(f"Saving results to {output_path}")

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"Results saved successfully")

        # åŒæ—¶ä¿å­˜ä¸€ä¸ªæœ€æ–°ç»“æœçš„è½¯é“¾æ¥
        latest_path = self.output_dir / f"eval_{self.config.dataset_name}_latest.json"
        with open(latest_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        return output_path

    def get_questions(self) -> List[str]:
        """è·å–é—®é¢˜åˆ—è¡¨"""
        if self.questions is None:
            self.load_dataset()
        return self.questions

    def get_docs(self) -> List[str]:
        """è·å–æ–‡æ¡£åˆ—è¡¨"""
        if self.docs is None:
            self.load_dataset()
        return self.docs

    def get_gold_answers(self) -> List[Set[str]]:
        """è·å–æ ‡å‡†ç­”æ¡ˆåˆ—è¡¨"""
        if self.gold_answers is None:
            self.load_dataset()
        return self.gold_answers

    def get_gold_docs(self) -> Optional[List[List[str]]]:
        """è·å–æ ‡å‡†æ–‡æ¡£åˆ—è¡¨"""
        if self.gold_docs is None and self.docs is None:
            self.load_dataset()
        return self.gold_docs

    async def upload_corpus(
        self,
        enable_extraction: bool = True,
        source_name: Optional[str] = None,
        source_description: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä¸Šä¼ æ•°æ®é›†çš„ markdown æ–‡ä»¶åˆ°ç³»ç»Ÿ

        Args:
            enable_extraction: æ˜¯å¦æ‰§è¡Œæå–é˜¶æ®µï¼ˆFalse åˆ™åªåŠ è½½æ–‡æ¡£ï¼‰
            source_name: ä¿¡æ¯æºåç§°ï¼Œé»˜è®¤ä¸º "{dataset_name} Corpus"
            source_description: ä¿¡æ¯æºæè¿°

        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼ŒåŒ…å« source_config_id, article_ids, ç»Ÿè®¡ä¿¡æ¯ç­‰
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹ä¸Šä¼  corpus åˆ°ç³»ç»Ÿ")
        logger.info("=" * 60)

        # æ£€æŸ¥ markdown æ–‡ä»¶ç›®å½•æ˜¯å¦å­˜åœ¨
        md_dir = Path(__file__).parent / "markdown_datasets" / self.config.dataset_name
        if not md_dir.exists():
            error_msg = f"é”™è¯¯ï¼šmarkdown ç›®å½•ä¸å­˜åœ¨: {md_dir}"
            logger.error(error_msg)
            return {'status': 'error', 'message': error_msg}

        # è·å–æ‰€æœ‰ md æ–‡ä»¶
        md_files = sorted(md_dir.glob("*.md"))
        if not md_files:
            error_msg = f"é”™è¯¯ï¼šåœ¨ {md_dir} ä¸­æœªæ‰¾åˆ° .md æ–‡ä»¶"
            logger.error(error_msg)
            return {'status': 'error', 'message': error_msg}

        logger.info(f"æ‰¾åˆ° {len(md_files)} ä¸ª markdown æ–‡ä»¶")

        # 1. ç”Ÿæˆä¿¡æ¯æº ID
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        source_config_id = f"{self.config.dataset_name}-{timestamp}"

        # è®¾ç½®é»˜è®¤åç§°å’Œæè¿°
        if source_name is None:
            source_name = f"{self.config.dataset_name}-{timestamp}"  # ä½¿ç”¨æ•°æ®é›†åç§°+æ—¶é—´æˆ³
        if source_description is None:
            source_description = f"Evaluation corpus for {self.config.dataset_name} dataset"

        logger.info(f"ä¿¡æ¯æº ID: {source_config_id}")
        logger.info(f"ä¿¡æ¯æºåç§°: {source_name}")
        logger.info(f"æè¿°: {source_description}\n")

        # 2. åˆ›å»º TaskConfigï¼ˆç”¨äºä¼ é€’ source_nameï¼‰
        task_config = TaskConfig(
            task_name=f"Upload {self.config.dataset_name} Corpus",
            source_config_id=source_config_id,
            source_name=source_name
        )

        # 3. åˆ›å»º DataFlowEngine
        engine = DataFlowEngine(task_config=task_config)

        # 4. å¾ªç¯å¤„ç†æ¯ä¸ª md æ–‡ä»¶
        file_results = []
        total_sections = 0
        total_events = 0

        for idx, md_file in enumerate(md_files, 1):
            logger.info(f"[{idx}/{len(md_files)}] å¤„ç†æ–‡ä»¶: {md_file.name}")
            file_size_mb = md_file.stat().st_size / 1024 / 1024
            logger.info(f"  æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB")

            # Load é˜¶æ®µ - åŠ è½½æ–‡æ¡£
            load_start = time.perf_counter()
            try:
                await engine.load_async(
                    DocumentLoadConfig(
                        path=str(md_file),
                        recursive=False,
                        source_config_id=source_config_id
                    )
                )
                load_time = time.perf_counter() - load_start
                logger.info(f"  âœ“ æ–‡æ¡£åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.1f} ç§’")
            except Exception as e:
                error_msg = f"æ–‡æ¡£åŠ è½½å¤±è´¥ ({md_file.name}): {e}"
                logger.error(error_msg, exc_info=True)
                file_results.append({
                    'file': md_file.name,
                    'status': 'error',
                    'message': str(e)
                })
                continue

            # è·å– Load ç»“æœ
            engine_result = engine.get_result()
            if not engine_result or not engine_result.load_result:
                error_msg = f"Load é˜¶æ®µå¤±è´¥ï¼šæ— æ³•è·å–åŠ è½½ç»“æœ ({md_file.name})"
                logger.error(error_msg)
                file_results.append({
                    'file': md_file.name,
                    'status': 'error',
                    'message': error_msg
                })
                continue

            # ä» engine_result è·å–æ•°æ®
            try:
                article_id = engine_result.article_id
                load_result = engine_result.load_result
                sections_count = load_result.stats.get("chunk_count", 0) if load_result.stats else 0
                total_sections += sections_count

                logger.info(f"  Article ID: {article_id}")
                logger.info(f"  æ–‡æ¡£ç‰‡æ®µæ•°: {sections_count}")
            except Exception as e:
                error_msg = f"è¯»å– Load ç»“æœå¤±è´¥ ({md_file.name}): {e}"
                logger.error(error_msg, exc_info=True)
                file_results.append({
                    'file': md_file.name,
                    'status': 'error',
                    'message': str(e)
                })
                continue

            events_count = 0

            # Extract é˜¶æ®µ - æå–äº‹é¡¹ï¼ˆå¯é€‰ï¼‰
            if enable_extraction:
                logger.info(f"  å¼€å§‹æå–äº‹é¡¹...")
                extract_start = time.perf_counter()

                try:
                    await engine.extract_async(
                        ExtractBaseConfig(
                            parallel=True,
                            max_concurrency=50
                        )
                    )
                    extract_time = time.perf_counter() - extract_start
                    logger.info(f"  âœ“ äº‹é¡¹æå–å®Œæˆï¼Œè€—æ—¶: {extract_time:.1f} ç§’")

                    # è·å– Extract ç»“æœ
                    engine_result = engine.get_result()
                    if engine_result and engine_result.extract_result:
                        extract_result = engine_result.extract_result
                        events_count = len(extract_result.data_ids) if extract_result.data_ids else 0
                        total_events += events_count
                        logger.info(f"  ç”Ÿæˆäº‹é¡¹æ•°: {events_count}")
                    else:
                        logger.warning(f"  âš ï¸  Extract ç»“æœä¸ºç©º")
                except Exception as e:
                    error_msg = f"äº‹é¡¹æå–å¤±è´¥ ({md_file.name}): {e}"
                    logger.error(error_msg, exc_info=True)
                    # æå–å¤±è´¥ä¸è¿”å›é”™è¯¯ï¼Œå› ä¸º Load å·²ç»æˆåŠŸ
            else:
                logger.info(f"  è·³è¿‡æå–é˜¶æ®µï¼ˆenable_extraction=Falseï¼‰")

            # è®°å½•æ–‡ä»¶å¤„ç†ç»“æœ
            file_results.append({
                'file': md_file.name,
                'article_id': article_id,
                'sections_count': sections_count,
                'events_count': events_count,
                'status': 'completed'
            })

            logger.info(f"  âœ“ æ–‡ä»¶å¤„ç†å®Œæˆ\n")

        # 5. ä¿å­˜ç»“æœåˆ° dataflow/evaluation/source/SAG/{timestamp}/
        source_dir = Path(__file__).parent / "source" / "SAG" / timestamp
        source_dir.mkdir(parents=True, exist_ok=True)

        result = {
            "source_config_id": source_config_id,
            "source_name": source_name,
            "source_description": source_description,
            "dataset_name": self.config.dataset_name,
            "file_count": len(md_files),
            "successful_files": len([r for r in file_results if r['status'] == 'completed']),
            "failed_files": len([r for r in file_results if r['status'] == 'error']),
            "total_sections_count": total_sections,
            "total_events_count": total_events,
            "file_results": file_results,
            "timestamp": timestamp,
            "status": "completed",
            "extraction_enabled": enable_extraction
        }

        # ä¿å­˜åˆ° source_info.json
        source_info_path = source_dir / "source_info.json"
        with open(source_info_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ä¿¡æ¯æºç»“æœå·²ä¿å­˜: {source_info_path}")

        # è¿”å›ç»“æœ
        logger.info("=" * 60)
        logger.info("âœ… Corpus ä¸Šä¼ å®Œæˆ")
        logger.info(f"  æ€»æ–‡ä»¶æ•°: {len(md_files)}")
        logger.info(f"  æˆåŠŸ: {result['successful_files']}")
        logger.info(f"  å¤±è´¥: {result['failed_files']}")
        logger.info(f"  æ€»ç‰‡æ®µæ•°: {total_sections}")
        logger.info(f"  æ€»äº‹é¡¹æ•°: {total_events}")
        logger.info("=" * 60)

        # ä¸»åŠ¨å…³é—­æ•°æ®åº“è¿æ¥å’ŒAIå®¢æˆ·ç«¯ï¼Œé¿å… "Event loop is closed" è­¦å‘Š
        try:
            logger.info("å…³é—­æ•°æ®åº“è¿æ¥å’ŒAIå®¢æˆ·ç«¯...")
            # å…³é—­æ•°æ®åº“è¿æ¥
            await close_database()

            logger.info("âœ“ æ‰€æœ‰è¿æ¥å·²å…³é—­")
        except Exception as e:
            logger.warning(f"å…³é—­è¿æ¥æ—¶å‡ºç°è­¦å‘Š: {e}")

        return result

    def print_summary(self, results: Dict[str, Any]):
        """
        æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦

        Args:
            results: è¯„ä¼°ç»“æœå­—å…¸
        """
        logger.info("\n" + "=" * 60)
        logger.info(f"Evaluation Summary - {results['dataset']}")
        logger.info("=" * 60)
        logger.info(f"Timestamp: {results['timestamp']}")
        logger.info(f"Num Questions: {results['num_questions']}")

        if 'retrieval' in results:
            logger.info("\n--- Retrieval Results ---")
            pooled = results['retrieval']['pooled']
            for metric, score in pooled.items():
                logger.info(f"{metric}: {score:.4f}")

        if 'qa' in results:
            logger.info("\n--- QA Results ---")
            pooled = results['qa']['pooled']
            for metric, score in pooled.items():
                logger.info(f"{metric}: {score:.4f}")

        logger.info("=" * 60 + "\n")


# ä¾¿æ·å‡½æ•°
def quick_evaluate(
    dataset_name: str,
    retrieved_docs_list: Optional[List[List[str]]] = None,
    predicted_answers: Optional[List[str]] = None,
    max_samples: Optional[int] = None,
    output_dir: str = "outputs/evaluation"
) -> Dict[str, Any]:
    """
    å¿«é€Ÿè¯„ä¼°å‡½æ•°

    Args:
        dataset_name: æ•°æ®é›†åç§°
        retrieved_docs_list: æ£€ç´¢ç»“æœåˆ—è¡¨
        predicted_answers: é¢„æµ‹ç­”æ¡ˆåˆ—è¡¨
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        è¯„ä¼°ç»“æœ
    """
    config = EvaluationConfig(
        dataset_name=dataset_name,
        max_samples=max_samples,
        output_dir=output_dir
    )

    evaluator = Evaluate(config)
    results = evaluator.evaluate_all(
        retrieved_docs_list=retrieved_docs_list,
        predicted_answers=predicted_answers
    )

    evaluator.print_summary(results)

    return results


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Evaluation benchmark for multi-hop QA datasets"
    )

    parser.add_argument(
        "--dataset",
        type=str,
        default="musique",
        choices=["musique", "hotpotqa", "2wikimultihopqa", "sample"],
        help="Dataset name to evaluate (default: musique)"
    )

    parser.add_argument(
        "--load",
        action="store_true",
        help="Load dataset and generate markdown files"
    )

    parser.add_argument(
        "--chunks-per-file",
        type=int,
        default=500,
        help="Number of chunks per markdown file (default: 500)"
    )


    parser.add_argument(
        "--upload",
        action="store_true",
        help="Upload markdown files to system (creates source in dataflow/evaluation/source/SAG/)"
    )

    parser.add_argument(
        "--show-retrieval-info",
        action="store_true",
        help="Show latest source_config_id, dataset info and retrieval information"
    )

    parser.add_argument(
        "--info-limit",
        type=int,
        default=None,
        help="Limit number of questions to display in --show-retrieval-info (default: show all)"
    )

    parser.add_argument(
        "--enable-search",
        action="store_true",
        help="Enable actual search functionality in --show-retrieval-info"
    )

    parser.add_argument(
        "--search-verbose",
        action="store_true",
        help="Show detailed search process logs"
    )

    parser.add_argument(
        "--no-paragraphs",
        action="store_true",
        help="Hide paragraph details in --show-retrieval-info"
    )

    args = parser.parse_args()

    # å¦‚æœæŒ‡å®šäº† --show-retrieval-infoï¼Œæ˜¾ç¤ºæ£€ç´¢ä¿¡æ¯å¹¶é€€å‡º
    if args.show_retrieval_info:
        try:
            # ä½¿ç”¨ asyncio.run è¿è¡Œå¼‚æ­¥å‡½æ•°
            retrieval_info = asyncio.run(Evaluate.show_retrieval_info(
                limit=args.info_limit,
                show_paragraphs=not args.no_paragraphs,
                enable_search=args.enable_search,
                search_verbose=args.search_verbose
            ))
            logger.info(f"\næ£€ç´¢ä¿¡æ¯æ˜¾ç¤ºå®Œæˆ")
            logger.info(f"å…±æ˜¾ç¤º {retrieval_info['display_limit']} ä¸ªé—®é¢˜")
            logger.info(f"ä¿¡æ¯æºID: {retrieval_info['source_info']['source_config_id']}")
            logger.info(f"æ•°æ®é›†: {retrieval_info['source_info']['dataset_name']}")
            if retrieval_info['enable_search'] and retrieval_info['search_results']:
                total_successful = sum(1 for r in retrieval_info['search_results'] if r['search_success'])
                logger.info(f"æ£€ç´¢ç»Ÿè®¡: {total_successful}/{len(retrieval_info['search_results'])} ä¸ªé—®é¢˜æ£€ç´¢æˆåŠŸ")
                
                # æ˜¾ç¤ºå¬å›ç‡è¯„ä¼°ç»“æœ
                if retrieval_info.get('recall_evaluation'):
                    recall_eval = retrieval_info['recall_evaluation']
                    logger.info(f"å¬å›ç‡è¯„ä¼°: åŸºäº {recall_eval['num_questions']} ä¸ªé—®é¢˜")
                    for metric, score in recall_eval['pooled_results'].items():
                        logger.info(f"  {metric}: {score:.4f} ({score*100:.2f}%)")
        except Exception as e:
            logger.error(f"\nè·å–æ£€ç´¢ä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
        sys.exit(0)  # æ˜¾ç¤ºä¿¡æ¯åæ­£å¸¸é€€å‡º

    # è¿è¡Œè¯„ä¼°
    results = Evaluate.evaluate(
        dataset_name=args.dataset,
        load_and_generate_md=args.load,
        chunks_per_file=args.chunks_per_file,
        force_regenerate=True
    )

    # æ‰“å°ç»“æœæ‘˜è¦
    logger.info("\n" + "=" * 70)
    logger.info("Evaluation Results")
    logger.info("=" * 70)
    logger.info(f"Dataset: {results['dataset']}")
    logger.info(f"Timestamp: {results['timestamp']}")

    if 'markdown_generation' in results:
        logger.info("\n--- Markdown Generation ---")
        stats = results['markdown_generation'].get('stats', {})
        logger.info(f"Output Directory: {results['markdown_generation']['output_dir']}")
        logger.info(f"Total Chunks: {stats.get('total_chunks', 'N/A'):,}")
        logger.info(f"Number of MD Files: {stats.get('num_files', 'N/A')} ä¸ª")
        logger.info(f"Chunks Per File: {results['markdown_generation']['chunks_per_file']}")
        if stats.get('last_file_chunks') is not None:
            logger.info(f"Last File Chunks: {stats['last_file_chunks']} ä¸ª")
        logger.info(f"Status: {results['markdown_generation']['status']}")

    logger.info("=" * 70 + "\n")

    # ä¸Šä¼ åˆ°ç³»ç»Ÿï¼ˆå¦‚æœæŒ‡å®šäº† --uploadï¼‰
    if args.upload:
        logger.info("\n" + "=" * 70)
        logger.info("å¼€å§‹ä¸Šä¼  corpus åˆ°ç³»ç»Ÿ")
        logger.info("=" * 70)

        config = EvaluationConfig(dataset_name=args.dataset)
        evaluator = Evaluate(config)

        async def upload_task():
            upload_result = await evaluator.upload_corpus(
                enable_extraction=True  # é»˜è®¤å¯ç”¨æå–
            )
            return upload_result

        # ä½¿ç”¨æ›´æ¸©å’Œçš„æ–¹å¼ç®¡ç† event loop
        try:
            upload_result = asyncio.run(upload_task())

            if upload_result['status'] == 'completed':
                logger.info("\n" + "=" * 70)
                logger.info("ä¸Šä¼ ç»“æœ")
                logger.info("=" * 70)
                logger.info(f"Source Config ID: {upload_result['source_config_id']}")
                logger.info(f"æ•°æ®é›†: {upload_result['dataset_name']}")
                logger.info(f"æ–‡ä»¶æ•°: {upload_result['file_count']}")
                logger.info(f"æˆåŠŸ: {upload_result['successful_files']}")
                logger.info(f"å¤±è´¥: {upload_result['failed_files']}")
                logger.info(f"æ€»ç‰‡æ®µæ•°: {upload_result['total_sections_count']:,}")
                logger.info(f"æ€»äº‹é¡¹æ•°: {upload_result['total_events_count']:,}")
                logger.info(f"ç»“æœä¿å­˜ä½ç½®: dataflow/evaluation/source/SAG/{upload_result['timestamp']}/")
                logger.info("=" * 70 + "\n")
            else:
                logger.error(f"\nâŒ ä¸Šä¼ å¤±è´¥: {upload_result.get('message', 'Unknown error')}\n")
        except KeyboardInterrupt:
            logger.info("\n\nç”¨æˆ·ä¸­æ–­ä¸Šä¼ ")
            sys.exit(1)
        except Exception as e:
            logger.error(f"\nâŒ ä¸Šä¼ è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\n")
            import traceback
            traceback.print_exc()
            sys.exit(1)
