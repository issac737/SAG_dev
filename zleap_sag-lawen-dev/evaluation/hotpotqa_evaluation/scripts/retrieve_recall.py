"""
Retrieve and Recall Evaluation Module

é›†æˆæ£€ç´¢å’Œå¬å›è¯„ä¼°åŠŸèƒ½ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†å’Œé€æ­¥ç»Ÿè®¡
"""

from __future__ import annotations

import json
import logging
import asyncio
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, field
from collections import defaultdict

# Add project root to path
import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))  # 4 ä¸ª .parent

# Load environment variables
from dotenv import load_dotenv
project_root = Path(__file__).parent.parent.parent.parent
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"[INFO] Loaded environment variables: {env_path}")
else:
    print(f"[WARN] .env file not found: {env_path}")

# Import required modules
from dataflow.modules.search import EventSearcher, SearchConfig, SAGSearcher
from dataflow.modules.search.config import (
    ReturnType, RecallConfig, ExpandConfig, RerankConfig, RecallMode
)
from dataflow.core.storage.elasticsearch import close_es_client
from dataflow.core.prompt.manager import PromptManager

# Import recall metrics
from evaluation.hotpotqa_evaluation.scripts.recall_metrics import (
    RecallCalculator, RecallResult, normalize_text, calculate_recall
)
# Import shared models & IO helpers
from evaluation.hotpotqa_evaluation.scripts.shared.data_models import RetrievalResult
from evaluation.hotpotqa_evaluation.scripts.shared.io_utils import write_jsonl
from dataflow.utils.logger import setup_logging

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œç¡®ä¿logger.info()èƒ½å¤Ÿè¾“å‡ºåˆ°ç»ˆç«¯
# å¦‚éœ€æŸ¥çœ‹æ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ï¼Œå¯ä»¥å°† level æ”¹ä¸º "DEBUG"
setup_logging(level="INFO")
def calculate_recall(
    oracle_chunks: List[Dict[str, Any]],
    retrieved_sections: List[Dict[str, Any]],
    verbose: bool = False
) -> Dict[str, Any]:
    """
    Calculate Recall metric using hybrid matching (title + text fallback)

    Matching strategy (in order of priority):
    1. Title exact match (normalized)
    2. Text substring match (first 100 chars)
    
    This hybrid approach is more robust than single-method matching.

    Args:
        oracle_chunks: Ground truth chunks with 'title' and 'text_preview' fields
        retrieved_sections: Retrieved sections with 'heading' and 'content_preview' fields
        verbose: Show matching details

    Returns:
        Recall statistics
    """
    if not oracle_chunks:
        return {
            'recall': 0.0,
            'total_oracle': 0,
            'recalled': 0,
            'retrieved': len(retrieved_sections),
            'recalled_details': []
        }

    def normalize_title(title: str) -> str:
        """Normalize title by stripping Markdown header prefix and whitespace"""
        if not title:
            return ""
        title = title.strip()
        if title.startswith("# "):
            title = title[2:]
        return title.lower().strip()

    def normalize_text(text: str, max_len: int = 150) -> str:
        """Normalize text for matching: lowercase, remove extra spaces, truncate"""
        if not text:
            return ""
        # Remove markdown header prefix if present at start
        text = text.strip()
        if text.startswith("# "):
            text = text[2:]
        # Normalize whitespace and lowercase
        text = ' '.join(text.lower().split())
        # Truncate to first N chars for matching (avoid length differences)
        return text[:max_len]

    # Build oracle lookup structures
    oracle_by_title = {}  # normalized_title -> oracle chunk
    oracle_by_text = {}   # normalized_text_prefix -> oracle chunk
    
    for chunk in oracle_chunks:
        title = chunk.get('title', '')
        text = chunk.get('text_preview', '')
        
        normalized_title = normalize_title(title)
        normalized_text = normalize_text(text)
        
        if normalized_title:
            oracle_by_title[normalized_title] = chunk
        if normalized_text:
            oracle_by_text[normalized_text] = chunk

    # Check which oracle chunks were recalled
    recalled_details = []
    recalled_oracle_ids = set()  # Track by chunk_id to avoid duplicates

    for section in retrieved_sections:
        retrieved_title = section.get('heading', '')
        retrieved_text = section.get('content_preview', '')
        
        normalized_title = normalize_title(retrieved_title)
        normalized_text = normalize_text(retrieved_text)
        
        matched_chunk = None
        match_method = None
        
        # Strategy 1: Title exact match
        if normalized_title in oracle_by_title:
            matched_chunk = oracle_by_title[normalized_title]
            match_method = "title"
        # Strategy 2: Title contains match (for compound headings like "# A | # B")
        else:
            for oracle_title, chunk in oracle_by_title.items():
                if oracle_title in normalized_title:
                    matched_chunk = chunk
                    match_method = "title_contains"
                    break
        # Strategy 3: Text prefix match
        if not matched_chunk and normalized_text in oracle_by_text:
            matched_chunk = oracle_by_text[normalized_text]
            match_method = "text"
        
        if matched_chunk:
            chunk_id = matched_chunk.get('chunk_id')
            if chunk_id and chunk_id not in recalled_oracle_ids:
                recalled_oracle_ids.add(chunk_id)
                recalled_details.append({
                    'oracle_chunk_id': chunk_id,
                    'oracle_title': matched_chunk.get('title'),
                    'retrieved_section_id': section.get('section_id'),
                    'retrieved_heading': section.get('heading'),
                    'match_method': match_method
                })

    # Calculate recall
    recall = len(recalled_oracle_ids) / len(oracle_chunks) if oracle_chunks else 0.0

    return {
        'recall': recall,
        'total_oracle': len(oracle_chunks),
        'recalled': len(recalled_oracle_ids),
        'retrieved': len(retrieved_sections),
        'recalled_details': recalled_details
    }



def normalize_text(text: str) -> str:
    """
    Normalize text for exact matching
    Remove extra whitespace and trim

    Args:
        text: Input text

    Returns:
        Normalized text
    """
    return ' '.join(text.strip().split())

# ç§»é™¤æœ¬åœ°çš„ calculate_recall å’Œ normalize_text å‡½æ•°ï¼Œä½¿ç”¨ recall_metrics ä¸­çš„ç‰ˆæœ¬


def load_corpus(corpus_path: Path) -> Dict[str, Dict[str, str]]:
    """
    åŠ è½½è¯­æ–™åº“ï¼Œè¿”å› {chunk_id: {title, text}} çš„å­—å…¸
    """
    corpus_dict = {}
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                chunk = json.loads(line)
                chunk_id = chunk['id']

                # å¤„ç†åˆå¹¶çš„ IDï¼ˆæ ¼å¼ï¼šid1//id2//id3ï¼‰
                if "//" in chunk_id:
                    original_ids = chunk_id.split("//")
                    for original_id in original_ids:
                        corpus_dict[original_id] = {
                            'title': chunk['title'],
                            'text': chunk['text']
                        }
                else:
                    corpus_dict[chunk_id] = {
                        'title': chunk['title'],
                        'text': chunk['text']
                    }
    return corpus_dict


def load_oracle_questions(oracle_path: Path, limit: Optional[int] = None) -> List[Dict[str, Any]]:
    """
    ä» oracle.jsonl åŠ è½½æµ‹è¯•é—®é¢˜

    Args:
        oracle_path: oracle.jsonl è·¯å¾„
        limit: åŠ è½½é—®é¢˜æ•°é‡é™åˆ¶ï¼ˆNone è¡¨ç¤ºåŠ è½½å…¨éƒ¨ï¼‰

    Returns:
        é—®é¢˜åˆ—è¡¨
    """
    questions = []

    if not oracle_path.exists():
        raise FileNotFoundError(f"Oracle æ–‡ä»¶ä¸å­˜åœ¨: {oracle_path}")

    with open(oracle_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if limit and i >= limit:
                break
            oracle = json.loads(line)
            questions.append({
                'id': oracle['id'],
                'question': oracle['question'],
                'answer': oracle['answer'],
                'oracle_chunk_ids': oracle['oracle_chunk_ids']
            })

    return questions


def load_zero_recall_questions(json_path: Path) -> List[Dict[str, Any]]:
    """
    ä» zero_recall_questions.json åŠ è½½å¬å›ç‡ä¸º0çš„é—®é¢˜

    Args:
        json_path: zero_recall_questions.json è·¯å¾„

    Returns:
        é—®é¢˜åˆ—è¡¨
    """
    if not json_path.exists():
        raise FileNotFoundError(f"Zero recall æ–‡ä»¶ä¸å­˜åœ¨: {json_path}")

    with open(json_path, 'r', encoding='utf-8') as f:
        questions_data = json.load(f)

    # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
    questions = []
    for q in questions_data:
        questions.append({
            'id': q['id'],
            'question': q['question'],
            'answer': q['answer'],
            'oracle_chunk_ids': q['oracle_chunk_ids']
        })

    return questions



async def process_single_question(
    q: Dict[str, Any],
    searcher: Any,
    source_config_id: str,
    corpus_dict: Dict[str, Dict[str, str]],
    index: int,
    total: int,
    verbose: bool = False,
    zero_recall_mode: bool = False
) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªé—®é¢˜çš„æ£€ç´¢

    Args:
        q: é—®é¢˜æ•°æ®
        searcher: æœç´¢å™¨å®ä¾‹
        source_config_id: æ•°æ®æºID
        corpus_dict: è¯­æ–™åº“å­—å…¸
        index: å½“å‰ç´¢å¼•
        total: æ€»æ•°
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        zero_recall_mode: æ˜¯å¦ä¸ºé›¶å¬å›è°ƒè¯•æ¨¡å¼

    Returns:
        æ£€ç´¢ç»“æœ
    """
    import logging
    logger = logging.getLogger(__name__)

    question_id = q['id']
    question = q['question']
    answer = q['answer']
    oracle_chunk_ids = q['oracle_chunk_ids']

    if verbose or zero_recall_mode:
        logger.info(f"\n{'='*80}")
        logger.info(f"[{index}/{total}] æœç´¢é—®é¢˜")
        logger.info(f"{'='*80}")
        logger.info(f"â“ é—®é¢˜: {question}")
        logger.info(f"âœ… ç­”æ¡ˆ: {answer}")

        # å±•ç¤ºæ ‡å‡†ç­”æ¡ˆæ®µè½
        if oracle_chunk_ids and corpus_dict:
            logger.info(f"\nğŸ“Œ æ ‡å‡†ç­”æ¡ˆå¯¹åº”çš„æ®µè½ ({len(oracle_chunk_ids)}ä¸ª):")
            for j, chunk_id in enumerate(oracle_chunk_ids[:3], 1):
                if chunk_id in corpus_dict:
                    chunk = corpus_dict[chunk_id]
                    logger.info(f"   [{j}] æ ‡é¢˜: {chunk['title']}")
                    text_preview = chunk['text'].replace('\n', ' ')
                    logger.info(f"       å†…å®¹: {text_preview}...")
            if len(oracle_chunk_ids) > 3:
                logger.info(f"   ... è¿˜æœ‰ {len(oracle_chunk_ids) - 3} ä¸ªæ®µè½")

    # æ‰§è¡Œæœç´¢
    search_config = SearchConfig(
        query=question,
        source_config_id=source_config_id,
        return_type=ReturnType.PARAGRAPH,
        recall=RecallConfig(use_fast_mode=False,vector_top_k=50,max_entities=50,recall_mode=RecallMode.FUZZY,entity_similarity_threshold=0.3,entity_weight_threshold=0.2),
        expand=ExpandConfig(max_hops=3),
        rerank=RerankConfig(max_results=10, score_threshold=0.45, strategy="pagerank")
    )

    sections = []
    try:
        search_result = await searcher.search(search_config)
        sections = search_result.get("sections", [])
        if verbose or zero_recall_mode:
            logger.info(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(sections)} ä¸ªç›¸å…³æ®µè½")
    except Exception as e:
        if verbose or zero_recall_mode:
            logger.error(f"âš ï¸  æœç´¢å¤±è´¥: {e}")
        sections = []

    # æ®µè½å»é‡
    seen_chunk_ids = set()
    unique_sections = []
    for section in sections:
        chunk_id = section.get('chunk_id')
        if chunk_id and chunk_id not in seen_chunk_ids:
            seen_chunk_ids.add(chunk_id)
            unique_sections.append(section)

    sections = unique_sections

    if (verbose or zero_recall_mode) and len(sections) > 0:
        logger.info(f"\nğŸ” æ£€ç´¢åˆ°çš„æ®µè½ (å…± {len(sections)} ä¸ª):")
        for j, section in enumerate(sections, 1):
            heading = section.get('heading', 'N/A')
            content = section.get('content', '').replace('\n', ' ')

            # è·å–å¾—åˆ†ä¿¡æ¯
            cosine_score = section.get('original_score', 0.0)  # ä½™å¼¦ç›¸ä¼¼åº¦
            pagerank_score = section.get('weight') or section.get('pagerank', 0.0)  # PageRankå¾—åˆ†

            # åŒæ—¶æ˜¾ç¤ºä½™å¼¦ç›¸ä¼¼åº¦å’ŒPageRankå¾—åˆ†
            logger.info(f"   [{j}] æ ‡é¢˜: {heading}")
            logger.info(f"       ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_score:.4f} | PageRank: {pagerank_score:.4f}")
            logger.info(f"       å†…å®¹: {content}...")
        if len(sections) > 5:
            logger.info(f"   ... è¿˜æœ‰ {len(sections) - 5} ä¸ªæ®µè½")
    elif (verbose or zero_recall_mode) and len(sections) == 0:
        logger.warning(f"âš ï¸  æœªæ£€ç´¢åˆ°ä»»ä½•æ®µè½ï¼")

    # æ„å»ºæ£€ç´¢ç»“æœ
    oracle_chunks = []
    for chunk_id in oracle_chunk_ids:
        if chunk_id in corpus_dict:
            chunk = corpus_dict[chunk_id]
            oracle_chunks.append({
                'chunk_id': chunk_id,
                'title': chunk['title'],
                'text_preview': chunk['text']  # ä¿ç•™å®Œæ•´å†…å®¹
            })

    retrieved_sections = []
    for section in sections:
        retrieved_sections.append({
            'section_id': section.get('chunk_id', ''),
            'event_id': section.get('event_id', ''),
            'event_title': section.get('event_title', ''),
            'heading': section.get('heading', ''),
            'content_preview': section.get('content', ''),  # ä¿ç•™å®Œæ•´å†…å®¹
            'similarity_score': section.get('pagerank') or section.get('weight')  # ä½¿ç”¨ pagerank æˆ– weight ä½œä¸ºåˆ†æ•°
        })

    return {
        'question_id': question_id,
        'question': question,
        'answer': answer,
        'oracle_chunk_ids': oracle_chunk_ids,
        'oracle_chunks': oracle_chunks,
        'retrieved_sections': retrieved_sections,
        'retrieval_metadata': {
            'source_config_id': source_config_id,
            'top_k': 10,
            'threshold': 0.45,
            'retrieved_count': len(retrieved_sections),
            'timestamp': datetime.now().isoformat()
        }
    }



@dataclass
class RecallStats:
    """å¬å›ç»Ÿè®¡ä¿¡æ¯"""
    total_questions: int = 0
    total_oracle: int = 0
    total_recalled: int = 0
    total_retrieved: int = 0
    perfect_recall_count: int = 0
    zero_recall_count: int = 0
    partial_recall_count: int = 0  # éƒ¨åˆ†å¬å›çš„é—®é¢˜æ•°
    partial_recall_questions: List[Dict[str, Any]] = field(default_factory=list)  # éƒ¨åˆ†å¬å›çš„é—®é¢˜åˆ—è¡¨
    per_question: List[Dict[str, Any]] = field(default_factory=list)
    cumulative_recall: float = 0.0

    def update(self, recall_info: Dict[str, Any], question_id: str, question: str):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.total_questions += 1
        self.total_oracle += recall_info['total_oracle']
        self.total_recalled += recall_info['recalled']
        self.total_retrieved += recall_info['retrieved']

        if recall_info['recall'] >= 1.0:
            self.perfect_recall_count += 1
        elif recall_info['recall'] == 0.0:
            self.zero_recall_count += 1
        else:
            # éƒ¨åˆ†å¬å› (0 < recall < 1.0)
            self.partial_recall_count += 1
            self.partial_recall_questions.append({
                'question_id': question_id,
                'question': question,
                'recall': recall_info['recall'],
                'recalled': recall_info['recalled'],
                'total_oracle': recall_info['total_oracle'],
                'percentage': f"{recall_info['recalled']}/{recall_info['total_oracle']}"
            })

        # ç´¯ç§¯å¬å›ç‡
        if self.total_oracle > 0:
            self.cumulative_recall = self.total_recalled / self.total_oracle

        # è®°å½•æ¯ä¸ªé—®é¢˜çš„ç»Ÿè®¡
        self.per_question.append({
            'question_id': question_id,
            'question': question,
            'recall': recall_info['recall'],
            'total_oracle': recall_info['total_oracle'],
            'recalled': recall_info['recalled'],
            'retrieved': recall_info['retrieved'],
            'recalled_details': recall_info['recalled_details']
        })

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'total_questions': self.total_questions,
            'total_oracle': self.total_oracle,
            'total_recalled': self.total_recalled,
            'total_retrieved': self.total_retrieved,
            'cumulative_recall': self.cumulative_recall,
            'perfect_recall_count': self.perfect_recall_count,
            'zero_recall_count': self.zero_recall_count,
            'partial_recall_count': self.partial_recall_count,
            'partial_recall_questions': self.partial_recall_questions,
            'per_question': self.per_question
        }


class RetrieveRecallEvaluator:
    """
    æ£€ç´¢å’Œå¬å›è¯„ä¼°å™¨

    åŠŸèƒ½ï¼š
    1. ä»æŒ‡å®šæ–‡ä»¶å¤¹è¯»å–å‚æ•°ï¼ˆsource_config_id, oracle.jsonl, corpus.jsonlï¼‰
    2. æ‰¹é‡æ‰§è¡Œæ£€ç´¢å’Œå¬å›è¯„ä¼°
    3. æ”¯æŒé€æ­¥å åŠ ç»Ÿè®¡ç»“æœ
    """

    def __init__(self, data_dir: Optional[Path] = None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            data_dir: æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆåŒ…å« oracle.jsonl, corpus.jsonl, process_result.jsonï¼‰
                     å¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„ evaluation/hotpotqa_evaluation/data/source/æœ€æ–°æ–‡ä»¶å¤¹
        """
        self.data_dir = data_dir
        self.source_config_id: Optional[str] = None
        self.corpus_dict: Dict[str, Dict[str, str]] = {}
        self.questions: List[Dict[str, Any]] = []
        self.searcher: Optional[EventSearcher] = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = RecallStats()

        # ç»“æœå­˜å‚¨
        self.retrieval_results: List["RetrievalResult"] = []

        # åˆå§‹åŒ–æ—¥å¿—
        self._setup_logging()

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        self.logger = logging.getLogger(__name__)

    def load_data_directory(self, data_dir: Optional[Path] = None) -> Path:
        """
        åŠ è½½æ•°æ®æ–‡ä»¶å¤¹

        Args:
            data_dir: æŒ‡å®šçš„æ•°æ®æ–‡ä»¶å¤¹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„

        Returns:
            å®é™…ä½¿ç”¨çš„æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„
        """
        if data_dir:
            # ä½¿ç”¨æŒ‡å®šçš„æ–‡ä»¶å¤¹
            target_dir = Path(data_dir)
            if not target_dir.exists():
                raise FileNotFoundError(f"Data directory not found: {target_dir}")
        else:
            # è‡ªåŠ¨æŸ¥æ‰¾æœ€åä¸€ä¸ªæ–‡ä»¶å¤¹ï¼ˆç›¸å¯¹äºè„šæœ¬ä½ç½®ï¼‰
            script_dir = Path(__file__).parent
            base_dir = script_dir.parent / "data" / "source"

            if not base_dir.exists():
                raise FileNotFoundError(f"Base directory not found: {base_dir}\n"
                                       f"  Please run from: zleap_sag directory\n"
                                       f"  Or specify --data-dir path")

            # è·å–æ‰€æœ‰å­æ–‡ä»¶å¤¹ï¼ˆæŒ‰ç›®å½•åæ’åºï¼Œæ ¼å¼: YYYYMMDD_HHMMSSï¼‰
            subdirs = [d for d in base_dir.iterdir() if d.is_dir()]
            if not subdirs:
                raise FileNotFoundError(f"No subdirectories found in {base_dir}\n"
                                       f"  Please ensure data has been processed first")

            # æŒ‰ç›®å½•åæ’åºï¼ˆç›®å½•åæ ¼å¼ YYYYMMDD_HHMMSS å¯ç›´æ¥å­—å…¸åºæ¯”è¾ƒï¼‰
            # ä¸ä½¿ç”¨ mtimeï¼Œå› ä¸º mtime ä¼šè¢«å„ç§æ–‡ä»¶æ“ä½œæ›´æ–°ï¼Œä¸å¯é 
            target_dir = max(subdirs, key=lambda d: d.name)
            print(f"[INFO] Auto-selected latest directory (by name): {target_dir}")

        self.data_dir = target_dir
        print(f"[INFO] Using data directory: {self.data_dir}")
        return self.data_dir

    def load_parameters(self) -> Dict[str, Any]:
        """
        ä»æ•°æ®æ–‡ä»¶å¤¹åŠ è½½å‚æ•°

        Returns:
            å‚æ•°å­—å…¸ï¼ŒåŒ…å« source_config_id, oracle_path, corpus_path
        """
        if not self.data_dir:
            raise ValueError("Data directory not loaded. Call load_data_directory() first.")

        # 1. è¯»å– source_config_id (ä» process_result.json)
        process_result_path = self.data_dir / "process_result.json"
        if process_result_path.exists():
            with open(process_result_path, 'r', encoding='utf-8') as f:
                process_result = json.load(f)
            self.source_config_id = process_result.get('source_config_id')
            print(f"[INFO] Loaded source_config_id: {self.source_config_id}")
        else:
            raise FileNotFoundError(f"process_result.json not found: {process_result_path}")

        # 2. åŠ è½½ corpus.jsonl
        corpus_path = self.data_dir / "corpus.jsonl"
        if corpus_path.exists():
            self.corpus_dict = load_corpus(corpus_path)
            print(f"[INFO] Loaded corpus: {len(self.corpus_dict)} chunks")
        else:
            print(f"[WARN] corpus.jsonl not found: {corpus_path}")
            self.corpus_dict = {}

        # 3. åŠ è½½ oracle.jsonl
        oracle_path = self.data_dir / "oracle.jsonl"
        if oracle_path.exists():
            self.questions = load_oracle_questions(oracle_path)
            print(f"[INFO] Loaded oracle: {len(self.questions)} questions")
        else:
            raise FileNotFoundError(f"oracle.jsonl not found: {oracle_path}")

        return {
            'source_config_id': self.source_config_id,
            'corpus_path': corpus_path,
            'oracle_path': oracle_path,
            'corpus_size': len(self.corpus_dict),
            'question_count': len(self.questions)
        }

    def load_bad_cases(self, bad_cases_path: str) -> List[str]:
        """
        ä» bad_cases_zero_recall.json æ–‡ä»¶åŠ è½½é—®é¢˜IDåˆ—è¡¨

        Args:
            bad_cases_path: bad_cases_zero_recall.json æ–‡ä»¶è·¯å¾„

        Returns:
            é—®é¢˜IDåˆ—è¡¨
        """
        bad_cases_file = Path(bad_cases_path)
        if not bad_cases_file.exists():
            raise FileNotFoundError(f"Bad cases file not found: {bad_cases_file}")

        print(f"[INFO] Loading bad cases from: {bad_cases_file}")

        with open(bad_cases_file, 'r', encoding='utf-8') as f:
            bad_cases = json.load(f)

        # æå–é—®é¢˜IDåˆ—è¡¨
        question_ids = [case['question_id'] for case in bad_cases]
        print(f"[INFO] Loaded {len(question_ids)} bad case question IDs")

        return question_ids

    def filter_questions_by_ids(self, question_ids: List[str]):
        """
        æ ¹æ®é—®é¢˜IDåˆ—è¡¨è¿‡æ»¤é—®é¢˜

        Args:
            question_ids: è¦ä¿ç•™çš„é—®é¢˜IDåˆ—è¡¨
        """
        question_id_set = set(question_ids)
        original_count = len(self.questions)

        # è¿‡æ»¤é—®é¢˜åˆ—è¡¨
        self.questions = [q for q in self.questions if q.get('id') in question_id_set]

        filtered_count = len(self.questions)
        print(f"[INFO] Filtered questions: {filtered_count}/{original_count} (based on bad cases)")

        if filtered_count == 0:
            print("[WARN] No questions matched the bad cases IDs!")
        elif filtered_count < len(question_ids):
            print(f"[WARN] Only {filtered_count}/{len(question_ids)} bad case IDs were found in the oracle data")


    def init_searcher(self):
        """åˆå§‹åŒ–æœç´¢å™¨"""
        if not self.source_config_id:
            raise ValueError("source_config_id not loaded. Call load_parameters() first.")

        print("[INFO] Initializing searcher...")
        prompt_manager = PromptManager()
        self.searcher = SAGSearcher(prompt_manager=prompt_manager)
        print("[INFO] Searcher initialized successfully")

    async def process_batch(
        self,
        start_idx: int = 0,
        end_idx: Optional[int] = None,
        concurrency: int = 5,
        verbose: bool = False,
        log_details: bool = True
    ) -> Tuple[List["RetrievalResult"], "RecallStats"]:
        """
        æ‰¹é‡å¤„ç†ä¸€æ‰¹é—®é¢˜

        Args:
            start_idx: èµ·å§‹ç´¢å¼•ï¼ˆåŒ…å«ï¼‰
            end_idx: ç»“æŸç´¢å¼•ï¼ˆä¸åŒ…å«ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™å¤„ç†åˆ°æœ«å°¾
            concurrency: å¹¶å‘æ•°
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
            log_details: æ˜¯å¦è®°å½•æ¯ä¸ªé—®é¢˜çš„è¯¦ç»†è¿›åº¦åˆ°æ—¥å¿—æ–‡ä»¶

        Returns:
            (æ£€ç´¢ç»“æœåˆ—è¡¨, å¬å›ç»Ÿè®¡ä¿¡æ¯)
        """
        if not self.questions:
            raise ValueError("Questions not loaded. Call load_parameters() first.")

        if not self.searcher:
            raise ValueError("Searcher not initialized. Call init_searcher() first.")

        # ç¡®å®šæ‰¹æ¬¡èŒƒå›´
        if end_idx is None or end_idx > len(self.questions):
            end_idx = len(self.questions)

        batch_questions = self.questions[start_idx:end_idx]
        print(f"\n{'='*60}")
        print(f"Processing batch: [{start_idx}:{end_idx}] ({len(batch_questions)} questions)")
        print(f"{'='*60}")

        # åˆ›å»ºä¿¡å·é‡
        semaphore = asyncio.Semaphore(concurrency)

        async def process_with_semaphore(q, index):
            """ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘çš„åŒ…è£…å‡½æ•°"""
            async with semaphore:
                return await process_single_question(
                    q=q,
                    searcher=self.searcher,
                    source_config_id=self.source_config_id,
                    corpus_dict=self.corpus_dict,
                    index=index,
                    total=len(self.questions),
                    verbose=verbose,
                    zero_recall_mode=False
                )

        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [
            process_with_semaphore(q, i + start_idx + 1)
            for i, q in enumerate(batch_questions, start_idx)
        ]

        # å¹¶å‘æ‰§è¡Œ
        print(f"[INFO] Starting concurrent processing with {concurrency} workers...")
        start_time = time.time()
        results_dicts = await asyncio.gather(*tasks, return_exceptions=True)
        elapsed = time.time() - start_time

        # å¤„ç†ç»“æœ
        batch_results = []
        batch_stats = RecallStats()
        failed_count = 0

        for i, result in enumerate(results_dicts, 1):
            if isinstance(result, Exception):
                print(f"âš ï¸  Question {start_idx + i} failed: {result}")
                failed_count += 1
                continue

            # è½¬æ¢ä¸º RetrievalResult å¯¹è±¡
            retrieval_result = RetrievalResult(**result)
            batch_results.append(retrieval_result)

            # è®¡ç®—å¬å›ç‡å¹¶æ›´æ–°ç»Ÿè®¡
            oracle_chunks = [chunk.dict() for chunk in retrieval_result.oracle_chunks]
            retrieved_sections = [section.dict() for section in retrieval_result.retrieved_sections]

            recall_info = calculate_recall(oracle_chunks, retrieved_sections, verbose=False)
            batch_stats.update(recall_info, retrieval_result.question_id, retrieval_result.question)

            if verbose:
                progress_msg = (f"  [{len(batch_stats.per_question)}/{len(batch_questions)}] "
                               f"Recall: {recall_info['recall']:.4f} "
                               f"({recall_info['recalled']}/{recall_info['total_oracle']})")
                print(progress_msg)
                # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶ï¼ˆé bad-cases æ¨¡å¼ï¼‰
                if log_details:
                    self.logger.info(f"Question {retrieval_result.question_id}: "
                                   f"Recall={recall_info['recall']:.4f} "
                                   f"({recall_info['recalled']}/{recall_info['total_oracle']})")

        print(f"\n{'='*60}")
        print(f"Batch completed:")
        print(f"  Processed: {len(batch_results)} questions")
        print(f"  Failed: {failed_count}")
        print(f"  Time elapsed: {elapsed:.2f}s")
        print(f"  Batch recall: {batch_stats.cumulative_recall:.4f}")
        print(f"{'='*60}")

        return batch_results, batch_stats

    def merge_stats(self, new_stats: RecallStats):
        """
        å°†æ–°çš„ç»Ÿè®¡ä¿¡æ¯åˆå¹¶åˆ°å…¨å±€ç»Ÿè®¡ä¸­

        Args:
            new_stats: æ–°çš„æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯
        """
        # æ›´æ–°å…¨å±€ç»Ÿè®¡
        self.stats.total_questions += new_stats.total_questions
        self.stats.total_oracle += new_stats.total_oracle
        self.stats.total_recalled += new_stats.total_recalled
        self.stats.total_retrieved += new_stats.total_retrieved
        self.stats.perfect_recall_count += new_stats.perfect_recall_count
        self.stats.zero_recall_count += new_stats.zero_recall_count
        self.stats.partial_recall_count += new_stats.partial_recall_count
        self.stats.partial_recall_questions.extend(new_stats.partial_recall_questions)
        self.stats.per_question.extend(new_stats.per_question)

        # é‡æ–°è®¡ç®—ç´¯ç§¯å¬å›ç‡
        if self.stats.total_oracle > 0:
            self.stats.cumulative_recall = self.stats.total_recalled / self.stats.total_oracle

    async def run_incremental(
        self,
        batch_size: int = 20,
        concurrency: int = 5,
        verbose: bool = False,
        save_results: bool = True,
        track_zero_recall: bool = False,
        log_file: Optional[Path] = None,
        output_dir: Optional[Path] = None,
        is_bad_cases_mode: bool = False,
        show_search_logs: bool = False
    ) -> Tuple[Dict[str, Any], Path]:
        """
        å¢é‡è¿è¡Œï¼šåˆ†æ‰¹å¤„ç†ï¼Œé€æ­¥å åŠ ç»Ÿè®¡

        Args:
            batch_size: æ¯æ‰¹å¤„ç†çš„é—®é¢˜æ•°é‡
            concurrency: æ¯æ‰¹çš„å¹¶å‘æ•°
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
            save_results: æ˜¯å¦ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            track_zero_recall: æ˜¯å¦ç‰¹åˆ«è¿½è¸ªé›¶å¬å›é—®é¢˜ï¼ˆç”ŸæˆBad CaseæŠ¥å‘Šï¼‰
            log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™åˆ›å»ºæ–°çš„æ—¶é—´æˆ³æ–‡ä»¶å¤¹ï¼‰
            is_bad_cases_mode: æ˜¯å¦ä¸º bad-cases æ¨¡å¼
            show_search_logs: æ˜¯å¦è¾“å‡º dataflow/SAG çš„å®Œæ•´æœç´¢æ—¥å¿—ï¼ˆINFOçº§åˆ«ï¼‰

        Returns:
            (æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯å­—å…¸, æ—¥å¿—æ–‡ä»¶è·¯å¾„)
        """
        if not self.questions:
            raise ValueError("No questions loaded. Call load_parameters() first.")

        # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œåˆ›å»ºæ–°çš„æ—¶é—´æˆ³æ–‡ä»¶å¤¹
        if output_dir is None:
            script_dir = Path(__file__).parent
            retrieval_base_dir = script_dir.parent / "data" / "retrieval"
            retrieval_base_dir.mkdir(parents=True, exist_ok=True)

            # åˆ›å»ºæ—¶é—´æˆ³æ–‡ä»¶å¤¹
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_dir = retrieval_base_dir / timestamp
            output_dir.mkdir(parents=True, exist_ok=True)
            print(f"\n[INFO] Created output directory: {output_dir}")
        else:
            print(f"\n[INFO] Using existing directory: {output_dir}")

        # è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
        if log_file is None:
            # åœ¨ bad-cases æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ä¸åŒçš„æ—¥å¿—æ–‡ä»¶å
            if is_bad_cases_mode:
                log_file = output_dir / "retrieve_recall_bad_cases.log"
            else:
                log_file = output_dir / "retrieve_recall.log"

        log_file.parent.mkdir(parents=True, exist_ok=True)

        # é…ç½®æ—¥å¿—ç³»ç»Ÿï¼šæ–‡ä»¶åªè®°å½•å½“å‰æ¨¡å—çš„INFOï¼Œæ§åˆ¶å°æ ¹æ®verboseå‚æ•°
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨ï¼ˆæ€»æ˜¯è®°å½• INFO åŠä»¥ä¸Šï¼‰
        file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S'
        ))

        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨ï¼ˆæ ¹æ® verbose å‚æ•°è®¾ç½®çº§åˆ«ï¼‰
        console_handler = logging.StreamHandler()
        if verbose or show_search_logs:
            console_handler.setLevel(logging.INFO)
        else:
            console_handler.setLevel(logging.WARNING)
        console_handler.setFormatter(logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S'
        ))

        # åªé…ç½®å½“å‰æ¨¡å—çš„loggerï¼Œä¸å½±å“å…¶ä»–æ¨¡å—
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        self.logger.handlers = []  # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)
        self.logger.propagate = False  # ä¸ä¼ æ’­åˆ°æ ¹logger

        # æ—¥å¿—ç²¾ç»†åŒ–æ§åˆ¶ï¼š
        # - é»˜è®¤å±è”½å¤–éƒ¨ INFOï¼ˆWARNINGï¼‰
        # - å¦‚æœ show_search_logs=Trueï¼Œä»…å¼€æ”¾ pagerank_section çš„ INFOï¼Œå…¶å®ƒæœç´¢æ¨¡å—ä»ä¿æŒ WARNING
        default_external_level = logging.WARNING
        logging.getLogger('dataflow').setLevel(default_external_level)
        logging.getLogger('dataflow.modules.search').setLevel(default_external_level)
        logging.getLogger('dataflow.modules.search.recall').setLevel(default_external_level)
        logging.getLogger('dataflow.modules.search.expand').setLevel(default_external_level)
        logging.getLogger('elasticsearch').setLevel(default_external_level)
        logging.getLogger('urllib3').setLevel(default_external_level)
        logging.getLogger('asyncio').setLevel(default_external_level)

        if show_search_logs:
            # åªæ”¾å¼€ pagerank_section çš„ INFO
            rerank_logger = logging.getLogger('dataflow.search.rerank.pagerank')
            rerank_logger.setLevel(logging.INFO)
            rerank_logger.propagate = True

            # æŠŠ handler æŒ‚åˆ° rootï¼Œç¡®ä¿å†’æ³¡çš„ INFO èƒ½å†™å…¥
            root_logger = logging.getLogger()
            root_logger.setLevel(logging.INFO)
            if file_handler not in root_logger.handlers:
                root_logger.addHandler(file_handler)
            if console_handler not in root_logger.handlers:
                root_logger.addHandler(console_handler)

        total_questions = len(self.questions)
        num_batches = (total_questions + batch_size - 1) // batch_size

        print(f"\n{'='*60}")
        print(f"INCREMENTAL PROCESSING")
        print(f"Total questions: {total_questions}")
        print(f"Batch size: {batch_size}")
        print(f"Number of batches: {num_batches}")
        print(f"Log file: {log_file}")
        print(f"{'='*60}\n")

        all_results = []
        global_start_time = time.time()

        # åˆ†æ‰¹å¤„ç†
        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, total_questions)

            print(f"\n[Batch {batch_idx + 1}/{num_batches}] Processing questions {start_idx}-{end_idx}")
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"Batch {batch_idx + 1}/{num_batches}: questions {start_idx}-{end_idx}")
            self.logger.info(f"{'='*60}")

            # å¤„ç†å½“å‰æ‰¹æ¬¡
            batch_results, batch_stats = await self.process_batch(
                start_idx=start_idx,
                end_idx=end_idx,
                concurrency=concurrency,
                verbose=verbose,
                log_details=not is_bad_cases_mode  # bad-cases æ¨¡å¼ä¸‹ä¸è®°å½•è¯¦ç»†æ—¥å¿—
            )

            # åˆå¹¶åˆ°å…¨å±€ç»“æœå’Œç»Ÿè®¡
            all_results.extend(batch_results)
            self.merge_stats(batch_stats)

            # æ‰“å°å½“å‰ç´¯ç§¯ç»Ÿè®¡
            print(f"\n[Progress after batch {batch_idx + 1}/{num_batches}]")
            print(f"  Cumulative recall: {self.stats.cumulative_recall:.4f}")
            print(f"  Total questions: {self.stats.total_questions}")
            print(f"  Perfect recall: {self.stats.perfect_recall_count}")
            print(f"  Partial recall: {self.stats.partial_recall_count}")
            print(f"  Zero recall: {self.stats.zero_recall_count}")

            # è®°å½•ç´¯ç§¯ç»Ÿè®¡åˆ°æ—¥å¿—
            self.logger.info(f"\n[Progress after batch {batch_idx + 1}/{num_batches}]")
            self.logger.info(f"  Cumulative recall: {self.stats.cumulative_recall:.4f}")
            self.logger.info(f"  Total questions: {self.stats.total_questions}")
            self.logger.info(f"  Perfect recall: {self.stats.perfect_recall_count}")
            self.logger.info(f"  Partial recall: {self.stats.partial_recall_count}")
            self.logger.info(f"  Zero recall: {self.stats.zero_recall_count}")

            # è®°å½•åˆ°æ—¥å¿—
            self.logger.info(f"Batch {batch_idx + 1}/{num_batches} completed:")
            self.logger.info(f"  Processed: {len(batch_results)} questions")
            self.logger.info(f"  Failed: {batch_stats.total_questions - len(batch_results)}")
            self.logger.info(f"  Batch recall: {batch_stats.cumulative_recall:.4f}")
            self.logger.info(f"  Cumulative recall: {self.stats.cumulative_recall:.4f}")

            # æ‰“å°éƒ¨åˆ†å¬å›çš„é—®é¢˜åˆ—è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.stats.partial_recall_questions:
                print(f"\n  éƒ¨åˆ†å¬å›çš„é—®é¢˜ï¼š")
                for q in self.stats.partial_recall_questions[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5ä¸ª
                    print(f"    - {q['question_id']}: {q['percentage']} ({q['recall']:.2f})")

        global_elapsed = time.time() - global_start_time

        # æœ€ç»ˆç»“æœç»Ÿè®¡
        final_stats = self.stats.to_dict()
        final_stats['processing_time_seconds'] = global_elapsed
        final_stats['average_time_per_question'] = global_elapsed / total_questions if total_questions > 0 else 0

        print(f"\n{'='*60}")
        print("FINAL RESULTS")
        print(f"{'='*60}")
        print(f"Total questions processed: {final_stats['total_questions']}")
        print(f"Overall recall: {final_stats['cumulative_recall']:.4f}")
        print(f"Perfect recall count: {final_stats['perfect_recall_count']}")
        print(f"Partial recall count: {final_stats['partial_recall_count']}")
        print(f"Zero recall count: {final_stats['zero_recall_count']}")

        # è®°å½•åˆ°æ—¥å¿—
        self.logger.info(f"\n{'='*60}")
        self.logger.info("FINAL RESULTS")
        self.logger.info(f"{'='*60}")
        self.logger.info(f"Total questions: {final_stats['total_questions']}")
        self.logger.info(f"Overall recall: {final_stats['cumulative_recall']:.4f}")
        self.logger.info(f"Perfect recall: {final_stats['perfect_recall_count']}")
        self.logger.info(f"Partial recall: {final_stats['partial_recall_count']}")
        self.logger.info(f"Zero recall: {final_stats['zero_recall_count']}")

        # è¯¦ç»†åˆ—å‡ºæ‰€æœ‰éƒ¨åˆ†å¬å›çš„é—®é¢˜
        if final_stats['partial_recall_questions']:
            print(f"\n[éƒ¨åˆ†å¬å›é—®é¢˜åˆ—è¡¨] ({len(final_stats['partial_recall_questions'])} ä¸ª)ï¼š")
            self.logger.info(f"\n[éƒ¨åˆ†å¬å›é—®é¢˜åˆ—è¡¨] ({len(final_stats['partial_recall_questions'])} ä¸ª)ï¼š")
            for q in final_stats['partial_recall_questions']:
                msg = f"  - {q['question_id']}: æ‰¾åˆ° {q['recalled']} / {q['total_oracle']} ä¸ªç­”æ¡ˆ " \
                      f"(å¬å›ç‡: {q['recall']:.2%})"
                print(msg)
                self.logger.info(msg)

        # è¯¦ç»†åˆ—å‡ºæ‰€æœ‰é›¶å¬å›çš„é—®é¢˜ï¼ˆç”ŸæˆBad Caseï¼‰
        zero_recall_questions = [q for q in final_stats['per_question'] if q['recall'] == 0.0]
        if zero_recall_questions:
            print(f"\n[é›¶å¬å›é—®é¢˜åˆ—è¡¨ï¼ˆBad Caseï¼‰] ({len(zero_recall_questions)} ä¸ª)ï¼š")
            self.logger.warning(f"\n[é›¶å¬å›é—®é¢˜åˆ—è¡¨ï¼ˆBad Caseï¼‰] ({len(zero_recall_questions)} ä¸ª)ï¼š")
            for q in zero_recall_questions:
                msg = f"  - {q['question_id']}: {q['question'][:100]}..."
                print(f"  - {q['question_id']}: {q['question'][:100]}...")
                self.logger.warning(msg)

        print(f"\nTotal processing time: {global_elapsed:.2f}s")
        print(f"{'='*60}")

        self.logger.info(f"Total processing time: {global_elapsed:.2f}s")
        self.logger.info(f"{'='*60}")

        # å¼ºåˆ¶åˆ·æ–°æ—¥å¿—åˆ°æ–‡ä»¶
        for handler in self.logger.handlers:
            handler.flush()

        # ä¿å­˜ç»“æœ
        if save_results and self.data_dir:
            self.save_results(all_results, final_stats, log_file, output_dir, track_zero_recall=track_zero_recall)
        else:
            print(f"\nğŸ’¡ æ£€ç´¢è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")

        return final_stats, log_file

    def save_results(self, results: List["RetrievalResult"], stats: Dict[str, Any], log_file: Path, output_dir: Path, track_zero_recall: bool = False):
        """
        ä¿å­˜ç»“æœåˆ°æ–‡ä»¶

        Args:
            results: æ£€ç´¢ç»“æœåˆ—è¡¨
            stats: ç»Ÿè®¡ä¿¡æ¯
            log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            track_zero_recall: æ˜¯å¦è¿½è¸ªé›¶å¬å›é—®é¢˜ï¼ˆç”ŸæˆBad CaseæŠ¥å‘Šï¼‰
        """
        if not self.data_dir:
            print("[WARN] Cannot save results: data_dir not set")
            return

        print(f"\n[INFO] Saving results to: {output_dir}")

        # ä¿å­˜æ£€ç´¢ç»“æœ
        output_path = output_dir / "retrieval_results.jsonl"
        write_jsonl(results, output_path)
        print(f"[INFO] Saved retrieval results: {output_path}")

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_path = output_dir / "recall_evaluation.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"[INFO] Saved evaluation stats: {stats_path}")

        # ä¿å­˜éƒ¨åˆ†å¬å›é—®é¢˜ï¼ˆéœ€è¦å…³æ³¨çš„é—®é¢˜ï¼‰
        if stats.get('partial_recall_questions'):
            partial_path = output_dir / "partial_recall_cases.json"
            with open(partial_path, 'w', encoding='utf-8') as f:
                json.dump(stats['partial_recall_questions'], f, ensure_ascii=False, indent=2)
            print(f"[INFO] Saved partial recall cases: {partial_path} ({len(stats['partial_recall_questions'])} ä¸ª)")

        # ä¿å­˜é›¶å¬å›é—®é¢˜ï¼ˆBad Caseï¼‰- ä»…åœ¨ track_zero_recall=True æ—¶ä¿å­˜
        if track_zero_recall:
            zero_recall_questions = [q for q in stats['per_question'] if q['recall'] == 0.0]
            if zero_recall_questions:
                zero_path = output_dir / "bad_cases_zero_recall.json"
                with open(zero_path, 'w', encoding='utf-8') as f:
                    json.dump(zero_recall_questions, f, ensure_ascii=False, indent=2)
                print(f"[INFO] Saved bad cases (zero recall): {zero_path} ({len(zero_recall_questions)} ä¸ª)")

        print(f"\nğŸ’¡ æ£€ç´¢è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
        print(f"ğŸ’¡ æ‰€æœ‰è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.searcher:
            await close_es_client()
            print("[INFO] Cleanup completed")


async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    import argparse

    parser = argparse.ArgumentParser(description='Retrieve and Recall Evaluation')
    parser.add_argument('--data-dir', type=str, help='Data directory path (optional)')
    parser.add_argument('--batch-size', type=int, default=5, help='Batch size (default: 50)')
    parser.add_argument('--concurrency', type=int, default=5, help='Concurrency (default: 5)')
    parser.add_argument('--verbose', action='store_true', help='Verbose output')
    parser.add_argument('--no-save', action='store_true', help='Do not save results')
    parser.add_argument('--track-zero-recall', action='store_true', help='Track and save zero recall questions as bad cases')
    parser.add_argument('--bad-cases', type=str, help='Path to bad_cases_zero_recall.json file to re-evaluate only those questions')
    parser.add_argument('--limit', type=int, help='Limit the number of questions to process (optional)')
    parser.add_argument('--show-search-logs', action='store_true', help='Show full search logs (dataflow/SAG set to INFO)')

    args = parser.parse_args()

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = RetrieveRecallEvaluator(data_dir=args.data_dir)

    try:
        # 1. åŠ è½½æ•°æ®æ–‡ä»¶å¤¹
        evaluator.load_data_directory()

        # 2. åŠ è½½å‚æ•°
        params = evaluator.load_parameters()
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ LOADED PARAMETERS")
        print(f"{'='*60}")
        print(f"  ğŸ”‘ source_config_id: {params['source_config_id']}")
        print(f"  ğŸ“š corpus_size: {params['corpus_size']}")
        print(f"  â“ question_count: {params['question_count']}")
        print(f"{'='*60}")

        # 2.3. å¦‚æœæŒ‡å®šäº† limitï¼Œåˆ™åªå¤„ç†å‰ N ä¸ªé—®é¢˜
        if args.limit and args.limit > 0:
            original_count = len(evaluator.questions)
            evaluator.questions = evaluator.questions[:args.limit]
            print(f"[INFO] Limited to first {args.limit} questions (original: {original_count})")

        # 2.5. å¦‚æœæŒ‡å®šäº† bad-cases æ–‡ä»¶ï¼Œåˆ™åªå¤„ç†è¿™äº›é—®é¢˜
        if args.bad_cases:
            print(f"\n{'='*60}")
            print("BAD CASES MODE")
            print(f"{'='*60}")
            bad_case_ids = evaluator.load_bad_cases(args.bad_cases)
            evaluator.filter_questions_by_ids(bad_case_ids)
            print(f"[INFO] Will re-evaluate {len(evaluator.questions)} bad case questions")
            print(f"[INFO] Bad-cases mode: Only log file will be saved (no result files)")
            print(f"{'='*60}\n")

        # 3. åˆå§‹åŒ–æœç´¢å™¨
        evaluator.init_searcher()

        # 4. è¿è¡Œå¢é‡å¤„ç†
        # åœ¨ bad-cases æ¨¡å¼ä¸‹ï¼Œé»˜è®¤ä¸ä¿å­˜ç»“æœæ–‡ä»¶ï¼Œåªä¿å­˜æ—¥å¿—ï¼Œå¹¶å¤ç”¨ bad_cases æ‰€åœ¨çš„æ–‡ä»¶å¤¹
        should_save_results = False if args.bad_cases else (not args.no_save)

        # åœ¨ bad-cases æ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ bad_cases æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•
        output_dir_arg = None
        if args.bad_cases:
            output_dir_arg = Path(args.bad_cases).parent

        stats, log_file = await evaluator.run_incremental(
            batch_size=args.batch_size,
            concurrency=args.concurrency,
            verbose=args.verbose,
            save_results=should_save_results,
            track_zero_recall=args.track_zero_recall,
            output_dir=output_dir_arg,
            is_bad_cases_mode=bool(args.bad_cases),
            show_search_logs=args.show_search_logs
        )

        # 5. æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        print(f"\n{'='*60}")
        print("SUMMARY")
        print(f"{'='*60}")
        print(f"Total questions: {stats['total_questions']}")
        print(f"Overall recall: {stats['cumulative_recall']:.2%}")
        print(f"Perfect recall: {stats['perfect_recall_count']}")
        print(f"Partial recall: {stats['partial_recall_count']}")
        print(f"Zero recall: {stats['zero_recall_count']}")
        print(f"\nLog file: {log_file}")

        # è®°å½•æœ€ç»ˆç»Ÿè®¡åˆ°æ—¥å¿—
        logger = logging.getLogger(__name__)
        logger.info(f"\n{'='*60}")
        logger.info("SUMMARY")
        logger.info(f"{'='*60}")
        logger.info(f"Total questions: {stats['total_questions']}")
        logger.info(f"Overall recall: {stats['cumulative_recall']:.2%}")
        logger.info(f"Perfect recall: {stats['perfect_recall_count']}")
        logger.info(f"Partial recall: {stats['partial_recall_count']}")
        logger.info(f"Zero recall: {stats['zero_recall_count']}")
        logger.info(f"\nLog file: {log_file}")

        # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶ï¼ˆä»…åœ¨é bad-cases æ¨¡å¼ä¸”ä¿å­˜äº†ç»“æœæ—¶æ˜¾ç¤ºï¼‰
        if should_save_results and evaluator.data_dir:
            print(f"\nGenerated files:")
            logger.info(f"\nGenerated files:")
            print(f"  - retrieval_results.jsonl")
            logger.info(f"  - retrieval_results.jsonl")
            print(f"  - recall_evaluation.json")
            logger.info(f"  - recall_evaluation.json")
            if stats.get('partial_recall_questions'):
                msg = f"  - partial_recall_cases.json ({len(stats['partial_recall_questions'])} cases)"
                print(msg)
                logger.info(msg)
            zero_count = len([q for q in stats['per_question'] if q['recall'] == 0.0])
            if zero_count > 0:
                msg = f"  - bad_cases_zero_recall.json ({zero_count} cases)"
                print(msg)
                logger.info(msg)

        # åˆ·æ–°æ—¥å¿—åˆ°æ–‡ä»¶
        for handler in logger.handlers:
            handler.flush()

    finally:
        # 6. æ¸…ç†èµ„æº
        await evaluator.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
