#!/usr/bin/env python3
"""
Entity-Event-Chunk Recall æµ‹è¯•è„šæœ¬

å¬å›é“¾è·¯ï¼šQuery â†’ Keyï¼ˆå‘é‡+åˆ†è¯ï¼‰â†’ Eventï¼ˆSQLå…³è”ï¼‰â†’ Chunkï¼ˆevent.chunk_idï¼‰

è¾“å…¥è¾“å‡ºä¸ query_recall.py ä¿æŒä¸€è‡´
"""

import json
import sys
import asyncio
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Tuple
from collections import defaultdict
import numpy as np

# Add project root to path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# Load environment variables
from dotenv import load_dotenv
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"[INFO] Loaded environment variables: {env_path}")
else:
    print(f"[WARN] .env file not found: {env_path}")

# Import ES and Embedding clients
from dataflow.core.storage.elasticsearch import ElasticsearchClient, ESConfig
from dataflow.core.ai.embedding import EmbeddingClient
from dataflow.core.storage.repositories.entity_repository import EntityVectorRepository
from dataflow.core.storage.repositories.event_repository import EventVectorRepository
from dataflow.core.storage.repositories.source_chunk_repository import SourceChunkRepository
from dataflow.core.ai.tokensize import extract_keywords
from dataflow.db import Entity, EventEntity, SourceEvent, SourceChunk, get_session_factory

# Import recall metrics
from evaluation.hotpotqa_evaluation.scripts.recall_metrics import RecallCalculator, RecallResult

# SQLAlchemy
from sqlalchemy import select, and_


# ==================== é…ç½®å‚æ•°ï¼ˆå‚è€ƒ recall.pyï¼‰ ====================

class RecallConfigParams:
    """å¬å›é…ç½®å‚æ•°"""
    # ç›¸ä¼¼åº¦é˜ˆå€¼
    entity_similarity_threshold: float = 0.4
    event_similarity_threshold: float = 0.4

    # æ•°é‡æ§åˆ¶
    vector_top_k: int = 40
    max_entities: int = 25
    max_events: int = 60

    # åˆ†è¯å™¨é…ç½®
    tokenizer_top_k: int = 15
    tokenizer_similarity: float = 0.99

    # æƒé‡å¹³è¡¡
    step4_event_key_balance: float = 0.5


# ==================== æœç´¢å™¨ä¸»ç±» ====================

class EntityEventRecallSearcher:
    """
    Entity-Event-Chunk å¬å›æœç´¢å™¨

    å®ç° 6 æ­¥å¬å›ç®—æ³•ï¼š
    1. Query â†’ Keyï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰
    2. Query â†’ Keyï¼ˆåˆ†è¯è¡¥å……ï¼‰
    3. Key å¹¶é›†æ’åº
    4. Key â†’ Eventï¼ˆSQLå…³è”ï¼‰
    5. Event æ’åº
    6. Event â†’ Chunk
    """

    def __init__(self):
        """åˆå§‹åŒ–æœç´¢å™¨"""
        # åˆå§‹åŒ– ES å®¢æˆ·ç«¯
        es_config = ESConfig.from_env()
        self.es_client = ElasticsearchClient(config=es_config)

        # åˆå§‹åŒ– Repositories
        self.entity_repo = EntityVectorRepository(self.es_client.client)
        self.event_repo = EventVectorRepository(self.es_client.client)
        self.chunk_repo = SourceChunkRepository(es_client=self.es_client)

        # åˆå§‹åŒ– Embedding å®¢æˆ·ç«¯
        self.embedding_client = EmbeddingClient()

        # æ•°æ®åº“ä¼šè¯å·¥å‚
        self.session_factory = get_session_factory()

        # é…ç½®å‚æ•°
        self.config = RecallConfigParams()

    @staticmethod
    def _cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦ï¼ŒèŒƒå›´ [-1, 1]
        """
        if not vec1 or not vec2:
            return 0.0

        try:
            v1 = np.array(vec1, dtype=np.float32)
            v2 = np.array(vec2, dtype=np.float32)

            if len(v1) != len(v2):
                return 0.0

            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            similarity = dot_product / (norm1 * norm2)
            return float(np.clip(similarity, -1.0, 1.0))

        except Exception as e:
            print(f"      [ERROR] ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    async def _step1_semantic_key_recall(
        self,
        query: str,
        query_vector: List[float],
        source_config_id: str
    ) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
        """
        Step 1: Query â†’ Keyï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦å¬å›ï¼‰

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            query_vector: æŸ¥è¯¢å‘é‡
            source_config_id: æ•°æ®æºé…ç½®ID

        Returns:
            (key_results, k1_weights) - å¬å›çš„ key åˆ—è¡¨å’Œæƒé‡æ˜ å°„
        """
        print(f"   [Step1] è¯­ä¹‰ç›¸ä¼¼åº¦å¬å› Key...")

        # å‘é‡æœç´¢
        similar_entities = await self.entity_repo.search_similar(
            query_vector=query_vector,
            k=self.config.vector_top_k,
            source_config_ids=[source_config_id],
            include_type_threshold=False
        )

        # è¿‡æ»¤å¹¶æ„å»ºç»“æœ
        key_results = []
        k1_weights = {}

        for entity in similar_entities:
            similarity = float(entity.get("_score", 0.0))
            if similarity >= self.config.entity_similarity_threshold:
                entity_id = entity.get("entity_id", "")
                key_results.append({
                    "entity_id": entity_id,
                    "name": entity.get("name", ""),
                    "type": entity.get("type", ""),
                    "similarity": similarity,
                    "source_method": "semantic"
                })
                k1_weights[entity_id] = similarity

        print(f"      å‘é‡å¬å› {len(similar_entities)} ä¸ªï¼Œé˜ˆå€¼è¿‡æ»¤å {len(key_results)} ä¸ª")
        return key_results, k1_weights

    async def _step2_tokenizer_key_recall(
        self,
        query: str,
        source_config_id: str,
        existing_entity_ids: Set[str]
    ) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
        """
        Step 2: Query â†’ Keyï¼ˆåˆ†è¯è¡¥å……å¬å›ï¼‰

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            source_config_id: æ•°æ®æºé…ç½®ID
            existing_entity_ids: å·²å¬å›çš„ entity_id é›†åˆ

        Returns:
            (key_results, k1_weights) - æ–°å¢çš„ key åˆ—è¡¨å’Œæƒé‡æ˜ å°„
        """
        print(f"   [Step2] åˆ†è¯è¡¥å……å¬å› Key...")

        # æå–å…³é”®è¯
        keywords = extract_keywords(query, top_k=self.config.tokenizer_top_k, mode="tokenizer")
        print(f"      æå–å…³é”®è¯: {keywords}")

        if not keywords:
            return [], {}

        # SQL å‰ç¼€åŒ¹é…
        key_results = []
        k1_weights = {}

        async with self.session_factory() as session:
            for keyword in keywords:
                # å‰ç¼€åŒ¹é…æŸ¥è¯¢
                stmt = (
                    select(Entity)
                    .where(
                        and_(
                            Entity.source_config_id == source_config_id,
                            Entity.normalized_name.like(f"{keyword.lower()}%")
                        )
                    )
                    .limit(2)  # æ¯ä¸ªå…³é”®è¯æœ€å¤šåŒ¹é… 2 ä¸ª
                )
                result = await session.execute(stmt)
                entities = result.scalars().all()

                for entity in entities:
                    if entity.id not in existing_entity_ids:
                        existing_entity_ids.add(entity.id)
                        key_results.append({
                            "entity_id": entity.id,
                            "name": entity.name,
                            "type": entity.type,
                            "similarity": 0.99,
                            "source_method": "tokenizer"
                        })
                        k1_weights[entity.id] = 0.99

        print(f"      åˆ†è¯å¬å›æ–°å¢ {len(key_results)} ä¸ª Key")
        return key_results, k1_weights

    def _step3_union_and_rank_keys(
        self,
        semantic_keys: List[Dict[str, Any]],
        tokenizer_keys: List[Dict[str, Any]],
        semantic_weights: Dict[str, float],
        tokenizer_weights: Dict[str, float]
    ) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
        """
        Step 3: Key å¹¶é›†æ’åº

        Args:
            semantic_keys: è¯­ä¹‰å¬å›çš„ key
            tokenizer_keys: åˆ†è¯å¬å›çš„ key
            semantic_weights: è¯­ä¹‰å¬å›æƒé‡
            tokenizer_weights: åˆ†è¯å¬å›æƒé‡

        Returns:
            (final_keys, final_weights) - æ’åºåçš„ key åˆ—è¡¨å’Œæƒé‡æ˜ å°„
        """
        print(f"   [Step3] Key å¹¶é›†æ’åº...")

        # åˆå¹¶ï¼ˆè¯­ä¹‰ä¼˜å…ˆï¼Œå·²åœ¨ step2 ä¸­å»é‡ï¼‰
        merged_keys = semantic_keys + tokenizer_keys
        merged_weights = {**semantic_weights, **tokenizer_weights}

        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        merged_keys.sort(key=lambda x: x["similarity"], reverse=True)

        # é™åˆ¶æ•°é‡
        final_keys = merged_keys[:self.config.max_entities]
        final_weights = {k["entity_id"]: merged_weights[k["entity_id"]] for k in final_keys}

        print(f"      åˆå¹¶å {len(merged_keys)} ä¸ªï¼Œå–å‰ {len(final_keys)} ä¸ª")
        return final_keys, final_weights

    async def _step4_keys_to_events(
        self,
        key_results: List[Dict[str, Any]],
        source_config_id: str
    ) -> Tuple[List[str], Dict[str, List[str]]]:
        """
        Step 4: Key â†’ Eventï¼ˆSQL å…³è”ï¼‰

        Args:
            key_results: Key åˆ—è¡¨
            source_config_id: æ•°æ®æºé…ç½®ID

        Returns:
            (event_ids, event_to_entities) - event_id åˆ—è¡¨å’Œ eventâ†’entities æ˜ å°„
        """
        print(f"   [Step4] Key â†’ Event SQL å…³è”...")

        entity_ids = [k["entity_id"] for k in key_results]

        if not entity_ids:
            return [], {}

        async with self.session_factory() as session:
            # æŸ¥è¯¢ EventEntity å…³è”è¡¨
            stmt = (
                select(EventEntity.event_id, EventEntity.entity_id)
                .where(EventEntity.entity_id.in_(entity_ids))
            )
            result = await session.execute(stmt)
            pairs = result.fetchall()

            # æ„å»ºæ˜ å°„
            event_ids = list(set(event_id for event_id, _ in pairs))
            event_to_entities: Dict[str, List[str]] = defaultdict(list)
            for event_id, entity_id in pairs:
                event_to_entities[event_id].append(entity_id)

        print(f"      æ‰¾åˆ° {len(event_ids)} ä¸ªå…³è”çš„ Event")
        return event_ids, dict(event_to_entities)

    async def _step5_rank_events(
        self,
        event_ids: List[str],
        event_to_entities: Dict[str, List[str]],
        k1_weights: Dict[str, float],
        query_vector: List[float],
        source_config_id: str
    ) -> List[Tuple[str, float]]:
        """
        Step 5: Event æ’åºï¼ˆrecall.py å…¬å¼ï¼‰

        å…¬å¼ï¼ševent_weight = 0.5 * e1_weight + 0.5 * key_weight_sum

        Args:
            event_ids: Event ID åˆ—è¡¨
            event_to_entities: Event â†’ Entity æ˜ å°„
            k1_weights: Key æƒé‡æ˜ å°„
            query_vector: æŸ¥è¯¢å‘é‡
            source_config_id: æ•°æ®æºé…ç½®ID

        Returns:
            æ’åºåçš„ (event_id, weight) åˆ—è¡¨
        """
        print(f"   [Step5] Event æ’åº...")

        if not event_ids:
            return []

        # è·å– Event å‘é‡
        events_data = await self.event_repo.get_events_by_ids(event_ids)
        event_vectors = {
            e.get("event_id"): e.get("content_vector")
            for e in events_data if e.get("event_id") and e.get("content_vector")
        }

        # è®¡ç®— Event æƒé‡
        event_weights = {}
        balance = self.config.step4_event_key_balance

        for event_id in event_ids:
            # e1_weight: event ä¸ query çš„ç›¸ä¼¼åº¦
            content_vector = event_vectors.get(event_id)
            if content_vector:
                e1_weight = self._cosine_similarity(query_vector, content_vector)
            else:
                e1_weight = 0.0

            # key_weight_sum: å…³è” key çš„æƒé‡ä¹‹å’Œ
            event_keys = event_to_entities.get(event_id, [])
            key_weight_sum = sum(k1_weights.get(key_id, 0.0) for key_id in event_keys)

            # ç»¼åˆæƒé‡
            combined_weight = balance * e1_weight + (1 - balance) * key_weight_sum
            event_weights[event_id] = combined_weight

        # æ’åºå¹¶é™åˆ¶æ•°é‡
        sorted_events = sorted(event_weights.items(), key=lambda x: x[1], reverse=True)
        final_events = sorted_events[:self.config.max_events]

        print(f"      è®¡ç®—å {len(event_weights)} ä¸ªï¼Œå–å‰ {len(final_events)} ä¸ª")
        return final_events

    async def _step6_events_to_chunks(
        self,
        ranked_events: List[Tuple[str, float]],
        source_config_id: str,
        top_k: int
    ) -> List[Dict[str, Any]]:
        """
        Step 6: Event â†’ Chunk

        Args:
            ranked_events: æ’åºåçš„ (event_id, weight) åˆ—è¡¨
            source_config_id: æ•°æ®æºé…ç½®ID
            top_k: è¿”å›æ•°é‡

        Returns:
            Chunk åˆ—è¡¨
        """
        print(f"   [Step6] Event â†’ Chunk...")

        if not ranked_events:
            return []

        event_ids = [event_id for event_id, _ in ranked_events]
        event_weight_map = {event_id: weight for event_id, weight in ranked_events}

        async with self.session_factory() as session:
            # è·å– Event çš„ chunk_id
            stmt = (
                select(SourceEvent)
                .where(
                    and_(
                        SourceEvent.source_config_id == source_config_id,
                        SourceEvent.id.in_(event_ids)
                    )
                )
            )
            result = await session.execute(stmt)
            events = result.scalars().all()

            # æ„å»º event_id â†’ chunk_id æ˜ å°„ï¼ˆä¿æŒ event é¡ºåºï¼‰
            event_to_chunk = {}
            chunk_ids = []
            for event in events:
                if event.chunk_id:
                    event_to_chunk[event.id] = event.chunk_id
                    if event.chunk_id not in chunk_ids:
                        chunk_ids.append(event.chunk_id)

            # è·å– Chunk è¯¦æƒ…
            if not chunk_ids:
                print(f"      æ²¡æœ‰æ‰¾åˆ°å…³è”çš„ Chunk")
                return []

            chunks_stmt = (
                select(SourceChunk)
                .where(SourceChunk.id.in_(chunk_ids))
            )
            chunks_result = await session.execute(chunks_stmt)
            chunks = chunks_result.scalars().all()
            chunk_map = {chunk.id: chunk for chunk in chunks}

        # æŒ‰ event æƒé‡é¡ºåºè¾“å‡º Chunk
        results = []
        seen_chunks = set()

        for event_id, event_weight in ranked_events:
            chunk_id = event_to_chunk.get(event_id)
            if chunk_id and chunk_id not in seen_chunks:
                chunk = chunk_map.get(chunk_id)
                if chunk:
                    seen_chunks.add(chunk_id)
                    results.append({
                        'chunk_id': chunk_id,
                        'heading': chunk.heading or '',
                        'content': chunk.content or '',
                        'score': event_weight,
                        'weight': event_weight,
                        'event_id': event_id
                    })

                    if len(results) >= top_k:
                        break

        print(f"      è¿”å› {len(results)} ä¸ª Chunk")
        return results

    async def search(
        self,
        query: str,
        source_config_id: str,
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ä¸»æœç´¢æ–¹æ³•

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            source_config_id: æ•°æ®æºé…ç½®ID
            top_k: è¿”å›æ®µè½æ•°é‡

        Returns:
            æ®µè½åˆ—è¡¨ï¼ŒåŒ…å« chunk_id, heading, content, score ç­‰å­—æ®µ
        """
        # ç”Ÿæˆ query embedding
        query_vector = await self.embedding_client.generate(query)
        print(f"      [DEBUG] query_vector ç»´åº¦: {len(query_vector)}")

        # Step 1: è¯­ä¹‰å¬å› Key
        semantic_keys, semantic_weights = await self._step1_semantic_key_recall(
            query, query_vector, source_config_id
        )

        # Step 2: åˆ†è¯è¡¥å……å¬å› Key
        existing_ids = {k["entity_id"] for k in semantic_keys}
        tokenizer_keys, tokenizer_weights = await self._step2_tokenizer_key_recall(
            query, source_config_id, existing_ids
        )

        # Step 3: Key å¹¶é›†æ’åº
        final_keys, k1_weights = self._step3_union_and_rank_keys(
            semantic_keys, tokenizer_keys, semantic_weights, tokenizer_weights
        )

        if not final_keys:
            print(f"      [WARN] æ²¡æœ‰å¬å›ä»»ä½• Key")
            return []

        # Step 4: Key â†’ Event
        event_ids, event_to_entities = await self._step4_keys_to_events(
            final_keys, source_config_id
        )

        if not event_ids:
            print(f"      [WARN] æ²¡æœ‰å…³è”åˆ°ä»»ä½• Event")
            return []

        # Step 5: Event æ’åº
        ranked_events = await self._step5_rank_events(
            event_ids, event_to_entities, k1_weights, query_vector, source_config_id
        )

        if not ranked_events:
            print(f"      [WARN] Event æ’åºåä¸ºç©º")
            return []

        # Step 6: Event â†’ Chunk
        chunks = await self._step6_events_to_chunks(
            ranked_events, source_config_id, top_k
        )

        return chunks

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self.es_client, 'client') and self.es_client.client:
            await self.es_client.client.close()


# ==================== è¾…åŠ©å‡½æ•°ï¼ˆä¸ query_recall.py ä¸€è‡´ï¼‰ ====================

def load_corpus(corpus_path: Path) -> Dict[str, Dict[str, str]]:
    """åŠ è½½è¯­æ–™åº“"""
    print(f"[INFO] åŠ è½½è¯­æ–™åº“: {corpus_path}")
    corpus_dict = {}
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if line.strip():
                try:
                    chunk = json.loads(line)
                    chunk_id = chunk['id']

                    # å¤„ç†åˆå¹¶çš„ID
                    if "//" in chunk_id:
                        original_ids = chunk_id.split("//")
                        for original_id in original_ids:
                            corpus_dict[original_id] = {
                                'title': chunk.get('title', ''),
                                'text': chunk.get('text', '')
                            }
                    else:
                        corpus_dict[chunk_id] = {
                            'title': chunk.get('title', ''),
                            'text': chunk.get('text', '')
                        }
                except json.JSONDecodeError as e:
                    print(f"[WARN] ç¬¬ {line_num} è¡ŒJSONè§£æå¤±è´¥: {e}")

    print(f"[INFO] è¯­æ–™åº“åŠ è½½å®Œæˆï¼Œå…± {len(corpus_dict)} ä¸ªæ®µè½")
    return corpus_dict


def load_questions(questions_path: Path) -> List[Dict[str, Any]]:
    """åŠ è½½é—®é¢˜åˆ—è¡¨"""
    print(f"[INFO] åŠ è½½é—®é¢˜åˆ—è¡¨: {questions_path}")
    questions = []
    with open(questions_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if line.strip():
                try:
                    q = json.loads(line)
                    questions.append(q)
                except json.JSONDecodeError as e:
                    print(f"[WARN] ç¬¬ {line_num} è¡ŒJSONè§£æå¤±è´¥: {e}")

    print(f"[INFO] é—®é¢˜åŠ è½½å®Œæˆï¼Œå…± {len(questions)} ä¸ªé—®é¢˜")
    return questions


def find_latest_source_dir(base_dir: Path) -> Path:
    """æŸ¥æ‰¾æœ€æ–°çš„æºæ•°æ®æ–‡ä»¶å¤¹"""
    print(f"[INFO] æŸ¥æ‰¾æœ€æ–°çš„æºæ•°æ®æ–‡ä»¶å¤¹: {base_dir}")

    if not base_dir.exists():
        print(f"[ERROR] æºæ•°æ®ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        sys.exit(1)

    dirs = [d for d in base_dir.iterdir() if d.is_dir()]

    if not dirs:
        print(f"[ERROR] æºæ•°æ®ç›®å½•ä¸‹æ²¡æœ‰æ–‡ä»¶å¤¹: {base_dir}")
        sys.exit(1)

    dirs.sort(key=lambda x: x.name, reverse=True)
    latest_dir = dirs[0]
    print(f"[INFO] æ‰¾åˆ°æœ€æ–°æ–‡ä»¶å¤¹: {latest_dir.name}")

    return latest_dir


# ==================== ä¸»å‡½æ•° ====================

async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='Entity-Event-Chunk Recall æµ‹è¯•è„šæœ¬')
    parser.add_argument('--source-config-id', type=str, required=False, default=None,
                       help='æ•°æ®æºé…ç½®ID')
    parser.add_argument('--input', type=Path, required=False, default=None,
                       help='é—®é¢˜åˆ—è¡¨æ–‡ä»¶è·¯å¾„ (JSONLæ ¼å¼)')
    parser.add_argument('--corpus', type=Path, required=False, default=None,
                       help='è¯­æ–™åº“æ–‡ä»¶è·¯å¾„ (JSONLæ ¼å¼)')
    parser.add_argument('--source-dir', type=Path, required=False, default=None,
                       help='æºæ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--max-questions', type=int, default=None,
                       help='æœ€å¤§å¤„ç†é—®é¢˜æ•°')
    parser.add_argument('--top-k', type=int, default=10,
                       help='æ¯ä¸ªé—®é¢˜è¿”å›çš„æ®µè½æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰')
    parser.add_argument('--output', type=Path, required=False, default=None,
                       help='ä¿å­˜æœç´¢ç»“æœçš„æ–‡ä»¶è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼‰')

    args = parser.parse_args()

    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ä½œä¸ºåŸºå‡†
    script_dir = Path(__file__).parent
    eval_base_dir = script_dir.parent

    # ç¡®å®šæºæ•°æ®ç›®å½•
    if args.source_dir:
        source_dir = Path(args.source_dir)
        print(f"[INFO] ä½¿ç”¨æŒ‡å®šçš„æºæ•°æ®ç›®å½•: {source_dir}")
    else:
        base_dir = eval_base_dir / "data" / "source"
        source_dir = find_latest_source_dir(base_dir)
        print(f"[INFO] è‡ªåŠ¨é€‰æ‹©æœ€æ–°æºæ•°æ®ç›®å½•: {source_dir}")

    # ç¡®å®š source_config_id
    if args.source_config_id:
        source_config_id = args.source_config_id
        print(f"[INFO] ä½¿ç”¨æŒ‡å®šçš„ source_config_id: {source_config_id}")
    else:
        process_result_file = source_dir / "process_result.json"
        if process_result_file.exists():
            with open(process_result_file, 'r', encoding='utf-8') as f:
                process_result = json.load(f)
                source_config_id = process_result.get('source_config_id')
                if source_config_id:
                    print(f"[INFO] ä» process_result.json è¯»å– source_config_id: {source_config_id}")
                else:
                    print(f"[ERROR] process_result.json ä¸­æ²¡æœ‰ source_config_id å­—æ®µ")
                    sys.exit(1)
        else:
            print(f"[ERROR] æœªæ‰¾åˆ° process_result.json: {process_result_file}")
            sys.exit(1)

    # ç¡®å®šè¾“å…¥æ–‡ä»¶
    if args.input:
        input_file = Path(args.input)
    else:
        input_file = source_dir / "oracle.jsonl"
        if not input_file.exists():
            print(f"[ERROR] æºæ•°æ®ç›®å½•ä¸‹æ‰¾ä¸åˆ° oracle.jsonl: {input_file}")
            sys.exit(1)
        print(f"[INFO] ä½¿ç”¨æºæ•°æ®ç›®å½•ä¸‹çš„ oracle.jsonl: {input_file}")

    # ç¡®å®šè¯­æ–™åº“æ–‡ä»¶
    if args.corpus:
        corpus_file = Path(args.corpus)
    else:
        corpus_file = source_dir / "corpus.jsonl"
        if not corpus_file.exists():
            print(f"[ERROR] æºæ•°æ®ç›®å½•ä¸‹æ‰¾ä¸åˆ° corpus.jsonl: {corpus_file}")
            sys.exit(1)
        print(f"[INFO] ä½¿ç”¨æºæ•°æ®ç›®å½•ä¸‹çš„ corpus.jsonl: {corpus_file}")

    # éªŒè¯æ–‡ä»¶
    if not input_file.exists():
        print(f"[ERROR] é—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        sys.exit(1)

    if not corpus_file.exists():
        print(f"[ERROR] è¯­æ–™åº“æ–‡ä»¶ä¸å­˜åœ¨: {corpus_file}")
        sys.exit(1)

    # åŠ è½½æ•°æ®
    corpus_dict = load_corpus(corpus_file)
    questions = load_questions(input_file)

    # é™åˆ¶é—®é¢˜æ•°
    if args.max_questions:
        questions = questions[:args.max_questions]
        print(f"[INFO] é™åˆ¶å¤„ç†å‰ {args.max_questions} ä¸ªé—®é¢˜")

    # è¾“å‡ºé—®é¢˜å’Œç­”æ¡ˆ
    print("\n" + "="*80)
    print("å¼€å§‹å¤„ç†é—®é¢˜å¹¶æ‰§è¡Œ Entity-Event-Chunk å¬å›")
    print("="*80)

    # åˆå§‹åŒ–æœç´¢å™¨
    print("\n[INFO] åˆå§‹åŒ– Entity-Event-Chunk æœç´¢å™¨...")
    searcher = EntityEventRecallSearcher()
    print("[INFO] æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ")

    # ç”¨äºä¿å­˜æ‰€æœ‰ç»“æœ
    all_results = []
    total_recall = 0.0
    total_questions_with_oracle = 0

    for i, q in enumerate(questions, 1):
        question_id = q.get('id', 'unknown')
        question = q.get('question', '')
        answer = q.get('answer', '')
        oracle_chunk_ids = q.get('oracle_chunk_ids', [])

        print(f"\n{'='*80}")
        print(f"[{i}/{len(questions)}] ID: {question_id}")
        print(f"â“ é—®é¢˜: {question}")
        print(f"âœ… ç­”æ¡ˆ: {answer}")
        print(f"ğŸ“Œ æ ‡å‡†ç­”æ¡ˆæ®µè½æ•°: {len(oracle_chunk_ids)}")

        # æ˜¾ç¤ºæ ‡å‡†ç­”æ¡ˆæ®µè½çš„è¯¦ç»†å†…å®¹
        if oracle_chunk_ids and corpus_dict:
            print(f"\n   æ ‡å‡†ç­”æ¡ˆæ®µè½:")
            for j, chunk_id in enumerate(oracle_chunk_ids, 1):
                if chunk_id in corpus_dict:
                    chunk = corpus_dict[chunk_id]
                    title = chunk.get('title', 'N/A')
                    text = chunk.get('text', '')[:200]

                    print(f"   [{j}] {title}")
                    print(f"       {text}...")
                else:
                    print(f"   [{j}] chunk_id: {chunk_id} (æœªæ‰¾åˆ°å¯¹åº”æ®µè½)")

        # æ‰§è¡Œæœç´¢
        print(f"\nğŸ” æ‰§è¡Œ Entity-Event-Chunk å¬å› (top-{args.top_k})...")
        try:
            start_time = time.time()
            search_results = await searcher.search(
                query=question,
                source_config_id=source_config_id,
                top_k=args.top_k
            )
            search_time = time.time() - start_time

            print(f"   æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªæ®µè½ (è€—æ—¶: {search_time:.3f}ç§’)")

            # å‡†å¤‡å¬å›ç‡è®¡ç®—æ‰€éœ€çš„æ•°æ®
            oracle_chunks_for_calc = []
            for chunk_id in oracle_chunk_ids:
                if chunk_id in corpus_dict:
                    chunk = corpus_dict[chunk_id]
                    oracle_chunks_for_calc.append({
                        'chunk_id': chunk_id,
                        'title': chunk.get('title', ''),
                        'text': chunk.get('text', '')
                    })

            # ä½¿ç”¨ RecallCalculator è®¡ç®—å¬å›ç‡
            recall_result = RecallCalculator.calculate(
                question_id=question_id,
                oracle_chunks=oracle_chunks_for_calc,
                retrieved_sections=search_results,
                verbose=False
            )

            # æ˜¾ç¤ºæœç´¢ç»“æœ
            print(f"\n   æ£€ç´¢ç»“æœ:")
            for idx, section in enumerate(search_results, 1):
                chunk_id = section.get('chunk_id', '')
                heading = section.get('heading', 'N/A')
                content = section.get('content', '')[:200]
                score = section.get('score', 0.0)

                # æ£€æŸ¥æ˜¯å¦å‘½ä¸­æ ‡å‡†ç­”æ¡ˆ
                is_hit = ''
                for detail in recall_result.recalled_details:
                    if detail.get('retrieved_chunk_id') == chunk_id:
                        is_hit = 'âœ… å‘½ä¸­'
                        break

                print(f"   [{idx}] {heading} {is_hit}")
                print(f"       Score: {score:.4f}")
                print(f"       å†…å®¹: {content}...")

            # æ˜¾ç¤ºå¬å›ç»Ÿè®¡
            print(f"\n   ğŸ“Š å¬å›ç»Ÿè®¡: {recall_result.recalled}/{recall_result.total_oracle} ({recall_result.recall:.2%})")
            if recall_result.recalled_details:
                print(f"   åŒ¹é…è¯¦æƒ…:")
                for detail in recall_result.recalled_details[:5]:
                    print(f"      - {detail['oracle_title']} <-> {detail['retrieved_heading']}")
                if len(recall_result.recalled_details) > 5:
                    print(f"      ... è¿˜æœ‰ {len(recall_result.recalled_details) - 5} ä¸ªåŒ¹é…")

            # ç´¯è®¡ç»Ÿè®¡
            if recall_result.total_oracle > 0:
                total_recall += recall_result.recall
                total_questions_with_oracle += 1

            # ä¿å­˜ç»“æœ
            result = {
                'question_id': question_id,
                'question': question,
                'answer': answer,
                'oracle_chunk_ids': oracle_chunk_ids,
                'search_results': [
                    {
                        'chunk_id': s.get('chunk_id', ''),
                        'heading': s.get('heading', ''),
                        'content': s.get('content', ''),
                        'score': s.get('score', 0.0),
                        'cosine_similarity': None
                    }
                    for s in search_results
                ],
                'recall': recall_result.recall,
                'total_oracle': recall_result.total_oracle,
                'recalled': recall_result.recalled,
                'retrieved': recall_result.retrieved,
                'recalled_details': recall_result.recalled_details,
                'search_time': search_time
            }
            all_results.append(result)

        except Exception as e:
            print(f"   âš ï¸ æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

            all_results.append({
                'question_id': question_id,
                'question': question,
                'answer': answer,
                'oracle_chunk_ids': oracle_chunk_ids,
                'error': str(e),
                'search_results': []
            })

    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    print("\n" + "="*80)
    print("ğŸ“Š æ€»ä½“ç»Ÿè®¡")
    print("="*80)
    print(f"æ€»é—®é¢˜æ•°: {len(questions)}")

    if total_questions_with_oracle > 0:
        avg_recall = total_recall / total_questions_with_oracle
        print(f"å¹³å‡å¬å›ç‡: {avg_recall:.2%}")
        print(f"  è®¡ç®—æ–¹å¼: å®å¹³å‡ (Macro Average)")

        # ç»Ÿè®¡ä¸åŒå¬å›æƒ…å†µçš„é—®é¢˜æ•°
        perfect_recall = 0
        partial_recall = 0
        zero_recall = 0

        for result in all_results:
            if 'error' in result:
                continue

            recall = result.get('recall', 0.0)
            if recall >= 1.0:
                perfect_recall += 1
            elif recall > 0.0:
                partial_recall += 1
            else:
                zero_recall += 1

        print(f"\nå¬å›æƒ…å†µåˆ†å¸ƒ:")
        print(f"  âœ… å®Œç¾å¬å› (100%): {perfect_recall} ä¸ªé—®é¢˜ ({perfect_recall/total_questions_with_oracle:.2%})")
        print(f"  ğŸ”¶ éƒ¨åˆ†å¬å› (1%-99%): {partial_recall} ä¸ªé—®é¢˜ ({partial_recall/total_questions_with_oracle:.2%})")
        print(f"  âŒ é›¶å¬å› (0%): {zero_recall} ä¸ªé—®é¢˜ ({zero_recall/total_questions_with_oracle:.2%})")

    print(f"\næœ‰æ ‡å‡†ç­”æ¡ˆçš„é—®é¢˜æ•°: {total_questions_with_oracle}")
    print("="*80)

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    if args.output:
        print(f"\n[INFO] ä¿å­˜ç»“æœåˆ°: {args.output}")
        args.output.parent.mkdir(parents=True, exist_ok=True)
        with open(args.output, 'w', encoding='utf-8') as f:
            for result in all_results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        print(f"[INFO] ç»“æœå·²ä¿å­˜")

    # æ¸…ç†èµ„æº
    print("\n[INFO] æ¸…ç†æœç´¢å™¨èµ„æº...")
    await searcher.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
