#!/usr/bin/env python3
"""
Query Event Recall æµ‹è¯•è„šæœ¬

ç®€åŒ–ç‰ˆï¼šè¾“å‡ºé—®é¢˜ã€å¬å›çš„äº‹é¡¹å’Œå¯¹åº”çš„æ®µè½
"""

import json
import sys
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional
import numpy as np

# Add project root to path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# Load environment variables
from dotenv import load_dotenv
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"[INFO] Loaded environment variables: {env_path}")
else:
    print(f"[WARN] .env file not found: {env_path}")

# Import ES and Embedding clients
from dataflow.core.storage.elasticsearch import ElasticsearchClient, ESConfig
from dataflow.core.ai.embedding import EmbeddingClient
from dataflow.core.storage.repositories.event_repository import EventVectorRepository

# Import database
from dataflow.db import get_session_factory, SourceEvent, SourceChunk
from sqlalchemy import select, and_

# Import recall metrics
from evaluation.hotpotqa_evaluation.scripts.recall_metrics import RecallCalculator, RecallResult


class SimpleEventSearcher:
    """ç®€å•çš„äº‹é¡¹å‘é‡æœç´¢å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æœç´¢å™¨"""
        # åˆå§‹åŒ– ES å®¢æˆ·ç«¯
        es_client = ElasticsearchClient(config=ESConfig.from_env())

        # åˆå§‹åŒ– EventVectorRepository
        self.event_repo = EventVectorRepository(es_client=es_client)

        # åˆå§‹åŒ– Embedding å®¢æˆ·ç«¯
        self.embedding_client = EmbeddingClient()

        # åˆå§‹åŒ–æ•°æ®åº“ä¼šè¯å·¥å‚
        self.session_factory = get_session_factory()

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if not vec1 or not vec2:
            return 0.0

        try:
            v1 = np.array(vec1, dtype=np.float32)
            v2 = np.array(vec2, dtype=np.float32)

            if len(v1) != len(v2):
                return 0.0

            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            similarity = dot_product / (norm1 * norm2)
            return float(np.clip(similarity, -1.0, 1.0))

        except Exception as e:
            print(f"      [ERROR] ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    async def check_es_data(self):
        """æ£€æŸ¥ESä¸­çš„äº‹é¡¹æ•°æ®æƒ…å†µ"""
        es_client = self.event_repo.es_client.client

        # 1. è·å–æ€»æ•°
        count_result = await es_client.count(index="event_vectors")
        total_count = count_result['count']
        print(f"      [DEBUG] ESä¸­event_vectorsæ€»æ•°: {total_count}")

        # 2. èšåˆæŸ¥è¯¢ï¼Œè·å–æ‰€æœ‰ä¸åŒçš„source_config_id
        agg_query = {
            "size": 0,
            "aggs": {
                "source_configs": {
                    "terms": {
                        "field": "source_config_id",
                        "size": 100
                    }
                }
            }
        }

        response = await es_client.search(index="event_vectors", body=agg_query)

        if 'aggregations' in response and 'source_configs' in response['aggregations']:
            buckets = response['aggregations']['source_configs']['buckets']
            print(f"      [DEBUG] ESä¸­çš„source_config_idåˆ—è¡¨:")
            for bucket in buckets:
                print(f"         - {bucket['key']}: {bucket['doc_count']} ä¸ªäº‹é¡¹")

        # 3. å¦‚æœæ€»æ•°ä¸º0ï¼Œç»™å‡ºæç¤º
        if total_count == 0:
            print(f"      [WARNING] ESä¸­æ²¡æœ‰ä»»ä½•äº‹é¡¹æ•°æ®ï¼è¯·å…ˆå¯¼å…¥æ•°æ®ã€‚")

        return total_count

    async def search(
        self,
        query: str,
        source_config_id: str,
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """
        å‘é‡æœç´¢äº‹é¡¹

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            source_config_id: æ•°æ®æºé…ç½®ID
            top_k: è¿”å›äº‹é¡¹æ•°é‡

        Returns:
            äº‹é¡¹åˆ—è¡¨
        """
        # 1. ç”Ÿæˆ query çš„ embedding
        query_vector = await self.embedding_client.generate(query)

        # 2. ä½¿ç”¨ EventVectorRepository æœç´¢
        print(f"      [DEBUG] source_config_id: {source_config_id}, top_k: {top_k}")
        print(f"      [DEBUG] query_vectorç»´åº¦: {len(query_vector)}")

        results = await self.event_repo.search_similar_by_content(
            query_vector=query_vector,
            k=top_k,
            source_config_id=source_config_id
        )

        print(f"      [DEBUG] æœç´¢è¿”å›ç»“æœæ•°: {len(results)}")
        if results:
            print(f"      [DEBUG] ç¬¬ä¸€ä¸ªç»“æœå­—æ®µ: {list(results[0].keys())}")

        # 3. è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼ï¼Œå¹¶æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        events = []
        for hit in results:
            event_id = hit.get('event_id', '')
            es_score = hit.get('_score', 0.0)

            # è·å–äº‹é¡¹çš„ content_vectorï¼ˆä» ES ç»“æœä¸­ï¼‰
            content_vector = hit.get('content_vector', None)

            # æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            if content_vector:
                manual_cosine_similarity = self._cosine_similarity(query_vector, content_vector)
            else:
                manual_cosine_similarity = None
                print(f"      [WARN] event {event_id[:8]}... æ²¡æœ‰ content_vector")

            events.append({
                'event_id': event_id,
                'title': hit.get('title', ''),
                'content': hit.get('content', ''),
                'summary': hit.get('summary', ''),
                'category': hit.get('category', ''),
                'start_time': hit.get('start_time', ''),
                'end_time': hit.get('end_time', ''),
                'score': es_score,
                'cosine_similarity': manual_cosine_similarity,
                'weight': es_score
            })

        return events

    async def get_chunks_from_events(
        self,
        source_config_id: str,
        event_ids: List[str]
    ) -> List[Dict[str, Any]]:
        """
        ä»äº‹é¡¹åˆ—è¡¨è·å–å»é‡åçš„æ®µè½ä¿¡æ¯

        Args:
            source_config_id: ä¿¡æ¯æºID
            event_ids: äº‹é¡¹IDåˆ—è¡¨

        Returns:
            å»é‡åçš„æ®µè½åˆ—è¡¨
        """
        if not event_ids:
            return []

        async with self.session_factory() as session:
            # 1. æŸ¥è¯¢äº‹é¡¹ï¼Œè·å– chunk_id
            event_query = (
                select(SourceEvent.id, SourceEvent.chunk_id)
                .where(
                    and_(
                        SourceEvent.source_config_id == source_config_id,
                        SourceEvent.id.in_(event_ids),
                        SourceEvent.chunk_id.isnot(None)  # è¿‡æ»¤æ‰æ²¡æœ‰ chunk_id çš„äº‹é¡¹
                    )
                )
            )

            event_result = await session.execute(event_query)
            events = event_result.all()

            print(f"      [DEBUG] æŸ¥è¯¢åˆ° {len(events)} ä¸ªäº‹é¡¹æœ‰ chunk_id")

            # 2. æå–å¹¶å»é‡ chunk_id
            chunk_ids = list(set([e.chunk_id for e in events]))

            if not chunk_ids:
                print(f"      [WARN] æ²¡æœ‰æ‰¾åˆ°ä»»ä½• chunk_id")
                return []

            print(f"      [DEBUG] å»é‡åæœ‰ {len(chunk_ids)} ä¸ªå”¯ä¸€ chunk_id")

            # 3. æŸ¥è¯¢æ®µè½ä¿¡æ¯
            chunk_query = (
                select(SourceChunk)
                .where(
                    and_(
                        SourceChunk.source_config_id == source_config_id,
                        SourceChunk.id.in_(chunk_ids)
                    )
                )
                .order_by(SourceChunk.rank)  # æŒ‰ rank æ’åº
            )

            chunk_result = await session.execute(chunk_query)
            chunks = chunk_result.scalars().all()

            print(f"      [DEBUG] æŸ¥è¯¢åˆ° {len(chunks)} ä¸ªæ®µè½")

            # 4. è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            return [
                {
                    'chunk_id': chunk.id,
                    'heading': chunk.heading or '',
                    'content': chunk.content or '',
                    'rank': chunk.rank,
                    'source_config_id': chunk.source_config_id
                }
                for chunk in chunks
            ]

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self.event_repo, 'es_client') and hasattr(self.event_repo.es_client, 'client'):
            await self.event_repo.es_client.client.close()


def load_questions(questions_path: Path) -> List[Dict[str, Any]]:
    """åŠ è½½é—®é¢˜åˆ—è¡¨"""
    print(f"[INFO] åŠ è½½é—®é¢˜åˆ—è¡¨: {questions_path}")
    questions = []
    with open(questions_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if line.strip():
                try:
                    q = json.loads(line)
                    questions.append(q)
                except json.JSONDecodeError as e:
                    print(f"[WARN] ç¬¬ {line_num} è¡ŒJSONè§£æå¤±è´¥: {e}")

    print(f"[INFO] é—®é¢˜åŠ è½½å®Œæˆï¼Œå…± {len(questions)} ä¸ªé—®é¢˜")
    return questions


def load_corpus(corpus_path: Path) -> Dict[str, Dict[str, str]]:
    """åŠ è½½è¯­æ–™åº“"""
    print(f"[INFO] åŠ è½½è¯­æ–™åº“: {corpus_path}")
    corpus_dict = {}
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if line.strip():
                try:
                    chunk = json.loads(line)
                    chunk_id = chunk['id']

                    # å¤„ç†åˆå¹¶çš„ID
                    if "//" in chunk_id:
                        original_ids = chunk_id.split("//")
                        for original_id in original_ids:
                            corpus_dict[original_id] = {
                                'title': chunk.get('title', ''),
                                'text': chunk.get('text', '')
                            }
                    else:
                        corpus_dict[chunk_id] = {
                            'title': chunk.get('title', ''),
                            'text': chunk.get('text', '')
                        }
                except json.JSONDecodeError as e:
                    print(f"[WARN] ç¬¬ {line_num} è¡ŒJSONè§£æå¤±è´¥: {e}")

    print(f"[INFO] è¯­æ–™åº“åŠ è½½å®Œæˆï¼Œå…± {len(corpus_dict)} ä¸ªæ®µè½")
    return corpus_dict


def find_latest_source_dir(base_dir: Path) -> Path:
    """æŸ¥æ‰¾æœ€æ–°çš„æºæ•°æ®æ–‡ä»¶å¤¹"""
    print(f"[INFO] æŸ¥æ‰¾æœ€æ–°çš„æºæ•°æ®æ–‡ä»¶å¤¹: {base_dir}")

    if not base_dir.exists():
        print(f"[ERROR] æºæ•°æ®ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        sys.exit(1)

    dirs = [d for d in base_dir.iterdir() if d.is_dir()]

    if not dirs:
        print(f"[ERROR] æºæ•°æ®ç›®å½•ä¸‹æ²¡æœ‰æ–‡ä»¶å¤¹: {base_dir}")
        sys.exit(1)

    dirs.sort(key=lambda x: x.name, reverse=True)
    latest_dir = dirs[0]
    print(f"[INFO] æ‰¾åˆ°æœ€æ–°æ–‡ä»¶å¤¹: {latest_dir.name}")

    return latest_dir


async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='Query Event Recall æµ‹è¯•è„šæœ¬')
    parser.add_argument('--source-config-id', type=str, required=False, default=None,
                       help='æ•°æ®æºé…ç½®ID (å¦‚æœä¸æŒ‡å®šï¼Œå°†ä»æºç›®å½•çš„ process_result.json ä¸­è¯»å–)')
    parser.add_argument('--input', type=Path, required=False, default=None,
                       help='é—®é¢˜åˆ—è¡¨æ–‡ä»¶è·¯å¾„ (JSONLæ ¼å¼)ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨æœ€æ–°æºæ•°æ®ç›®å½•ä¸‹çš„ oracle.jsonl')
    parser.add_argument('--corpus', type=Path, required=False, default=None,
                       help='è¯­æ–™åº“æ–‡ä»¶è·¯å¾„ (JSONLæ ¼å¼)ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨æœ€æ–°æºæ•°æ®ç›®å½•ä¸‹çš„ corpus.jsonl')
    parser.add_argument('--source-dir', type=Path, required=False, default=None,
                       help='æºæ•°æ®ç›®å½•è·¯å¾„ (åŒ…å« oracle.jsonl)ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°ç›®å½•')
    parser.add_argument('--max-questions', type=int, default=None,
                       help='æœ€å¤§å¤„ç†é—®é¢˜æ•°ï¼ˆé»˜è®¤: å…¨éƒ¨ï¼‰')
    parser.add_argument('--top-k', type=int, default=100,
                       help='æ¯ä¸ªé—®é¢˜è¿”å›çš„äº‹é¡¹æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰')
    parser.add_argument('--output', type=Path, required=False, default=None,
                       help='ä¿å­˜æœç´¢ç»“æœçš„æ–‡ä»¶è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼‰')
    parser.add_argument('--show-events', action='store_true', default=False,
                       help='æ˜¯å¦æ˜¾ç¤ºå¬å›çš„äº‹é¡¹ä¿¡æ¯ï¼ˆé»˜è®¤: åªæ˜¾ç¤ºæ®µè½ï¼‰')

    args = parser.parse_args()

    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ä½œä¸ºåŸºå‡†
    script_dir = Path(__file__).parent
    eval_base_dir = script_dir.parent

    # ç¡®å®šæºæ•°æ®ç›®å½•
    if args.source_dir:
        source_dir = Path(args.source_dir)
        print(f"[INFO] ä½¿ç”¨æŒ‡å®šçš„æºæ•°æ®ç›®å½•: {source_dir}")
    else:
        base_dir = eval_base_dir / "data" / "source"
        source_dir = find_latest_source_dir(base_dir)
        print(f"[INFO] è‡ªåŠ¨é€‰æ‹©æœ€æ–°æºæ•°æ®ç›®å½•: {source_dir}")

    # ç¡®å®š source_config_id
    if args.source_config_id:
        source_config_id = args.source_config_id
        print(f"[INFO] ä½¿ç”¨æŒ‡å®šçš„ source_config_id: {source_config_id}")
    else:
        # ä» process_result.json ä¸­è¯»å–
        process_result_file = source_dir / "process_result.json"
        if process_result_file.exists():
            with open(process_result_file, 'r', encoding='utf-8') as f:
                process_result = json.load(f)
                source_config_id = process_result.get('source_config_id')
                if source_config_id:
                    print(f"[INFO] ä» process_result.json è¯»å– source_config_id: {source_config_id}")
                else:
                    print(f"[ERROR] process_result.json ä¸­æ²¡æœ‰ source_config_id å­—æ®µ")
                    sys.exit(1)
        else:
            print(f"[ERROR] æœªæ‰¾åˆ° process_result.json: {process_result_file}")
            print(f"[ERROR] è¯·ä½¿ç”¨ --source-config-id å‚æ•°æ‰‹åŠ¨æŒ‡å®š")
            sys.exit(1)

    # ç¡®å®šè¾“å…¥æ–‡ä»¶
    if args.input:
        input_file = Path(args.input)
    else:
        input_file = source_dir / "oracle.jsonl"
        if not input_file.exists():
            print(f"[ERROR] æºæ•°æ®ç›®å½•ä¸‹æ‰¾ä¸åˆ° oracle.jsonl: {input_file}")
            sys.exit(1)
        print(f"[INFO] ä½¿ç”¨æºæ•°æ®ç›®å½•ä¸‹çš„ oracle.jsonl: {input_file}")

    # éªŒè¯æ–‡ä»¶
    if not input_file.exists():
        print(f"[ERROR] é—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        sys.exit(1)

    # ç¡®å®šè¯­æ–™åº“æ–‡ä»¶
    if args.corpus:
        corpus_file = Path(args.corpus)
    else:
        corpus_file = source_dir / "corpus.jsonl"
        if not corpus_file.exists():
            print(f"[ERROR] æºæ•°æ®ç›®å½•ä¸‹æ‰¾ä¸åˆ° corpus.jsonl: {corpus_file}")
            sys.exit(1)
        print(f"[INFO] ä½¿ç”¨æºæ•°æ®ç›®å½•ä¸‹çš„ corpus.jsonl: {corpus_file}")

    if not corpus_file.exists():
        print(f"[ERROR] è¯­æ–™åº“æ–‡ä»¶ä¸å­˜åœ¨: {corpus_file}")
        sys.exit(1)

    # ç¡®å®šè¾“å‡ºç›®å½• (data/query_event_chunk/æ—¶é—´æˆ³)
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = eval_base_dir / "data" / "query_event_chunk" / timestamp
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"[INFO] è¾“å‡ºç›®å½•: {output_dir}")

    # è®¾ç½®è¾“å‡ºæ–‡ä»¶è·¯å¾„
    if args.output:
        output_file = Path(args.output)
    else:
        output_file = output_dir / "results.jsonl"
    print(f"[INFO] ç»“æœæ–‡ä»¶: {output_file}")

    # åŠ è½½æ•°æ®
    corpus_dict = load_corpus(corpus_file)
    questions = load_questions(input_file)

    # é™åˆ¶é—®é¢˜æ•°
    if args.max_questions:
        questions = questions[:args.max_questions]
        print(f"[INFO] é™åˆ¶å¤„ç†å‰ {args.max_questions} ä¸ªé—®é¢˜")

    # è¾“å‡ºé—®é¢˜å’Œå¬å›çš„äº‹é¡¹
    print("\n" + "="*80)
    print("å¼€å§‹å¤„ç†é—®é¢˜å¹¶æ‰§è¡Œäº‹é¡¹å‘é‡æœç´¢")
    print("="*80)

    # åˆå§‹åŒ–æœç´¢å™¨
    print("\n[INFO] åˆå§‹åŒ–äº‹é¡¹å‘é‡æœç´¢å™¨...")
    searcher = SimpleEventSearcher()
    print("[INFO] æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ")

    # æ£€æŸ¥ESæ•°æ®
    print("\n[INFO] æ£€æŸ¥ESä¸­çš„äº‹é¡¹æ•°æ®...")
    await searcher.check_es_data()
    print()

    # ç”¨äºä¿å­˜æ‰€æœ‰ç»“æœ
    all_results = []
    total_recall = 0.0
    total_questions_with_oracle = 0

    for i, q in enumerate(questions, 1):
        question_id = q.get('id', 'unknown')
        question = q.get('question', '')
        answer = q.get('answer', '')
        oracle_chunk_ids = q.get('oracle_chunk_ids', [])

        print(f"\n{'='*80}")
        print(f"[{i}/{len(questions)}] ID: {question_id}")
        print(f"â“ é—®é¢˜: {question}")
        print(f"âœ… ç­”æ¡ˆ: {answer}")
        print(f"ğŸ“Œ æ ‡å‡†ç­”æ¡ˆæ®µè½æ•°: {len(oracle_chunk_ids)}")

        # æ˜¾ç¤ºæ ‡å‡†ç­”æ¡ˆæ®µè½çš„è¯¦ç»†å†…å®¹
        if oracle_chunk_ids and corpus_dict:
            print(f"\n   æ ‡å‡†ç­”æ¡ˆæ®µè½:")
            for j, chunk_id in enumerate(oracle_chunk_ids, 1):
                if chunk_id in corpus_dict:
                    chunk = corpus_dict[chunk_id]
                    title = chunk.get('title', 'N/A')
                    text = chunk.get('text', '')[:200]  # é™åˆ¶é¢„è§ˆé•¿åº¦

                    print(f"   [{j}] {title}")
                    print(f"       {text}...")
                else:
                    print(f"   [{j}] chunk_id: {chunk_id} (æœªæ‰¾åˆ°å¯¹åº”æ®µè½)")

        # æ‰§è¡Œäº‹é¡¹å‘é‡æœç´¢
        print(f"\nğŸ” æ‰§è¡Œäº‹é¡¹å‘é‡æœç´¢ (top-{args.top_k})...")
        try:
            import time
            start_time = time.time()
            search_results = await searcher.search(
                query=question,
                source_config_id=source_config_id,
                top_k=args.top_k
            )
            search_time = time.time() - start_time

            print(f"   æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªäº‹é¡¹ (è€—æ—¶: {search_time:.3f}ç§’)")

            # æ˜¾ç¤ºæœç´¢ç»“æœï¼ˆå¯é€‰ï¼‰
            if args.show_events:
                print(f"\n   æ£€ç´¢åˆ°çš„äº‹é¡¹:")
                for idx, event in enumerate(search_results, 1):
                    event_id = event.get('event_id', '')
                    title = event.get('title', 'N/A')
                    content = event.get('content', '')[:200]  # é™åˆ¶é•¿åº¦
                    summary = event.get('summary', '')[:100]  # é™åˆ¶é•¿åº¦
                    category = event.get('category', 'N/A')
                    start_time_str = event.get('start_time', 'N/A')
                    es_score = event.get('score', 0.0)
                    manual_cosine = event.get('cosine_similarity', None)

                    print(f"   [{idx}] {title}")
                    print(f"       äº‹é¡¹ID: {event_id[:16]}...")
                    print(f"       åˆ†ç±»: {category}")
                    print(f"       æ—¶é—´: {start_time_str}")
                    if summary:
                        print(f"       æ‘˜è¦: {summary}...")
                    print(f"       ES Score: {es_score:.4f}")
                    if manual_cosine is not None:
                        print(f"       Cosine Similarity: {manual_cosine:.4f}")
                        diff = abs(es_score - manual_cosine)
                        print(f"       å·®å¼‚: {diff:.4f}")
                    else:
                        print(f"       Cosine Similarity: N/A (æ— å‘é‡)")
                    print(f"       å†…å®¹: {content}...")

            # ğŸ†• è·å–æ®µè½ä¿¡æ¯
            print(f"\nğŸ“„ ä»äº‹é¡¹ä¸­æå–æ®µè½ä¿¡æ¯...")
            event_ids = [e.get('event_id', '') for e in search_results]
            chunks = await searcher.get_chunks_from_events(
                source_config_id=source_config_id,
                event_ids=event_ids
            )

            print(f"   æå–åˆ° {len(chunks)} ä¸ªå”¯ä¸€æ®µè½")

            # å‡†å¤‡å¬å›ç‡è®¡ç®—æ‰€éœ€çš„æ•°æ®
            # 1. å‡†å¤‡ oracle chunksï¼ˆæ ‡å‡†ç­”æ¡ˆæ®µè½ï¼‰
            oracle_chunks_for_calc = []
            for chunk_id in oracle_chunk_ids:
                if chunk_id in corpus_dict:
                    chunk = corpus_dict[chunk_id]
                    oracle_chunks_for_calc.append({
                        'chunk_id': chunk_id,
                        'title': chunk.get('title', ''),
                        'text': chunk.get('text', '')
                    })

            # 2. å°†æå–çš„ chunks è½¬æ¢ä¸ºå¬å›è®¡ç®—æ ¼å¼
            retrieved_sections = [
                {
                    'chunk_id': c.get('chunk_id', ''),
                    'heading': c.get('heading', ''),
                    'content': c.get('content', ''),
                    'score': 1.0  # æ®µè½é€šè¿‡äº‹é¡¹å¬å›ï¼Œä¸ä½¿ç”¨åˆ†æ•°æ’åº
                }
                for c in chunks
            ]

            # 3. ä½¿ç”¨ RecallCalculator è®¡ç®—å¬å›ç‡
            recall_result = RecallCalculator.calculate(
                question_id=question_id,
                oracle_chunks=oracle_chunks_for_calc,
                retrieved_sections=retrieved_sections,
                verbose=False
            )

            # æ˜¾ç¤ºå¬å›ç»Ÿè®¡
            print(f"\n   ğŸ“Š å¬å›ç»Ÿè®¡: {recall_result.recalled}/{recall_result.total_oracle} ({recall_result.recall:.2%})")
            if recall_result.recalled_details:
                print(f"   åŒ¹é…è¯¦æƒ…:")
                for detail in recall_result.recalled_details[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"      - {detail['oracle_title']} <-> {detail['retrieved_heading']}")
                if len(recall_result.recalled_details) > 5:
                    print(f"      ... è¿˜æœ‰ {len(recall_result.recalled_details) - 5} ä¸ªåŒ¹é…")

            if chunks:
                print(f"\n   æ®µè½è¯¦æƒ…:")
                for idx, chunk in enumerate(chunks, 1):
                    chunk_id = chunk.get('chunk_id', '')
                    heading = chunk.get('heading', 'N/A')
                    content = chunk.get('content', '')[:300]  # é™åˆ¶é•¿åº¦
                    rank = chunk.get('rank', 0)

                    # æ£€æŸ¥æ˜¯å¦å‘½ä¸­æ ‡å‡†ç­”æ¡ˆ
                    is_hit = ''
                    for detail in recall_result.recalled_details:
                        if detail.get('retrieved_chunk_id') == chunk_id:
                            is_hit = 'âœ… å‘½ä¸­'
                            break

                    print(f"   [{idx}] {heading} {is_hit}")
                    print(f"       æ®µè½ID: {chunk_id[:16]}...")
                    print(f"       æ’åº: {rank}")
                    print(f"       å†…å®¹: {content}...")
                    print()

            # ç´¯è®¡ç»Ÿè®¡
            if recall_result.total_oracle > 0:
                total_recall += recall_result.recall
                total_questions_with_oracle += 1

            # ä¿å­˜ç»“æœ
            result = {
                'question_id': question_id,
                'question': question,
                'answer': answer,
                'oracle_chunk_ids': oracle_chunk_ids,
                'search_results': [
                    {
                        'event_id': e.get('event_id', ''),
                        'title': e.get('title', ''),
                        'content': e.get('content', ''),
                        'summary': e.get('summary', ''),
                        'category': e.get('category', ''),
                        'start_time': e.get('start_time', ''),
                        'score': e.get('score', 0.0),
                        'cosine_similarity': e.get('cosine_similarity', None)
                    }
                    for e in search_results
                ],
                'chunks': [
                    {
                        'chunk_id': c.get('chunk_id', ''),
                        'heading': c.get('heading', ''),
                        'content': c.get('content', ''),
                        'rank': c.get('rank', 0)
                    }
                    for c in chunks
                ],
                'recall': recall_result.recall,
                'total_oracle': recall_result.total_oracle,
                'recalled': recall_result.recalled,
                'retrieved': recall_result.retrieved,
                'recalled_details': recall_result.recalled_details,
                'search_time': search_time
            }
            all_results.append(result)

        except Exception as e:
            print(f"   âš ï¸ æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

            # ä¿å­˜å¤±è´¥è®°å½•
            all_results.append({
                'question_id': question_id,
                'question': question,
                'answer': answer,
                'oracle_chunk_ids': oracle_chunk_ids,
                'error': str(e),
                'search_results': [],
                'chunks': []
            })

    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    print("\n" + "="*80)
    print("ğŸ“Š æ€»ä½“ç»Ÿè®¡")
    print("="*80)
    print(f"æ€»é—®é¢˜æ•°: {len(questions)}")
    print(f"æˆåŠŸæœç´¢: {sum(1 for r in all_results if 'error' not in r)} ä¸ª")
    print(f"æœç´¢å¤±è´¥: {sum(1 for r in all_results if 'error' in r)} ä¸ª")

    # ç»Ÿè®¡å¹³å‡å¬å›äº‹é¡¹æ•°å’Œæ®µè½æ•°
    successful_results = [r for r in all_results if 'error' not in r]
    if successful_results:
        avg_events = sum(len(r['search_results']) for r in successful_results) / len(successful_results)
        avg_chunks = sum(len(r.get('chunks', [])) for r in successful_results) / len(successful_results)
        print(f"å¹³å‡å¬å›äº‹é¡¹æ•°: {avg_events:.2f}")
        print(f"å¹³å‡æå–æ®µè½æ•°: {avg_chunks:.2f}")

    # å¬å›ç‡ç»Ÿè®¡
    if total_questions_with_oracle > 0:
        avg_recall = total_recall / total_questions_with_oracle
        print(f"\nå¹³å‡å¬å›ç‡: {avg_recall:.2%}")
        print(f"  è®¡ç®—æ–¹å¼: å®å¹³å‡ (Macro Average)")
        print(f"  å…¬å¼: Î£(æ¯ä¸ªé—®é¢˜çš„å¬å›ç‡) / é—®é¢˜æ€»æ•°")
        print(f"  è¯´æ˜: æ¯ä¸ªé—®é¢˜æƒé‡ç›¸åŒï¼Œä¸è®ºå…¶æ ‡å‡†ç­”æ¡ˆæ•°é‡")

        # ç»Ÿè®¡ä¸åŒå¬å›æƒ…å†µçš„é—®é¢˜æ•°
        perfect_recall = 0  # å®Œç¾å¬å›ï¼ˆ100%ï¼‰
        partial_recall = 0  # éƒ¨åˆ†å¬å›ï¼ˆ0% < recall < 100%ï¼‰
        zero_recall = 0     # é›¶å¬å›ï¼ˆ0%ï¼‰

        # ç»Ÿè®¡éƒ¨åˆ†å¬å›çš„è¯¦ç»†æƒ…å†µ
        from collections import defaultdict
        partial_recall_details = defaultdict(int)  # {å¬å›æ•°é‡: é—®é¢˜æ•°}

        # ç»Ÿè®¡æ€»çš„æ ‡å‡†ç­”æ¡ˆæ•°å’Œå¬å›æ•°ï¼ˆç”¨äºè®¡ç®—å¾®å¹³å‡ï¼‰
        total_oracle_chunks = 0
        total_recalled_chunks = 0

        for result in all_results:
            if 'error' in result:
                continue

            recall = result.get('recall', 0.0)
            recalled_count = result.get('recalled', 0)
            total_oracle_count = result.get('total_oracle', 0)

            total_oracle_chunks += total_oracle_count
            total_recalled_chunks += recalled_count

            if recall >= 1.0:
                perfect_recall += 1
            elif recall > 0.0:
                partial_recall += 1
                # è®°å½•éƒ¨åˆ†å¬å›çš„è¯¦ç»†æƒ…å†µ
                partial_recall_details[recalled_count] += 1
            else:
                zero_recall += 1

        print(f"\nå¬å›æƒ…å†µåˆ†å¸ƒ:")
        print(f"  âœ… å®Œç¾å¬å› (100%): {perfect_recall} ä¸ªé—®é¢˜ ({perfect_recall/total_questions_with_oracle:.2%})")
        print(f"  ğŸ”¶ éƒ¨åˆ†å¬å› (1%-99%): {partial_recall} ä¸ªé—®é¢˜ ({partial_recall/total_questions_with_oracle:.2%})")

        # æ˜¾ç¤ºéƒ¨åˆ†å¬å›çš„è¯¦ç»†åˆ†å¸ƒ
        if partial_recall_details:
            print(f"     éƒ¨åˆ†å¬å›è¯¦æƒ…:")
            for recalled_count in sorted(partial_recall_details.keys()):
                count = partial_recall_details[recalled_count]
                print(f"       - å¬å› {recalled_count} ä¸ªç­”æ¡ˆ: {count} ä¸ªé—®é¢˜")

        print(f"  âŒ é›¶å¬å› (0%): {zero_recall} ä¸ªé—®é¢˜ ({zero_recall/total_questions_with_oracle:.2%})")

    print(f"\næœ‰æ ‡å‡†ç­”æ¡ˆçš„é—®é¢˜æ•°: {total_questions_with_oracle}")
    print("="*80)

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    print(f"\n[INFO] ä¿å­˜ç»“æœåˆ°: {output_file}")
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        for result in all_results:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    print(f"[INFO] ç»“æœå·²ä¿å­˜")

    # æ¸…ç†èµ„æº
    print("\n[INFO] æ¸…ç†æœç´¢å™¨èµ„æº...")
    await searcher.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
