"""
Stage3: è¯„ä¼°

åŠŸèƒ½ï¼š
1. ä» retrieval_results.jsonl åŠ è½½æ£€ç´¢ç»“æœ
2. ä» generated_answers.jsonl åŠ è½½ç”Ÿæˆçš„ç­”æ¡ˆ
3. ä½¿ç”¨ RAGAs è®¡ç®—è¯„ä¼°æŒ‡æ ‡
4. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

è¾“å…¥ï¼š
- retrieval_results.jsonlï¼ˆStage1 è¾“å‡ºï¼‰
- generated_answers.jsonlï¼ˆStage2 è¾“å‡ºï¼‰

è¾“å‡ºï¼š
- evaluation_report.jsonï¼ˆè¯„ä¼°æŠ¥å‘Šï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    # åŸºæœ¬ç”¨æ³•
    python stage3_evaluate.py --max-workers 16  

    # è‡ªå®šä¹‰è¾“å…¥/è¾“å‡ºè·¯å¾„
    python stage3_evaluate.py --retrieval data/retrieval_results.jsonl --answers data/generated_answers.jsonl --output data/report.json

    # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
    python stage3_evaluate.py --verbose
"""

import argparse
import logging
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import pandas as pd

# æ·»åŠ  evaluation ç›®å½•åˆ°è·¯å¾„
import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
project_root = Path(__file__).parent.parent.parent.parent
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"[OK] å·²åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
else:
    print(f"[WARN] æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")

from hotpotqa_evaluation import config

# å¯¼å…¥ RAGAs
try:
    from datasets import Dataset
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
    )
    RAGAS_AVAILABLE = True
except ImportError:
    print("[WARN] RAGAs æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install ragas datasets pillow")
    RAGAS_AVAILABLE = False

# å¯¼å…¥å…±äº«æ¨¡å—
from shared import (
    RetrievalResult,
    GeneratedAnswer,
    EvaluationReport,
    QuestionScore,
    read_jsonl,
    write_json,
    create_ragas_llm,
    create_ragas_embeddings,
    print_model_config,
)


def convert_to_ragas_format(
    retrieval_results: List[RetrievalResult],
    generated_answers: List[GeneratedAnswer]
) -> Dict[str, List]:
    """
    è½¬æ¢ä¸º RAGAs éœ€è¦çš„æ ¼å¼

    Returns:
        {
            "question": [...],
            "answer": [...],
            "contexts": [...],
            "ground_truth": [...]
        }
    """
    questions = []
    answers = []
    contexts = []
    ground_truths = []

    for retrieval, answer in zip(retrieval_results, generated_answers):
        # question
        questions.append(retrieval.question)

        # answer (LLMç”Ÿæˆçš„)
        answers.append(answer.generated_answer)

        # contexts (æ£€ç´¢åˆ°çš„æ®µè½å†…å®¹åˆ—è¡¨)
        retrieved_contexts = [
            section.content_preview
            for section in retrieval.retrieved_sections
        ]
        contexts.append(retrieved_contexts)

        # ground_truth (æ ‡å‡†ç­”æ¡ˆ)
        ground_truths.append(retrieval.answer)

    return {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths
    }


def main():
    parser = argparse.ArgumentParser(description='Stage3: è¯„ä¼°')
    parser.add_argument('--retrieval', type=str,
                       default=str(config.DATA_DIR / "retrieval_results.jsonl"),
                       help='æ£€ç´¢ç»“æœè¾“å…¥è·¯å¾„ï¼ˆStage1 è¾“å‡ºï¼‰')
    parser.add_argument('--answers', type=str,
                       default=str(config.DATA_DIR / "generated_answers.jsonl"),
                       help='ç”Ÿæˆç­”æ¡ˆè¾“å…¥è·¯å¾„ï¼ˆStage2 è¾“å‡ºï¼‰')
    parser.add_argument('--output', type=str,
                       default=str(config.DATA_DIR / "evaluation_report.json"),
                       help='è¯„ä¼°æŠ¥å‘Šè¾“å‡ºè·¯å¾„')
    parser.add_argument('--max-workers', type=int, default=16,
                       help='å¹¶å‘æ•°ï¼ˆé»˜è®¤ 16ï¼Œæ ¹æ® API é™æµè°ƒæ•´ï¼‰')
    parser.add_argument('--timeout', type=int, default=180,
                       help='å•ä¸ªè¯„ä¼°ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼Œé»˜è®¤ 180ï¼‰')
    parser.add_argument('--verbose', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—')

    args = parser.parse_args()

    # é…ç½®æ—¥å¿—
    if args.verbose:
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S'
        )
    else:
        logging.basicConfig(level=logging.WARNING)

    print("=" * 60)
    print("ğŸš€ Stage3: è¯„ä¼°")
    print("=" * 60)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # æ£€æŸ¥ RAGAs æ˜¯å¦å¯ç”¨
    if not RAGAS_AVAILABLE:
        print("[X] RAGAs æœªå®‰è£…ï¼Œæ— æ³•ç»§ç»­")
        print("   è¯·è¿è¡Œ: pip install ragas datasets")
        return

    # 1. åŠ è½½æ£€ç´¢ç»“æœ
    retrieval_path = Path(args.retrieval)
    print(f"ğŸ“‚ åŠ è½½æ£€ç´¢ç»“æœ: {retrieval_path}")

    try:
        retrieval_results = read_jsonl(retrieval_path, RetrievalResult)
    except FileNotFoundError as e:
        print(f"[X] é”™è¯¯: {e}")
        print("   è¯·å…ˆè¿è¡Œ Stage1 (stage1_retrieve.py)")
        return
    except Exception as e:
        print(f"[X] åŠ è½½å¤±è´¥: {e}")
        return

    print(f"[OK] åŠ è½½äº† {len(retrieval_results)} ä¸ªæ£€ç´¢ç»“æœ")

    # 2. åŠ è½½ç”Ÿæˆçš„ç­”æ¡ˆ
    answers_path = Path(args.answers)
    print(f"ğŸ“‚ åŠ è½½ç”Ÿæˆçš„ç­”æ¡ˆ: {answers_path}")

    try:
        generated_answers = read_jsonl(answers_path, GeneratedAnswer)
    except FileNotFoundError as e:
        print(f"[X] é”™è¯¯: {e}")
        print("   è¯·å…ˆè¿è¡Œ Stage2 (stage2_generate.py)")
        return
    except Exception as e:
        print(f"[X] åŠ è½½å¤±è´¥: {e}")
        return

    print(f"[OK] åŠ è½½äº† {len(generated_answers)} ä¸ªç”Ÿæˆçš„ç­”æ¡ˆ")
    print()

    # éªŒè¯æ•°æ®ä¸€è‡´æ€§
    if len(retrieval_results) != len(generated_answers):
        print(f"[X] é”™è¯¯: æ£€ç´¢ç»“æœæ•°é‡ ({len(retrieval_results)}) ä¸ç­”æ¡ˆæ•°é‡ ({len(generated_answers)}) ä¸åŒ¹é…")
        return

    # 3. è½¬æ¢ä¸º RAGAs æ ¼å¼
    print("ğŸ”„ è½¬æ¢ä¸º RAGAs æ ¼å¼...")
    ragas_data = convert_to_ragas_format(retrieval_results, generated_answers)

    # åˆ›å»º Dataset
    try:
        dataset = Dataset.from_dict(ragas_data)
        print(f"[OK] è½¬æ¢å®Œæˆï¼Œæ•°æ®é›†å¤§å°: {len(dataset)}\n")
    except Exception as e:
        print(f"[X] åˆ›å»ºæ•°æ®é›†å¤±è´¥: {e}")
        return

    # 4. åˆ›å»º RAGAs ä½¿ç”¨çš„æ¨¡å‹å®ä¾‹
    print("ğŸ¤– åˆå§‹åŒ– RAGAs è¯„ä¼°æ¨¡å‹...")
    print()

    try:
        # ä½¿ç”¨é¡¹ç›®é…ç½®çš„æ¨¡å‹
        ragas_llm = create_ragas_llm(temperature=0.0, verbose=True)
        ragas_embeddings = create_ragas_embeddings(verbose=True)
    except Exception as e:
        print(f"[X] æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # 5. è¿è¡Œ RAGAs è¯„ä¼°ï¼ˆæ”¯æŒå¹¶å‘é…ç½®ï¼‰
    print("ğŸ“Š è¿è¡Œ RAGAs è¯„ä¼°...")
    print("   è¯„ä¼°æŒ‡æ ‡:")
    print("   1. faithfulness (å¿ å®åº¦) - ç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢åˆ°çš„æ®µè½")
    print("   2. answer_relevancy (ç­”æ¡ˆç›¸å…³æ€§) - ç­”æ¡ˆæ˜¯å¦å›ç­”äº†é—®é¢˜")
    print("   3. context_precision (ä¸Šä¸‹æ–‡ç²¾åº¦) - æ£€ç´¢æ®µè½çš„ç²¾å‡†åº¦")
    print("   4. context_recall (ä¸Šä¸‹æ–‡å¬å›ç‡) - æ ‡å‡†ç­”æ¡ˆä¿¡æ¯çš„è¦†ç›–åº¦")
    print(f"   å¹¶å‘æ•°: {args.max_workers}")
    print()

    try:
        # ğŸ†• é…ç½®å¹¶å‘å‚æ•°
        from ragas.run_config import RunConfig

        run_config = RunConfig(
            max_workers=args.max_workers,  # å¹¶å‘æ•°
            timeout=args.timeout,           # è¶…æ—¶æ—¶é—´
            max_retries=3                   # æœ€å¤§é‡è¯•æ¬¡æ•°
        )

        # ä¼ å…¥è‡ªå®šä¹‰çš„ LLM å’Œ Embeddings
        results = evaluate(
            dataset,
            metrics=[
                faithfulness,
                answer_relevancy,
                context_precision,
                context_recall,
            ],
            llm=ragas_llm,
            embeddings=ragas_embeddings,
            run_config=run_config  # ğŸ†• ä¼ å…¥å¹¶å‘é…ç½®
        )
        print("[OK] RAGAs è¯„ä¼°å®Œæˆ\n")
    except Exception as e:
        print(f"[X] RAGAs è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # 6. å±•ç¤ºç»“æœ
    print("=" * 60)
    print("ğŸ“Š RAGAs è¯„ä¼°ç»“æœ")
    print("=" * 60)

    # å°† EvaluationResult è½¬æ¢ä¸ºå­—å…¸
    if hasattr(results, 'to_pandas'):
        df = results.to_pandas()
        # åªé€‰æ‹©æ•°å€¼åˆ—è®¡ç®—å¹³å‡å€¼
        numeric_cols = df.select_dtypes(include=['number']).columns
        results_dict = df[numeric_cols].mean().to_dict()

        # æå–æ¯ä¸ªé—®é¢˜çš„è¯¦ç»†è¯„åˆ†
        per_question_scores = []
        for idx, row in df.iterrows():
            question_score = QuestionScore(
                question_id=retrieval_results[idx].question_id,
                faithfulness=float(row.get('faithfulness', 0)),
                answer_relevancy=float(row.get('answer_relevancy', 0)),
                context_precision=float(row.get('context_precision', 0)),
                context_recall=float(row.get('context_recall', 0))
            )
            per_question_scores.append(question_score)

        # å¦‚æœå¼€å¯ verboseï¼Œå±•ç¤ºæ¯ä¸ªé—®é¢˜çš„è¯¦ç»†è¯„åˆ†
        if args.verbose and len(df) > 0:
            print("\n[*] æ¯ä¸ªé—®é¢˜çš„è¯¦ç»†è¯„åˆ†:")
            print("=" * 60)
            for idx, row in df.iterrows():
                print(f"\né—®é¢˜ {idx + 1}: {ragas_data['question'][idx][:60]}...")
                print(f"  æ ‡å‡†ç­”æ¡ˆ: {ragas_data['ground_truth'][idx][:50]}...")
                print(f"  ç”Ÿæˆç­”æ¡ˆ: {ragas_data['answer'][idx][:50]}...")
                print(f"  æ£€ç´¢æ®µè½æ•°: {len(ragas_data['contexts'][idx])}")
                print(f"  è¯„åˆ†:")
                for col in numeric_cols:
                    if col in row and pd.notna(row[col]):
                        print(f"    {col:25s}: {row[col]:.4f}")
            print(f"\n{'='*60}\n")
    else:
        results_dict = dict(results)
        per_question_scores = []

    print("\n[*] å¹³å‡è¯„åˆ†:")
    for metric_name, score in results_dict.items():
        print(f"  {metric_name:25s}: {score:.4f}")

    print()

    # 7. ä¿å­˜è¯„ä¼°æŠ¥å‘Š
    # è·å– retrieval metadata (ä»ç¬¬ä¸€ä¸ªç»“æœä¸­æå–)
    first_retrieval = retrieval_results[0]

    report = EvaluationReport(
        metadata={
            "timestamp": datetime.now().isoformat(),
            "total_questions": len(retrieval_results),
            "source_config_id": first_retrieval.retrieval_metadata.source_config_id,
            "top_k": first_retrieval.retrieval_metadata.top_k,
            "threshold": first_retrieval.retrieval_metadata.threshold,
            "retrieval_file": str(args.retrieval),
            "answers_file": str(args.answers),
        },
        ragas_metrics={
            metric_name: float(score)
            for metric_name, score in results_dict.items()
        },
        per_question_scores=per_question_scores
    )

    output_path = Path(args.output)
    write_json(report, output_path)

    print("=" * 60)
    print("âœ… Stage3 å®Œæˆ")
    print("=" * 60)
    print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
    print("=" * 60)


if __name__ == "__main__":
    main()
