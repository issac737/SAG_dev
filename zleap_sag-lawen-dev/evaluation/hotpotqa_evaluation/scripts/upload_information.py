"""
HotpotQA æ•°æ®å¤„ç†æ¨¡å—

å°è£…ä¸‰ä¸ªå¤„ç†é˜¶æ®µï¼š
1. æ„å»ºè¯­æ–™åº“ (build_corpus)
2. æå–æ ‡å‡†ç­”æ¡ˆ (extract_oracle)
3. ä¸Šä¼ åˆ°ç³»ç»Ÿ (upload_corpus)
"""

import json
import logging
import time
import asyncio
import os
from pathlib import Path
from typing import Any, Dict, List, Optional
from datetime import datetime
import sys
import io

# Ensure UTF-8 encoding for stdout/stderr on Windows
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
if sys.stderr.encoding != 'utf-8':
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
# è„šæœ¬åœ¨ scripts/ ç›®å½•ä¸‹è¿è¡Œï¼Œéœ€è¦æ·»åŠ çˆ¶ç›®å½•çš„çˆ¶ç›®å½•
# å³ï¼šC:\Users\user\zleap_sag
from pathlib import Path
project_root = Path(__file__).resolve().parent.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# åŠ è½½ .env æ–‡ä»¶
from dotenv import load_dotenv
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"[INFO] å·²åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
else:
    print(f"[WARN] æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")

# å¯¼å…¥ DataFlowEngine å’Œç›¸å…³é…ç½®
from dataflow import DataFlowEngine, ExtractBaseConfig
from dataflow.modules.load.config import DocumentLoadConfig
from evaluation.hotpotqa_evaluation.modules.hotpotqa_loader import HotpotQALoader
from evaluation.hotpotqa_evaluation.modules.utils import (
    format_chunk_id,
    split_merged_id,
    print_stats,
    ChunkDeduplicator
)
from evaluation.hotpotqa_evaluation import config as hotpot_config


class HotpotQAProcessor:
    """
    HotpotQA æ•°æ®é›†å¤„ç†å™¨

    åŠŸèƒ½ç‰¹æ€§ï¼š
    - æ„å»ºå…¨å±€è¯­æ–™åº“ï¼ˆæ–‡æ¡£çº§æ‹¼æ¥ã€å»é‡ï¼‰
    - æå–æ ‡å‡†ç­”æ¡ˆï¼ˆground truthï¼‰
    - ä¸Šä¼ è¯­æ–™åº“åˆ°ç³»ç»Ÿï¼ˆæ”¯æŒæµ‹è¯•ï¼‰
    - è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å¤¹ï¼Œéš”ç¦»æ¯æ¬¡è¿è¡Œ

    ç¤ºä¾‹ï¼š
        processor = HotpotQAProcessor(
            dataset_path="path/to/hotpotqa",
            data_dir="data"
        )

        # é˜¶æ®µ1ï¼šæ„å»ºè¯­æ–™åº“
        stats = processor.build_corpus(sample_start=0, sample_end=100)

        # é˜¶æ®µ2ï¼šæå–æ ‡å‡†ç­”æ¡ˆ
        stats = processor.extract_oracle(sample_start=0, sample_end=100)

        # é˜¶æ®µ3ï¼šä¸Šä¼ å¹¶æµ‹è¯•
        result = await processor.upload_corpus(test_queries=True)
    """

    def __init__(
        self,
        dataset_path: Optional[str] = None,
        base_data_dir: Optional[str] = None,
        enable_logging: bool = True,
        use_timestamp_folder: bool = True
    ):
        """
        åˆå§‹åŒ–å¤„ç†å™¨

        Args:
            dataset_path: HotpotQAæ•°æ®é›†è·¯å¾„ï¼Œä¸ºNoneåˆ™ä½¿ç”¨configä¸­çš„é…ç½®
            base_data_dir: åŸºç¡€æ•°æ®ç›®å½•ï¼Œä¸ºNoneåˆ™ä½¿ç”¨configä¸­çš„é…ç½®
            enable_logging: æ˜¯å¦å¯ç”¨æ—¥å¿—
            use_timestamp_folder: æ˜¯å¦ä¸ºæ¯æ¬¡è¿è¡Œåˆ›å»ºæ—¶é—´æˆ³å­æ–‡ä»¶å¤¹
        """
        # Configure paths
        self.dataset_path = dataset_path or hotpot_config.HOTPOTQA_DATASET_PATH

        # Initialize logger first to avoid AttributeError in _log calls
        self.logger = None
        if enable_logging:
            self._setup_logging()

        # Base data directory
        base_dir = Path(base_data_dir) if base_data_dir else hotpot_config.DATA_DIR

        if use_timestamp_folder:
            # Create timestamp subfolder: data/source/20251209_143025/
            self.run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.data_dir = base_dir / "source" / self.run_timestamp
            self.data_dir.mkdir(parents=True, exist_ok=True)
            self._log(f"Created timestamped folder: {self.data_dir}")
        else:
            # Use base directory directly
            self.data_dir = base_dir
            self.data_dir.mkdir(parents=True, exist_ok=True)

        # Output file paths (all inside the timestamped folder)
        self.corpus_path = self.data_dir / "corpus.jsonl"
        self.corpus_md_path = self.data_dir / "corpus_merged.md"
        self.oracle_path = self.data_dir / "oracle.jsonl"
        self.process_result_path = self.data_dir / "process_result.json"

        # é”™è¯¯æ”¶é›†
        self.errors = []

    @property
    def output_files(self) -> Dict[str, Path]:
        """è·å–æ‰€æœ‰è¾“å‡ºæ–‡ä»¶çš„è·¯å¾„"""
        return {
            'corpus_jsonl': self.corpus_path,
            'corpus_md': self.corpus_md_path,
            'oracle': self.oracle_path,
            'process_result': self.process_result_path
        }

    def get_output_folder(self) -> Path:
        """è·å–è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„"""
        return self.data_dir

    def print_output_info(self):
        """æ‰“å°è¾“å‡ºæ–‡ä»¶ä¿¡æ¯"""
        self._log("=" * 60)
        self._log("è¾“å‡ºæ–‡ä»¶ä½ç½®")
        self._log("=" * 60)
        self._log(f"è¾“å‡ºæ–‡ä»¶å¤¹: {self.data_dir}")
        self._log("æ–‡ä»¶åˆ—è¡¨:")
        self._log(f"  - è¯­æ–™åº“ (JSONL): {self.corpus_path.name}")
        self._log(f"  - è¯­æ–™åº“ (Markdown): {self.corpus_md_path.name}")
        self._log(f"  - æ ‡å‡†ç­”æ¡ˆ: {self.oracle_path.name}")
        self._log(f"  - å¤„ç†ç»“æœ: {self.process_result_path.name}")
        self._log("=" * 60)

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S'
        )
        self.logger = logging.getLogger(self.__class__.__name__)

    def _log(self, message: str, level: str = 'info'):
        """ç»Ÿä¸€æ—¥å¿—æ–¹æ³•"""
        if self.logger:
            getattr(self.logger, level.lower())(message)
        else:
            print(message)

    def _log_error(self, message: str, exc_info: bool = False):
        """è®°å½•é”™è¯¯"""
        self.errors.append({
            'timestamp': datetime.now().isoformat(),
            'message': message
        })
        if self.logger:
            self.logger.error(message, exc_info=exc_info)
        else:
            print(f"é”™è¯¯: {message}")

    # ==================== Stage 1: Build Corpus ====================

    def _extract_chunks_from_sample(self, sample: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ä»å•ä¸ªæ ·æœ¬ä¸­æå–chunksï¼ˆæ–‡æ¡£çº§æ‹¼æ¥ï¼‰

        Args:
            sample: HotpotQAæ ·æœ¬

        Returns:
            chunksåˆ—è¡¨
        """
        sample_id = sample['id']
        context = sample['context']

        chunks = []

        for idx, (title, sentences) in enumerate(zip(context['title'], context['sentences'])):
            # Document-level concatenation: Markdown format
            chunk_text = f"# {title}\n{' '.join(sentences)}"

            # Generate chunk ID
            chunk_id = format_chunk_id(sample_id, idx)

            chunks.append({
                "id": chunk_id,
                "title": title,
                "text": chunk_text,
                "sample_id": sample_id,
                "local_index": idx
            })

        return chunks

    def build_corpus(
        self,
        sample_start: int = 0,
        sample_end: Optional[int] = None,
        enable_dedup: bool = True,
        append_mode: bool = False
    ) -> Dict[str, Any]:
        """
        æ„å»ºå…¨å±€è¯­æ–™åº“

        Args:
            sample_start: èµ·å§‹ç´¢å¼•ï¼ˆåŒ…å«ï¼‰
            sample_end: ç»“æŸç´¢å¼•ï¼ˆä¸åŒ…å«ï¼‰
            enable_dedup: æ˜¯å¦å¯ç”¨å»é‡
            append_mode: è¿½åŠ æ¨¡å¼

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        self._log("=" * 60)
        self._log("é˜¶æ®µ1ï¼šæ„å»ºå…¨å±€è¯­æ–™åº“")
        self._log("=" * 60)

        try:
            # 1. Load dataset
            self._log(f"\nLoad dataset: {hotpot_config.DATASET_CONFIG}/{hotpot_config.DATASET_SPLIT}")
            loader = HotpotQALoader(self.dataset_path)

            if hotpot_config.DATASET_SPLIT == "validation":
                samples = loader.load_validation(config=hotpot_config.DATASET_CONFIG, limit=None)
            else:
                samples = loader.load_train(config=hotpot_config.DATASET_CONFIG, limit=None)

            total_samples = len(samples)

            # Adjust boundaries
            if sample_end is None or sample_end > total_samples:
                sample_end = total_samples

            if sample_start >= total_samples:
                error_msg = f"Error: Start index {sample_start} exceeds total samples ({total_samples})"
                self._log_error(error_msg)
                return {'status': 'error', 'message': error_msg}

            # Slice dataset
            samples = samples[sample_start:sample_end]

            self._log(f"âœ“ Total samples: {total_samples}")
            self._log(f"âœ“ Processing range: [{sample_start}:{sample_end}]")
            self._log(f"âœ“ Actual samples to process: {len(samples)}\n")

            # 2. Extract all chunks
            self._log("Extract document-level chunks...")
            all_chunks = []

            for sample in samples:
                chunks = self._extract_chunks_from_sample(sample)
                all_chunks.extend(chunks)

            self._log(f"[OK] Extracted {len(all_chunks)} chunks\n")

            # 3. Deduplication
            final_chunks = all_chunks
            if enable_dedup:
                self._log("Deduplicate...")
                deduplicator = ChunkDeduplicator()
                deduplicated_chunks = {}

                for chunk in all_chunks:
                    chunk_id = chunk['id']
                    chunk_text = chunk['text']
                    final_id = deduplicator.add_chunk(chunk_id, chunk_text)

                    if final_id not in deduplicated_chunks:
                        deduplicated_chunks[final_id] = {
                            "id": final_id,
                            "title": chunk['title'],
                            "text": chunk_text
                        }

                dedup_stats = deduplicator.get_stats()
                if hasattr(hotpot_config, 'PRINT_STATS') and hotpot_config.PRINT_STATS:
                    print_stats("Deduplication Stats", dedup_stats)

                final_chunks = list(deduplicated_chunks.values())
                self._log(f"âœ“ After deduplication: {len(final_chunks)} chunks\n")

            # 4. Save to file
            write_mode = 'a' if append_mode else 'w'
            mode_desc = "Append" if append_mode else "Overwrite"

            self._log(f"Save to: {self.corpus_path} ({mode_desc} mode)")

            # Append mode: check existing data
            existing_ids = set()
            if append_mode and self.corpus_path.exists():
                self._log("Read existing data to check duplicates...")
                with open(self.corpus_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            existing_chunk = json.loads(line)
                            existing_ids.add(existing_chunk['id'])
                self._log(f"âœ“ Found {len(existing_ids)} existing chunks\n")

            # Filter and save
            chunks_to_save = []
            duplicates_count = 0
            for chunk in final_chunks:
                if chunk['id'] in existing_ids:
                    duplicates_count += 1
                else:
                    chunks_to_save.append(chunk)

            if append_mode and duplicates_count > 0:
                self._log(f"âš ï¸  Skipped {duplicates_count} duplicate chunks\n")

            with open(self.corpus_path, write_mode, encoding='utf-8') as f:
                for i, chunk in enumerate(chunks_to_save):
                    try:
                        line = json.dumps(chunk, ensure_ascii=False) + '\n'
                        f.write(line)
                        # æ¯100ä¸ªchunkåˆ·æ–°ä¸€æ¬¡ç¼“å†²åŒºï¼Œé¿å…ç¼“å†²åŒºè¿‡å¤§
                        if (i + 1) % 100 == 0:
                            f.flush()
                            os.fsync(f.fileno())  # å¼ºåˆ¶å†™å…¥ç£ç›˜
                    except OSError as e:
                        self._log(f"âš ï¸ å†™å…¥chunk {i} å¤±è´¥ (OSError): {e}, chunk_id={chunk.get('id', 'unknown')}")
                        # å°è¯•æˆªæ–­è¿‡é•¿çš„textå­—æ®µåé‡è¯•
                        if 'text' in chunk and len(chunk['text']) > 50000:
                            chunk['text'] = chunk['text'][:50000] + '... [truncated]'
                            try:
                                line = json.dumps(chunk, ensure_ascii=False) + '\n'
                                f.write(line)
                            except Exception as retry_e:
                                self._log(f"âš ï¸ é‡è¯•å†™å…¥chunk {i} ä»å¤±è´¥: {retry_e}")
                        raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚å¤„ç†
                    except Exception as e:
                        self._log(f"âš ï¸ å†™å…¥chunk {i} å¤±è´¥: {e}, chunk_id={chunk.get('id', 'unknown')}")
                        raise
                f.flush()
                os.fsync(f.fileno())  # æœ€ç»ˆå¼ºåˆ¶å†™å…¥ç£ç›˜

            if append_mode:
                total_records = len(existing_ids) + len(chunks_to_save)
                self._log(f"âœ“ Added {len(chunks_to_save)} chunks (total {total_records})\n")
            else:
                self._log(f"âœ“ Saved {len(chunks_to_save)} chunks\n")

            # 5. Generate merged Markdown file
            md_write_mode = 'a' if append_mode else 'w'
            self._log(f"Generate merged Markdown: {self.corpus_md_path} ({mode_desc} mode)")

            with open(self.corpus_md_path, md_write_mode, encoding='utf-8') as f:
                for i, chunk in enumerate(chunks_to_save, 1):
                    f.write(chunk['text'])
                    f.write('\n\n')

                    if i % 100 == 0:
                        self._log(f"  Processed {i}/{len(chunks_to_save)} chunks...")

            self._log(f"âœ“ Markdown file generated\n")

            # 6. Return statistics
            stats = {
                'status': 'success',
                'sample_count': len(samples),
                'original_chunks': len(all_chunks),
                'final_chunks': len(final_chunks),
                'chunks_saved': len(chunks_to_save),
                'duplicates_skipped': duplicates_count if append_mode else 0,
                'corpus_path': str(self.corpus_path),
                'corpus_md_path': str(self.corpus_md_path),
                'timestamp': datetime.now().isoformat()
            }

            self._log("=" * 60)
            self._log("âœ… é˜¶æ®µ1å®Œæˆ")
            self._log("=" * 60)

            return stats

        except Exception as e:
            error_msg = f"é˜¶æ®µ1å¤±è´¥: {e}"
            self._log_error(error_msg, exc_info=True)
            return {'status': 'error', 'message': error_msg}

    # ==================== Stage 2: Extract Oracle ====================

    def _load_corpus_index(self) -> Dict[str, Dict]:
        """
        Load corpus and build index

        Returns:
            chunk_id -> chunk mapping
        """
        self._log(f"Load corpus index: {self.corpus_path}")

        corpus_index = {}

        if not self.corpus_path.exists():
            error_msg = f"Error: corpus.jsonl not found: {self.corpus_path}"
            self._log_error(error_msg)
            return corpus_index

        with open(self.corpus_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    chunk = json.loads(line)
                    chunk_id = chunk['id']

                    # Handle merged IDs
                    if "//" in chunk_id:
                        original_ids = split_merged_id(chunk_id)
                        for original_id in original_ids:
                            corpus_index[original_id] = chunk
                    else:
                        corpus_index[chunk_id] = chunk

        self._log(f"âœ“ Loaded {len(corpus_index)} chunk indexes\n")
        return corpus_index

    def _extract_oracle_for_sample(self, sample: Dict[str, Any], corpus_index: Dict[str, Dict]) -> Dict[str, Any]:
        """
        Extract oracle chunks for single sample

        Args:
            sample: HotpotQA sample
            corpus_index: Corpus index

        Returns:
            Oracle data
        """
        sample_id = sample['id']
        supporting_facts = sample['supporting_facts']
        context = sample['context']

        # 1. Extract titles from supporting_facts
        oracle_titles = supporting_facts['title']

        # 2. Map titles to indices
        title_to_indices = {}
        for idx, title in enumerate(context['title']):
            if title not in title_to_indices:
                title_to_indices[title] = []
            title_to_indices[title].append(idx)

        # 3. Construct chunk IDs
        oracle_chunk_ids = []
        oracle_titles_unique = []

        for title in oracle_titles:
            if title in title_to_indices:
                idx = title_to_indices[title][0]
                chunk_id = format_chunk_id(sample_id, idx)

                if chunk_id in corpus_index:
                    oracle_chunk_ids.append(chunk_id)
                    if title not in oracle_titles_unique:
                        oracle_titles_unique.append(title)
                else:
                    self._log(f"âš ï¸  Warning: chunk ID {chunk_id} not in corpus (sample {sample_id})", 'warning')

        # 4. Deduplicate (preserve order)
        seen = set()
        deduplicated_ids = []
        for chunk_id in oracle_chunk_ids:
            if chunk_id not in seen:
                deduplicated_ids.append(chunk_id)
                seen.add(chunk_id)

        return {
            "id": sample_id,
            "question": sample['question'],
            "answer": sample['answer'],
            "oracle_chunk_ids": deduplicated_ids,
            "oracle_titles": oracle_titles_unique,
            "type": sample['type'],
            "level": sample['level']
        }

    def extract_oracle(
        self,
        sample_start: int = 0,
        sample_end: Optional[int] = None,
        append_mode: bool = False
    ) -> Dict[str, Any]:
        """
        Extract oracle (ground truth)

        Args:
            sample_start: Start index (inclusive)
            sample_end: End index (exclusive)
            append_mode: Append mode

        Returns:
            Statistics dictionary
        """
        self._log("=" * 60)
        self._log("é˜¶æ®µ2ï¼šæå–æ ‡å‡†ç­”æ¡ˆï¼ˆOracleï¼‰")
        self._log("=" * 60)

        try:
            # 1. Load corpus index
            corpus_index = self._load_corpus_index()
            if not corpus_index:
                return {'status': 'error', 'message': 'Failed to load corpus index'}

            # 2. Load dataset
            self._log(f"Load dataset: {hotpot_config.DATASET_CONFIG}/{hotpot_config.DATASET_SPLIT}")
            loader = HotpotQALoader(self.dataset_path)

            if hotpot_config.DATASET_SPLIT == "validation":
                samples = loader.load_validation(config=hotpot_config.DATASET_CONFIG, limit=None)
            else:
                samples = loader.load_train(config=hotpot_config.DATASET_CONFIG, limit=None)

            total_samples = len(samples)

            # Adjust boundaries
            if sample_end is None or sample_end > total_samples:
                sample_end = total_samples

            if sample_start >= total_samples:
                error_msg = f"Error: Start index {sample_start} exceeds total samples ({total_samples})"
                self._log_error(error_msg)
                return {'status': 'error', 'message': error_msg}

            samples = samples[sample_start:sample_end]

            self._log(f"âœ“ Total samples: {total_samples}")
            self._log(f"âœ“ Processing range: [{sample_start}:{sample_end}]")
            self._log(f"âœ“ Actual samples to process: {len(samples)}\n")

            # 3. Extract oracle
            self._log("Extract oracle chunks...")

            oracles = []
            total_oracle_chunks = 0
            missing_chunks = 0

            for sample in samples:
                oracle = self._extract_oracle_for_sample(sample, corpus_index)
                oracles.append(oracle)
                total_oracle_chunks += len(oracle['oracle_chunk_ids'])

                expected_count = len(sample['supporting_facts']['title'])
                actual_count = len(oracle['oracle_chunk_ids'])
                if actual_count < expected_count:
                    missing_chunks += (expected_count - actual_count)

            self._log(f"âœ“ Extracted oracle for {len(oracles)} questions\n")

            # 4. Save to file
            write_mode = 'a' if append_mode else 'w'
            mode_desc = "Append" if append_mode else "Overwrite"

            self._log(f"Save to: {self.oracle_path} ({mode_desc} mode)")

            # Append mode: check existing data
            existing_ids = set()
            if append_mode and self.oracle_path.exists():
                self._log("Read existing data to check duplicates...")
                with open(self.oracle_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            existing_oracle = json.loads(line)
                            existing_ids.add(existing_oracle['id'])
                self._log(f"âœ“ Found {len(existing_ids)} existing records\n")

            # Filter and save
            oracles_to_save = []
            duplicates_count = 0
            for oracle in oracles:
                if oracle['id'] in existing_ids:
                    duplicates_count += 1
                else:
                    oracles_to_save.append(oracle)

            if append_mode and duplicates_count > 0:
                self._log(f"âš ï¸  Skipped {duplicates_count} duplicate oracle records\n")

            with open(self.oracle_path, write_mode, encoding='utf-8') as f:
                for oracle in oracles_to_save:
                    f.write(json.dumps(oracle, ensure_ascii=False) + '\n')

            if append_mode:
                total_records = len(existing_ids) + len(oracles_to_save)
                self._log(f"âœ“ Added {len(oracles_to_save)} oracle records (total {total_records})\n")
            else:
                self._log(f"âœ“ Saved {len(oracles_to_save)} oracle records\n")

            # 5. Statistics
            oracle_counts = [len(o['oracle_chunk_ids']) for o in oracles]
            avg_oracle_count = sum(oracle_counts) / len(oracle_counts) if oracle_counts else 0

            type_stats = {}
            level_stats = {}
            for oracle in oracles:
                type_stats[oracle['type']] = type_stats.get(oracle['type'], 0) + 1
                level_stats[oracle['level']] = level_stats.get(oracle['level'], 0) + 1

            stats = {
                'status': 'success',
                'question_count': len(oracles),
                'oracle_chunk_total': total_oracle_chunks,
                'avg_oracle_per_question': avg_oracle_count,
                'missing_chunks': missing_chunks,
                'type_distribution': type_stats,
                'level_distribution': level_stats,
                'oracle_path': str(self.oracle_path),
                'file_size_kb': self.oracle_path.stat().st_size / 1024 if self.oracle_path.exists() else 0,
                'timestamp': datetime.now().isoformat()
            }

            self._log("=" * 60)
            self._log("âœ… é˜¶æ®µ2å®Œæˆ")
            self._log("=" * 60)

            return stats

        except Exception as e:
            error_msg = f"é˜¶æ®µ2å¤±è´¥: {e}"
            self._log_error(error_msg, exc_info=True)
            return {'status': 'error', 'message': error_msg}

    # ==================== Stage 3: Upload Corpus ====================

    def _load_existing_result(self) -> Optional[Dict[str, Any]]:
        """Load existing processing result"""
        if not self.process_result_path.exists():
            return None

        try:
            with open(self.process_result_path, 'r', encoding='utf-8') as f:
                result = json.load(f)

            if 'source_config_id' in result and 'article_id' in result:
                return result
            else:
                self._log("âš ï¸  process_result.json missing required fields", 'warning')
                return None
        except Exception as e:
            self._log(f"âš ï¸  Failed to read process_result.json: {e}", 'warning')
            return None

    async def upload_corpus(
        self,
        source_name: str = "HotpotQA Corpus",
        source_description: str = "HotpotQA Evaluation Corpus (Document-level concatenation, deduplicated)",
        enable_extraction: bool = True
    ) -> Dict[str, Any]:
        """
        ä¸Šä¼ è¯­æ–™åº“åˆ°ç³»ç»Ÿï¼ˆä½¿ç”¨ DataFlowEngineï¼‰

        Args:
            source_name: ä¿¡æ¯æºåç§°
            source_description: ä¿¡æ¯æºæè¿°
            enable_extraction: æ˜¯å¦æ‰§è¡Œæå–é˜¶æ®µï¼ˆFalse åˆ™åªåŠ è½½æ–‡æ¡£ï¼‰

        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        self._log("=" * 60)
        self._log("é˜¶æ®µ3ï¼šå¤„ç†è¯­æ–™åº“å¹¶åŠ è½½åˆ°ç³»ç»Ÿ")
        self._log("=" * 60)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not self.corpus_md_path.exists():
            error_msg = f"é”™è¯¯ï¼šcorpus_merged.md ä¸å­˜åœ¨: {self.corpus_md_path}"
            self._log_error(error_msg)
            return {'status': 'error', 'message': error_msg}

        file_size_mb = self.corpus_md_path.stat().st_size / 1024 / 1024
        self._log(f"\nè¯­æ–™åº“æ–‡ä»¶: {self.corpus_md_path}")
        self._log(f"   æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB\n")

        # åˆ›å»º DataFlowEngine
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        source_config_id = f"hotpotqa-corpus-{timestamp}"

        self._log(f"åˆ›å»ºä¿¡æ¯æº...")
        self._log(f"   ä¿¡æ¯æº ID: {source_config_id}")
        self._log(f"   ä¿¡æ¯æºåç§°: {source_name}")
        self._log(f"   æè¿°: {source_description}\n")

        engine = DataFlowEngine(source_config_id=source_config_id)

        # Load é˜¶æ®µ - åŠ è½½æ–‡æ¡£
        self._log(f"å¼€å§‹åŠ è½½è¯­æ–™åº“æ–‡ä»¶...")
        load_start = time.perf_counter()

        try:
            await engine.load_async(
                DocumentLoadConfig(
                    path=str(self.corpus_md_path),
                    recursive=False,
                    source_config_id=source_config_id
                )
            )
            load_time = time.perf_counter() - load_start
            self._log(f"âœ… æ–‡æ¡£åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.1f} ç§’\n")
        except Exception as e:
            error_msg = f"æ–‡æ¡£åŠ è½½å¤±è´¥: {e}"
            self._log_error(error_msg, exc_info=True)
            return {'status': 'error', 'message': error_msg}

        # è·å– Load ç»“æœ
        engine_result = engine.get_result()
        if not engine_result or not engine_result.load_result:
            error_msg = "Load é˜¶æ®µå¤±è´¥ï¼šæ— æ³•è·å–åŠ è½½ç»“æœ"
            self._log_error(error_msg)
            return {'status': 'error', 'message': error_msg}

        # ä» engine_result è·å–æ•°æ®
        # Note: article_id åœ¨ engine_result ä¸Šï¼Œchunk_count åœ¨ load_result.stats ä¸­
        try:
            article_id = engine_result.article_id
            load_result = engine_result.load_result
            sections_count = load_result.stats.get("chunk_count", 0) if load_result.stats else 0

            self._log(f"Load ç»Ÿè®¡:")
            self._log(f"   Article ID: {article_id}")
            self._log(f"   æ–‡æ¡£ç‰‡æ®µæ•°: {sections_count}\n")
        except Exception as e:
            error_msg = f"è¯»å– Load ç»“æœå¤±è´¥: {e}"
            self._log_error(error_msg, exc_info=True)
            return {'status': 'error', 'message': error_msg}

        events_count = 0

        # Extract é˜¶æ®µ - æå–äº‹é¡¹ï¼ˆå¯é€‰ï¼‰
        if enable_extraction:
            self._log(f"å¼€å§‹æå–äº‹é¡¹...")
            extract_start = time.perf_counter()

            try:
                await engine.extract_async(
                    ExtractBaseConfig(
                        parallel=True,
                        max_concurrency=50
                    )
                )
                extract_time = time.perf_counter() - extract_start
                self._log(f"âœ… äº‹é¡¹æå–å®Œæˆï¼Œè€—æ—¶: {extract_time:.1f} ç§’\n")

                # è·å– Extract ç»“æœ
                engine_result = engine.get_result()
                if engine_result and engine_result.extract_result:
                    extract_result = engine_result.extract_result
                    events_count = len(extract_result.data_ids) if extract_result.data_ids else 0
                    self._log(f"Extract ç»Ÿè®¡:")
                    self._log(f"   ç”Ÿæˆäº‹é¡¹æ•°: {events_count}\n")
                else:
                    self._log(f"âš ï¸  Extract ç»“æœä¸ºç©º\n")
            except Exception as e:
                error_msg = f"äº‹é¡¹æå–å¤±è´¥: {e}"
                self._log_error(error_msg, exc_info=True)
                # æå–å¤±è´¥ä¸è¿”å›é”™è¯¯ï¼Œå› ä¸º Load å·²ç»æˆåŠŸ
        else:
            extract_time = 0
            self._log(f"è·³è¿‡æå–é˜¶æ®µï¼ˆenable_extraction=Falseï¼‰\n")

        # æ„å»ºç»“æœ
        total_time = load_time + extract_time
        result = {
            "source_config_id": source_config_id,
            "source_name": source_name,
            "source_description": source_description,
            "article_id": article_id,
            "sections_count": sections_count,
            "events_count": events_count,
            "load_time_seconds": load_time,
            "extract_time_seconds": extract_time if enable_extraction else 0,
            "total_processing_time_seconds": total_time,
            "corpus_file": str(self.corpus_md_path),
            "corpus_size_mb": file_size_mb,
            "timestamp": timestamp,
            "status": "completed",
            "extraction_enabled": enable_extraction
        }

        # ä¿å­˜ç»“æœ
        with open(self.process_result_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        self._log(f"ğŸ’¾ å¤„ç†ç»“æœå·²ä¿å­˜: {self.process_result_path}\n")

        # è¿”å›ç»“æœ
        self._log("=" * 60)
        self._log("âœ… é˜¶æ®µ3å®Œæˆ")
        self._log("=" * 60)

        return result



# ==================== Usage Examples ====================

if __name__ == "__main__":
    import argparse

    # è®¾ç½®å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='HotpotQA æ•°æ®å¤„ç†æµç¨‹')
    parser.add_argument('--start', type=int, default=0, help='èµ·å§‹æ ·æœ¬ç´¢å¼• (default: 0)')
    parser.add_argument('--end', type=int, default=None, help='ç»“æŸæ ·æœ¬ç´¢å¼• (default: None, å¤„ç†å…¨éƒ¨æ•°æ®)')
    parser.add_argument('--enable-extraction', action='store_false', help='ç¦ç”¨äº‹é¡¹æå– (é»˜è®¤: å¯ç”¨)')
    parser.add_argument('--no-upload', action='store_true', help='ä¸ä¸Šä¼ åˆ°ç³»ç»Ÿï¼Œåªæ„å»ºæ•°æ®æ–‡ä»¶ (é»˜è®¤: ä¸Šä¼ )')
    parser.add_argument('--no-log', action='store_true', help='ç¦ç”¨æ—¥å¿—è¾“å‡º')

    args = parser.parse_args()

    # ç¤ºä¾‹1ï¼šåŸºæœ¬ç”¨æ³•ï¼ˆå¸¦è‡ªåŠ¨æ—¶é—´æˆ³æ–‡ä»¶å¤¹ï¼‰
    async def example1(sample_start: int, sample_end: int, enable_extraction: bool, skip_upload: bool):
        # åˆ›å»ºæ–‡ä»¶å¤¹: data/source/YYYYMMDD_HHMMSS/
        processor = HotpotQAProcessor(
            enable_logging=not args.no_log,
            use_timestamp_folder=True  # é»˜è®¤é€‰é¡¹ï¼Œåˆ›å»ºæ—¶é—´æˆ³å­æ–‡ä»¶å¤¹
        )

        # æ‰“å°è¾“å‡ºä½ç½®
        processor.print_output_info()
        print(f"\nå¤„ç†æ ·æœ¬èŒƒå›´: [{sample_start}:{sample_end}]")
        print(f"äº‹é¡¹æå–: {'å¯ç”¨' if enable_extraction else 'ç¦ç”¨'}")
        print(f"ä¸Šä¼ ä¿¡æ¯æº: {'ç¦ç”¨' if skip_upload else 'å¯ç”¨'}")

        # åˆ†é˜¶æ®µå¤„ç†
        print("\né˜¶æ®µ1: æ„å»ºè¯­æ–™åº“...")
        stats = processor.build_corpus(sample_start=sample_start, sample_end=sample_end)
        print(f"âœ“ è¯­æ–™åº“æ„å»ºå®Œæˆ: {stats.get('final_chunks', 0)} ä¸ªchunks")

        print("\né˜¶æ®µ2: æå–æ ‡å‡†ç­”æ¡ˆ...")
        stats = processor.extract_oracle(sample_start=sample_start, sample_end=sample_end)
        print(f"âœ“ æ ‡å‡†ç­”æ¡ˆæå–å®Œæˆ: {stats.get('question_count', 0)} ä¸ªé—®é¢˜")

        # é˜¶æ®µ3: ä¸Šä¼ åˆ°ç³»ç»Ÿï¼ˆå¯é€‰ï¼‰
        if not skip_upload:
            print("\né˜¶æ®µ3: ä¸Šä¼ åˆ°ç³»ç»Ÿ...")
            result = await processor.upload_corpus(enable_extraction=enable_extraction)
            print(f"âœ“ ä¸Šä¼ å®Œæˆ: source_config_id={result.get('source_config_id')}")
        else:
            print("\né˜¶æ®µ3: è·³è¿‡ä¸Šä¼ ï¼ˆ--no-uploadï¼‰")

        # æ˜¾ç¤ºæ–‡ä»¶ä¿å­˜ä½ç½®
        print(f"\n{'='*60}")
        print("æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜è‡³:")
        print(f"  {processor.get_output_folder()}")
        print(f"{'='*60}")


    # è¿è¡Œç¤ºä¾‹
    print("=" * 60)
    print("HotpotQAProcessor ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    print(f"\nå¤„ç†å‚æ•°:")
    print(f"  æ ·æœ¬èŒƒå›´: [{args.start}:{args.end}]")
    print(f"  äº‹é¡¹æå–: {'å¯ç”¨' if args.enable_extraction else 'ç¦ç”¨'}")
    print(f"  ä¸Šä¼ ä¿¡æ¯æº: {'ç¦ç”¨' if args.no_upload else 'å¯ç”¨'}")
    print(f"  æ—¥å¿—è¾“å‡º: {'å¯ç”¨' if not args.no_log else 'ç¦ç”¨'}")
    print("=" * 60)

    # æ‰§è¡Œå¤„ç†ï¼ˆæ³¨æ„ï¼š--enable-extraction ç°åœ¨æ˜¯å¦å®šæ ‡å¿—ï¼‰
    asyncio.run(example1(args.start, args.end, args.enable_extraction, args.no_upload))
