#!/usr/bin/env python3
"""
ç®€å•çš„ RAG æ–‡æ¡£ä¸Šä¼ è„šæœ¬

åŠŸèƒ½ï¼š
1. ä½¿ç”¨ MarkdownParser è§£æ md æ–‡ä»¶
2. ä½¿ç”¨ EmbeddingClient æ‰¹é‡ç”Ÿæˆ embedding
3. æ‰¹é‡ä¸Šä¼ åˆ° Elasticsearch
4. è®°å½•ä¿¡æ¯æº ID å’Œæ®µè½ ID

ç‰¹æ€§ï¼š
- Embedding ç”Ÿæˆé‡è¯•æœºåˆ¶
- ES ä¸Šä¼ é‡è¯•æœºåˆ¶
- è¯¦ç»†çš„æ—¥å¿—è®°å½•
- è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶å¤¹
"""

import asyncio
import json
import logging
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).resolve().parent.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# åŠ è½½ .env æ–‡ä»¶
from dotenv import load_dotenv
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"[INFO] å·²åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
else:
    print(f"[WARN] æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")

# å¯¼å…¥ä¾èµ–
from dataflow.modules.load.parser import MarkdownParser
from dataflow.core.ai.embedding import EmbeddingClient
from dataflow.core.storage.elasticsearch import ElasticsearchClient, ESConfig
from dataflow.core.storage.repositories.source_chunk_repository import SourceChunkRepository


class SimpleRAGUploader:
    """ç®€å•çš„ RAG æ–‡æ¡£ä¸Šä¼ å™¨"""

    def __init__(
        self,
        output_dir: Optional[Path] = None,
        enable_logging: bool = True,
        use_timestamp_folder: bool = True,
        max_retries: int = 3,
        retry_delay: int = 2,
        embedding_batch_size: int = 10,
        es_batch_size: int = 50
    ):
        """
        åˆå§‹åŒ–ä¸Šä¼ å™¨

        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: evaluation/hotpotqa_evaluation/data/rag_uploadsï¼‰
            enable_logging: æ˜¯å¦å¯ç”¨æ—¥å¿—
            use_timestamp_folder: æ˜¯å¦ä¸ºæ¯æ¬¡è¿è¡Œåˆ›å»ºæ—¶é—´æˆ³å­æ–‡ä»¶å¤¹
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
            embedding_batch_size: Embedding æ‰¹é‡å¤§å°
            es_batch_size: ES æ‰¹é‡ç´¢å¼•å¤§å°
        """
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = None
        if enable_logging:
            self._setup_logging()

        # é…ç½®å‚æ•°
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.embedding_batch_size = embedding_batch_size
        self.es_batch_size = es_batch_size

        # è¾“å‡ºç›®å½•
        base_dir = output_dir or (Path(__file__).parent.parent / "data" / "rag_uploads")

        if use_timestamp_folder:
            self.run_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.output_dir = base_dir / self.run_timestamp
            self.output_dir.mkdir(parents=True, exist_ok=True)
            self._log(f"åˆ›å»ºæ—¶é—´æˆ³æ–‡ä»¶å¤¹: {self.output_dir}")
        else:
            self.output_dir = base_dir
            self.output_dir.mkdir(parents=True, exist_ok=True)

        # è¾“å‡ºæ–‡ä»¶è·¯å¾„
        self.result_path = self.output_dir / "upload_result.json"
        self.chunks_path = self.output_dir / "chunks.jsonl"

        # åˆå§‹åŒ–ç»„ä»¶
        self.parser = None
        self.embedding_client = None
        self.es_client = None
        self.chunk_repo = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_chunks': 0,
            'embedding_success': 0,
            'embedding_failed': 0,
            'embedding_retries': 0,
            'es_success': 0,
            'es_failed': 0,
            'es_retries': 0,
        }

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S'
        )
        self.logger = logging.getLogger(self.__class__.__name__)

    def _log(self, message: str, level: str = 'info'):
        """ç»Ÿä¸€æ—¥å¿—æ–¹æ³•"""
        if self.logger:
            getattr(self.logger, level.lower())(message)
        else:
            print(message)

    async def _init_components(self):
        """åˆå§‹åŒ–ç»„ä»¶"""
        self._log("åˆå§‹åŒ–ç»„ä»¶...")

        # 1. åˆå§‹åŒ– MarkdownParser
        self.parser = MarkdownParser(
            max_tokens=1000,
            min_content_length=100,
            merge_short_sections=True
        )
        self._log("âœ“ MarkdownParser åˆå§‹åŒ–å®Œæˆ")

        # 2. åˆå§‹åŒ– EmbeddingClient
        self.embedding_client = EmbeddingClient()
        self._log("âœ“ EmbeddingClient åˆå§‹åŒ–å®Œæˆ")

        # 3. åˆå§‹åŒ– ElasticsearchClient
        self.es_client = ElasticsearchClient(config=ESConfig.from_env())
        self._log("âœ“ ElasticsearchClient åˆå§‹åŒ–å®Œæˆ")

        # 4. åˆå§‹åŒ– SourceChunkRepository
        self.chunk_repo = SourceChunkRepository(es_client=self.es_client)
        self._log("âœ“ SourceChunkRepository åˆå§‹åŒ–å®Œæˆ")

    async def _generate_embeddings_with_retry(
        self,
        texts: List[str]
    ) -> Optional[List[List[float]]]:
        """
        æ‰¹é‡ç”Ÿæˆ embeddingï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨

        Returns:
            embedding å‘é‡åˆ—è¡¨ï¼Œå¤±è´¥è¿”å› None
        """
        for attempt in range(self.max_retries):
            try:
                embeddings = await self.embedding_client.batch_generate(texts)
                self.stats['embedding_success'] += len(texts)
                return embeddings
            except Exception as e:
                self.stats['embedding_retries'] += 1
                if attempt < self.max_retries - 1:
                    self._log(
                        f"âš ï¸ Embedding ç”Ÿæˆå¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}ï¼Œ"
                        f"{self.retry_delay}ç§’åé‡è¯•...",
                        'warning'
                    )
                    await asyncio.sleep(self.retry_delay)
                else:
                    self._log(f"âŒ Embedding ç”Ÿæˆå¤±è´¥ï¼ˆå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰: {e}", 'error')
                    self.stats['embedding_failed'] += len(texts)
                    return None

    async def _bulk_index_with_retry(
        self,
        documents: List[Dict[str, Any]],
        source_config_id: str
    ) -> bool:
        """
        æ‰¹é‡ç´¢å¼•åˆ° ESï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰

        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            source_config_id: ä¿¡æ¯æº ID

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        for attempt in range(self.max_retries):
            try:
                result = await self.es_client.bulk_index(
                    index="source_chunks",
                    documents=documents,
                    return_details=True,
                    routing=source_config_id
                )

                if result['success']:
                    self.stats['es_success'] += result['success_count']
                    self._log(f"âœ“ æ‰¹é‡ç´¢å¼•æˆåŠŸ: {result['success_count']} ä¸ªæ–‡æ¡£")
                    return True
                else:
                    self._log(
                        f"âš ï¸ æ‰¹é‡ç´¢å¼•éƒ¨åˆ†å¤±è´¥: æˆåŠŸ {result['success_count']}, "
                        f"å¤±è´¥ {result['error_count']}",
                        'warning'
                    )
                    self.stats['es_success'] += result['success_count']
                    self.stats['es_failed'] += result['error_count']

                    # å¦‚æœæœ‰é”™è¯¯ï¼Œè®°å½•é”™è¯¯è¯¦æƒ…
                    if result['errors']:
                        for error in result['errors'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                            self._log(f"   é”™è¯¯: ID={error.get('id')}, {error.get('error')}", 'error')

                    return False

            except Exception as e:
                self.stats['es_retries'] += 1
                if attempt < self.max_retries - 1:
                    self._log(
                        f"âš ï¸ ES æ‰¹é‡ç´¢å¼•å¤±è´¥ (å°è¯• {attempt + 1}/{self.max_retries}): {e}ï¼Œ"
                        f"{self.retry_delay}ç§’åé‡è¯•...",
                        'warning'
                    )
                    await asyncio.sleep(self.retry_delay)
                else:
                    self._log(f"âŒ ES æ‰¹é‡ç´¢å¼•å¤±è´¥ï¼ˆå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰: {e}", 'error')
                    self.stats['es_failed'] += len(documents)
                    return False

    async def upload_markdown(
        self,
        md_file_path: Path,
        source_name: Optional[str] = None,
        source_description: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        ä¸Šä¼  Markdown æ–‡ä»¶

        Args:
            md_file_path: Markdown æ–‡ä»¶è·¯å¾„
            source_name: ä¿¡æ¯æºåç§°ï¼ˆé»˜è®¤ä½¿ç”¨æ–‡ä»¶åï¼‰
            source_description: ä¿¡æ¯æºæè¿°

        Returns:
            ä¸Šä¼ ç»“æœå­—å…¸
        """
        self._log("=" * 60)
        self._log("å¼€å§‹ä¸Šä¼  Markdown æ–‡ä»¶")
        self._log("=" * 60)

        # éªŒè¯æ–‡ä»¶
        if not md_file_path.exists():
            error_msg = f"æ–‡ä»¶ä¸å­˜åœ¨: {md_file_path}"
            self._log(error_msg, 'error')
            return {'status': 'error', 'message': error_msg}

        file_size_mb = md_file_path.stat().st_size / 1024 / 1024
        self._log(f"æ–‡ä»¶: {md_file_path}")
        self._log(f"æ–‡ä»¶å¤§å°: {file_size_mb:.2f} MB\n")

        # åˆå§‹åŒ–ç»„ä»¶
        await self._init_components()

        # ç”Ÿæˆä¿¡æ¯æº ID
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        source_config_id = f"rag-{md_file_path.stem}-{timestamp}"
        source_name = source_name or md_file_path.stem
        source_description = source_description or f"RAG upload from {md_file_path.name}"

        self._log(f"ä¿¡æ¯æº ID: {source_config_id}")
        self._log(f"ä¿¡æ¯æºåç§°: {source_name}")
        self._log(f"æè¿°: {source_description}\n")

        # 1. è§£ææ–‡ä»¶
        self._log("é˜¶æ®µ 1: è§£æ Markdown æ–‡ä»¶...")
        parse_start = time.perf_counter()

        try:
            content, sections = self.parser.parse_file(md_file_path)
            parse_time = time.perf_counter() - parse_start

            self.stats['total_chunks'] = len(sections)
            self._log(f"âœ“ è§£æå®Œæˆ: {len(sections)} ä¸ªæ®µè½ï¼Œè€—æ—¶ {parse_time:.2f}ç§’\n")
        except Exception as e:
            error_msg = f"æ–‡ä»¶è§£æå¤±è´¥: {e}"
            self._log(error_msg, 'error')
            return {'status': 'error', 'message': error_msg}

        # 2. æ‰¹é‡ç”Ÿæˆ Embedding
        self._log("é˜¶æ®µ 2: æ‰¹é‡ç”Ÿæˆ Embedding...")
        embedding_start = time.perf_counter()

        # å‡†å¤‡æ–‡æœ¬åˆ—è¡¨ï¼ˆä½¿ç”¨ contentï¼Œå¦‚æœæœ‰ heading åˆ™æ‹¼æ¥ï¼‰
        chunks_data = []
        for idx, section in enumerate(sections):
            heading = section.heading or ""
            content = section.content or ""

            # æ‹¼æ¥æ ‡é¢˜å’Œå†…å®¹
            full_text = f"{heading}\n{content}" if heading else content

            chunks_data.append({
                'index': idx,
                'heading': heading,
                'content': content,
                'full_text': full_text,
                'section': section
            })

        # æ‰¹é‡ç”Ÿæˆ embedding
        all_embeddings = []
        failed_indices = []

        for i in range(0, len(chunks_data), self.embedding_batch_size):
            batch = chunks_data[i:i + self.embedding_batch_size]
            batch_texts = [chunk['content'] for chunk in batch]  # åªç”¨ content ç”Ÿæˆ embedding

            self._log(f"ç”Ÿæˆ Embedding æ‰¹æ¬¡ {i // self.embedding_batch_size + 1}: "
                     f"{len(batch)} ä¸ªæ®µè½...")

            embeddings = await self._generate_embeddings_with_retry(batch_texts)

            if embeddings:
                all_embeddings.extend(embeddings)
            else:
                # è®°å½•å¤±è´¥çš„ç´¢å¼•
                failed_indices.extend([chunk['index'] for chunk in batch])
                # æ·»åŠ  None å ä½ç¬¦
                all_embeddings.extend([None] * len(batch))

        embedding_time = time.perf_counter() - embedding_start
        self._log(f"âœ“ Embedding ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {embedding_time:.2f}ç§’")
        self._log(f"  æˆåŠŸ: {self.stats['embedding_success']}, "
                 f"å¤±è´¥: {self.stats['embedding_failed']}, "
                 f"é‡è¯•æ¬¡æ•°: {self.stats['embedding_retries']}\n")

        # 3. æ‰¹é‡ä¸Šä¼ åˆ° ES
        self._log("é˜¶æ®µ 3: æ‰¹é‡ä¸Šä¼ åˆ° Elasticsearch...")
        es_start = time.perf_counter()

        # å‡†å¤‡ ES æ–‡æ¡£ï¼ˆåªåŒ…å«æˆåŠŸç”Ÿæˆ embedding çš„æ®µè½ï¼‰
        es_documents = []
        chunk_ids = []

        for idx, (chunk, embedding) in enumerate(zip(chunks_data, all_embeddings)):
            if embedding is None:
                continue  # è·³è¿‡å¤±è´¥çš„ embedding

            chunk_id = f"{source_config_id}_chunk_{idx}"
            chunk_ids.append(chunk_id)

            # æ„å»º ES æ–‡æ¡£
            document = {
                "id": chunk_id,
                "chunk_id": chunk_id,
                "source_id": source_config_id,  # ä½¿ç”¨ source_config_id ä½œä¸º source_id
                "source_config_id": source_config_id,
                "rank": idx,
                "heading": chunk['heading'],
                "content": chunk['content'],
                "heading_vector": embedding if chunk['heading'] else None,  # å¦‚æœæœ‰æ ‡é¢˜ï¼Œä½¿ç”¨åŒæ ·çš„ embedding
                "content_vector": embedding,
                "references": [],
                "chunk_type": "paragraph",
                "content_length": len(chunk['content'])
            }

            es_documents.append(document)

        # æ‰¹é‡ä¸Šä¼ 
        total_uploaded = 0
        for i in range(0, len(es_documents), self.es_batch_size):
            batch = es_documents[i:i + self.es_batch_size]

            self._log(f"ä¸Šä¼ æ‰¹æ¬¡ {i // self.es_batch_size + 1}: {len(batch)} ä¸ªæ–‡æ¡£...")

            success = await self._bulk_index_with_retry(batch, source_config_id)
            if success:
                total_uploaded += len(batch)

        es_time = time.perf_counter() - es_start
        self._log(f"âœ“ ES ä¸Šä¼ å®Œæˆï¼Œè€—æ—¶ {es_time:.2f}ç§’")
        self._log(f"  æˆåŠŸ: {self.stats['es_success']}, "
                 f"å¤±è´¥: {self.stats['es_failed']}, "
                 f"é‡è¯•æ¬¡æ•°: {self.stats['es_retries']}\n")

        # 4. ä¿å­˜æ®µè½ä¿¡æ¯åˆ° JSONL
        self._log("ä¿å­˜æ®µè½ä¿¡æ¯...")
        with open(self.chunks_path, 'w', encoding='utf-8') as f:
            for idx, (chunk, embedding) in enumerate(zip(chunks_data, all_embeddings)):
                chunk_info = {
                    'chunk_id': f"{source_config_id}_chunk_{idx}",
                    'index': idx,
                    'heading': chunk['heading'],
                    'content': chunk['content'],
                    'has_embedding': embedding is not None,
                }
                f.write(json.dumps(chunk_info, ensure_ascii=False) + '\n')

        self._log(f"âœ“ æ®µè½ä¿¡æ¯å·²ä¿å­˜: {self.chunks_path}\n")

        # 5. æ„å»ºç»“æœ
        total_time = parse_time + embedding_time + es_time
        result = {
            "status": "completed",
            "source_config_id": source_config_id,
            "source_name": source_name,
            "source_description": source_description,
            "file_path": str(md_file_path),
            "file_size_mb": file_size_mb,
            "total_chunks": self.stats['total_chunks'],
            "chunks_uploaded": total_uploaded,
            "embedding_stats": {
                "success": self.stats['embedding_success'],
                "failed": self.stats['embedding_failed'],
                "retries": self.stats['embedding_retries']
            },
            "es_stats": {
                "success": self.stats['es_success'],
                "failed": self.stats['es_failed'],
                "retries": self.stats['es_retries']
            },
            "chunk_ids": chunk_ids,
            "failed_chunk_indices": failed_indices,
            "timing": {
                "parse_time": parse_time,
                "embedding_time": embedding_time,
                "es_time": es_time,
                "total_time": total_time
            },
            "output_dir": str(self.output_dir),
            "chunks_file": str(self.chunks_path),
            "timestamp": timestamp
        }

        # 6. ä¿å­˜ç»“æœ
        with open(self.result_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        self._log(f"ğŸ’¾ ä¸Šä¼ ç»“æœå·²ä¿å­˜: {self.result_path}\n")

        # 7. æ‰“å°æ‘˜è¦
        self._log("=" * 60)
        self._log("âœ… ä¸Šä¼ å®Œæˆ")
        self._log("=" * 60)
        self._log(f"æ€»æ®µè½æ•°: {self.stats['total_chunks']}")
        self._log(f"æˆåŠŸä¸Šä¼ : {total_uploaded}")
        self._log(f"å¤±è´¥æ•°: {self.stats['es_failed']}")
        self._log(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
        self._log(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        self._log("=" * 60)

        return result

    async def delete_by_source_config_id(
        self,
        source_config_id: str,
        dry_run: bool = False
    ) -> Dict[str, Any]:
        """
        æ ¹æ® source_config_id åˆ é™¤æ‰€æœ‰ç›¸å…³æ–‡æ¡£

        Args:
            source_config_id: ä¿¡æ¯æº ID
            dry_run: æ˜¯å¦ä»…é¢„è§ˆï¼ˆä¸å®é™…åˆ é™¤ï¼‰

        Returns:
            åˆ é™¤ç»“æœå­—å…¸
        """
        self._log("=" * 60)
        self._log(f"åˆ é™¤ä¿¡æ¯æº: {source_config_id}")
        if dry_run:
            self._log("âš ï¸ é¢„è§ˆæ¨¡å¼ï¼ˆä¸ä¼šå®é™…åˆ é™¤ï¼‰")
        self._log("=" * 60)

        # åˆå§‹åŒ–ç»„ä»¶ï¼ˆåªéœ€è¦ ES å®¢æˆ·ç«¯ï¼‰
        if not self.es_client:
            self.es_client = ElasticsearchClient(config=ESConfig.from_env())
            self._log("âœ“ ElasticsearchClient åˆå§‹åŒ–å®Œæˆ")

        try:
            # 1. æŸ¥è¯¢è¯¥ source_config_id ä¸‹çš„æ‰€æœ‰æ–‡æ¡£
            self._log(f"\næŸ¥è¯¢ source_config_id={source_config_id} çš„æ–‡æ¡£...")

            query = {
                "query": {
                    "term": {
                        "source_config_id": source_config_id
                    }
                },
                "size": 10000  # æœ€å¤šè¿”å› 10000 ä¸ªæ–‡æ¡£
            }

            response = await self.es_client.client.search(
                index="source_chunks",
                body=query,
                routing=source_config_id  # ä½¿ç”¨è·¯ç”±æé«˜æŸ¥è¯¢æ•ˆç‡
            )

            hits = response["hits"]["hits"]
            total_count = response["hits"]["total"]["value"]

            self._log(f"æ‰¾åˆ° {total_count} ä¸ªæ–‡æ¡£")

            if total_count == 0:
                self._log("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£", 'warning')
                return {
                    'status': 'success',
                    'source_config_id': source_config_id,
                    'deleted_count': 0,
                    'message': 'æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£'
                }

            # æ˜¾ç¤ºå‰å‡ ä¸ªæ–‡æ¡£çš„ä¿¡æ¯
            self._log(f"\né¢„è§ˆå‰ 5 ä¸ªæ–‡æ¡£:")
            for i, hit in enumerate(hits[:5], 1):
                doc = hit["_source"]
                self._log(f"  [{i}] ID: {doc.get('chunk_id', 'N/A')[:50]}...")
                self._log(f"      Heading: {doc.get('heading', 'N/A')[:50]}...")

            if total_count > 5:
                self._log(f"  ... è¿˜æœ‰ {total_count - 5} ä¸ªæ–‡æ¡£")

            if dry_run:
                self._log(f"\nâš ï¸ é¢„è§ˆæ¨¡å¼ï¼šå°†åˆ é™¤ {total_count} ä¸ªæ–‡æ¡£ï¼ˆæœªå®é™…åˆ é™¤ï¼‰")
                return {
                    'status': 'dry_run',
                    'source_config_id': source_config_id,
                    'would_delete_count': total_count,
                    'message': f'é¢„è§ˆæ¨¡å¼ï¼šå°†åˆ é™¤ {total_count} ä¸ªæ–‡æ¡£'
                }

            # 2. ç”¨æˆ·ç¡®è®¤åˆ é™¤
            self._log(f"\n{'='*60}")
            self._log(f"âš ï¸  è­¦å‘Šï¼šå³å°†åˆ é™¤ {total_count} ä¸ªæ–‡æ¡£")
            self._log(f"âš ï¸  ä¿¡æ¯æº ID: {source_config_id}")
            self._log(f"âš ï¸  æ­¤æ“ä½œä¸å¯æ’¤é”€ï¼")
            self._log(f"{'='*60}")

            # äº¤äº’å¼ç¡®è®¤
            try:
                confirmation = input("\næ˜¯å¦ç¡®è®¤åˆ é™¤ï¼Ÿè¾“å…¥ 'yes' æˆ– 'y' ç¡®è®¤ï¼Œå…¶ä»–ä»»æ„é”®å–æ¶ˆ: ").strip().lower()

                if confirmation not in ['yes', 'y']:
                    self._log("\nâŒ ç”¨æˆ·å–æ¶ˆåˆ é™¤æ“ä½œ", 'warning')
                    return {
                        'status': 'cancelled',
                        'source_config_id': source_config_id,
                        'message': 'ç”¨æˆ·å–æ¶ˆåˆ é™¤æ“ä½œ'
                    }

                self._log("\nâœ“ ç”¨æˆ·å·²ç¡®è®¤åˆ é™¤")
            except (EOFError, KeyboardInterrupt):
                self._log("\n\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ", 'warning')
                return {
                    'status': 'cancelled',
                    'source_config_id': source_config_id,
                    'message': 'ç”¨æˆ·ä¸­æ–­æ“ä½œ'
                }

            # 3. æ‰§è¡Œåˆ é™¤
            self._log(f"\nå¼€å§‹åˆ é™¤ {total_count} ä¸ªæ–‡æ¡£...")
            delete_start = time.perf_counter()

            # ä½¿ç”¨ delete_by_query API æ‰¹é‡åˆ é™¤
            delete_response = await self.es_client.client.delete_by_query(
                index="source_chunks",
                body=query,
                routing=source_config_id,
                refresh=True  # ç«‹å³åˆ·æ–°ç´¢å¼•
            )

            deleted_count = delete_response.get("deleted", 0)
            delete_time = time.perf_counter() - delete_start

            self._log(f"âœ“ åˆ é™¤å®Œæˆ: {deleted_count} ä¸ªæ–‡æ¡£ï¼Œè€—æ—¶ {delete_time:.2f}ç§’")

            # 4. éªŒè¯åˆ é™¤ç»“æœ
            self._log("\néªŒè¯åˆ é™¤ç»“æœ...")
            verify_response = await self.es_client.client.search(
                index="source_chunks",
                body=query,
                routing=source_config_id
            )

            remaining_count = verify_response["hits"]["total"]["value"]

            if remaining_count == 0:
                self._log("âœ“ éªŒè¯é€šè¿‡ï¼šæ‰€æœ‰æ–‡æ¡£å·²åˆ é™¤")
            else:
                self._log(f"âš ï¸ è­¦å‘Šï¼šä»æœ‰ {remaining_count} ä¸ªæ–‡æ¡£æœªåˆ é™¤", 'warning')

            # 5. æ„å»ºç»“æœ
            result = {
                "status": "completed",
                "source_config_id": source_config_id,
                "deleted_count": deleted_count,
                "remaining_count": remaining_count,
                "delete_time": delete_time,
                "timestamp": datetime.now().isoformat()
            }

            # 6. ä¿å­˜åˆ é™¤ç»“æœ
            delete_result_path = self.output_dir / f"delete_result_{source_config_id}.json"
            with open(delete_result_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            self._log(f"ğŸ’¾ åˆ é™¤ç»“æœå·²ä¿å­˜: {delete_result_path}")

            # 7. æ‰“å°æ‘˜è¦
            self._log("\n" + "=" * 60)
            self._log("âœ… åˆ é™¤å®Œæˆ")
            self._log("=" * 60)
            self._log(f"ä¿¡æ¯æº ID: {source_config_id}")
            self._log(f"åˆ é™¤æ–‡æ¡£æ•°: {deleted_count}")
            self._log(f"å‰©ä½™æ–‡æ¡£æ•°: {remaining_count}")
            self._log(f"è€—æ—¶: {delete_time:.2f}ç§’")
            self._log("=" * 60)

            return result

        except Exception as e:
            error_msg = f"åˆ é™¤å¤±è´¥: {e}"
            self._log(error_msg, 'error')
            return {
                'status': 'error',
                'source_config_id': source_config_id,
                'message': error_msg
            }

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.es_client and hasattr(self.es_client, 'client'):
            await self.es_client.client.close()


async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description='ç®€å•çš„ RAG æ–‡æ¡£ä¸Šä¼ /åˆ é™¤è„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä¸Šä¼ æ–‡æ¡£ï¼ˆé»˜è®¤ï¼‰
  python %(prog)s document.md

  # åˆ é™¤æŒ‡å®šä¿¡æ¯æº
  python %(prog)s --delete --source-config-id rag-document-20231209_143025

  # åˆ é™¤å‰é¢„è§ˆï¼ˆä¸å®é™…åˆ é™¤ï¼‰
  python %(prog)s --delete --source-config-id rag-document-20231209_143025 --dry-run
        """
    )

    # æ“ä½œæ¨¡å¼
    parser.add_argument('--delete', action='store_true',
                       help='åˆ é™¤æ¨¡å¼ï¼ˆåˆ é™¤æŒ‡å®š source_config_id çš„æ‰€æœ‰æ–‡æ¡£ï¼‰')
    parser.add_argument('--source-config-id', type=str,
                       help='[åˆ é™¤æ¨¡å¼] è¦åˆ é™¤çš„ä¿¡æ¯æº ID')
    parser.add_argument('--dry-run', action='store_true',
                       help='[åˆ é™¤æ¨¡å¼] é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…åˆ é™¤')

    # ä¸Šä¼ æ¨¡å¼å‚æ•°
    parser.add_argument('md_file', type=Path, nargs='?',
                       help='[ä¸Šä¼ æ¨¡å¼] Markdown æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--name', type=str, help='[ä¸Šä¼ æ¨¡å¼] ä¿¡æ¯æºåç§°ï¼ˆé»˜è®¤ä½¿ç”¨æ–‡ä»¶åï¼‰')
    parser.add_argument('--description', type=str, help='[ä¸Šä¼ æ¨¡å¼] ä¿¡æ¯æºæè¿°')
    parser.add_argument('--output-dir', type=Path, help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max-retries', type=int, default=3, help='æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤: 3ï¼‰')
    parser.add_argument('--retry-delay', type=int, default=2, help='é‡è¯•å»¶è¿Ÿç§’æ•°ï¼ˆé»˜è®¤: 2ï¼‰')
    parser.add_argument('--embedding-batch-size', type=int, default=10,
                       help='[ä¸Šä¼ æ¨¡å¼] Embedding æ‰¹é‡å¤§å°ï¼ˆé»˜è®¤: 10ï¼‰')
    parser.add_argument('--es-batch-size', type=int, default=50,
                       help='[ä¸Šä¼ æ¨¡å¼] ES æ‰¹é‡ç´¢å¼•å¤§å°ï¼ˆé»˜è®¤: 50ï¼‰')
    parser.add_argument('--no-timestamp', action='store_true',
                       help='ä¸åˆ›å»ºæ—¶é—´æˆ³æ–‡ä»¶å¤¹')

    args = parser.parse_args()

    # åˆ›å»ºä¸Šä¼ å™¨/åˆ é™¤å™¨
    uploader = SimpleRAGUploader(
        output_dir=args.output_dir,
        enable_logging=True,
        use_timestamp_folder=not args.no_timestamp,
        max_retries=args.max_retries,
        retry_delay=args.retry_delay,
        embedding_batch_size=args.embedding_batch_size,
        es_batch_size=args.es_batch_size
    )

    try:
        # åˆ é™¤æ¨¡å¼
        if args.delete:
            if not args.source_config_id:
                print("[ERROR] åˆ é™¤æ¨¡å¼éœ€è¦æŒ‡å®š --source-config-id")
                sys.exit(1)

            # æ‰§è¡Œåˆ é™¤
            result = await uploader.delete_by_source_config_id(
                source_config_id=args.source_config_id,
                dry_run=args.dry_run
            )

            # æ‰“å°ç»“æœ
            print(f"\nåˆ é™¤ç»“æœ:")
            print(f"  çŠ¶æ€: {result.get('status')}")
            print(f"  ä¿¡æ¯æº ID: {result.get('source_config_id')}")

            if result.get('status') == 'dry_run':
                print(f"  é¢„è®¡åˆ é™¤: {result.get('would_delete_count')} ä¸ªæ–‡æ¡£")
                print(f"\næç¤ºï¼šè¿™æ˜¯é¢„è§ˆæ¨¡å¼ï¼Œæœªå®é™…åˆ é™¤æ–‡æ¡£")
                print(f"å¦‚éœ€å®é™…åˆ é™¤ï¼Œè¯·ç§»é™¤ --dry-run å‚æ•°")
            elif result.get('status') == 'cancelled':
                print(f"  ä¿¡æ¯: {result.get('message')}")
                print(f"\næ“ä½œå·²å–æ¶ˆï¼Œæœªåˆ é™¤ä»»ä½•æ–‡æ¡£")
            elif result.get('status') == 'completed':
                print(f"  åˆ é™¤æ–‡æ¡£æ•°: {result.get('deleted_count')}")
                print(f"  å‰©ä½™æ–‡æ¡£æ•°: {result.get('remaining_count')}")
                print(f"  è€—æ—¶: {result.get('delete_time', 0):.2f}ç§’")
            elif result.get('status') == 'error':
                print(f"  é”™è¯¯: {result.get('message')}")
                sys.exit(1)

        # ä¸Šä¼ æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        else:
            if not args.md_file:
                print("[ERROR] ä¸Šä¼ æ¨¡å¼éœ€è¦æŒ‡å®š Markdown æ–‡ä»¶è·¯å¾„")
                parser.print_help()
                sys.exit(1)

            # éªŒè¯æ–‡ä»¶
            if not args.md_file.exists():
                print(f"[ERROR] æ–‡ä»¶ä¸å­˜åœ¨: {args.md_file}")
                sys.exit(1)

            # æ‰§è¡Œä¸Šä¼ 
            result = await uploader.upload_markdown(
                md_file_path=args.md_file,
                source_name=args.name,
                source_description=args.description
            )

            # æ‰“å°ç»“æœ
            print(f"\nä¸Šä¼ ç»“æœ:")
            print(f"  çŠ¶æ€: {result.get('status')}")
            print(f"  ä¿¡æ¯æº ID: {result.get('source_config_id')}")
            print(f"  æ€»æ®µè½æ•°: {result.get('total_chunks')}")
            print(f"  æˆåŠŸä¸Šä¼ : {result.get('chunks_uploaded')}")
            print(f"  ç»“æœæ–‡ä»¶: {result.get('output_dir')}")

            if result.get('status') == 'error':
                sys.exit(1)

    except Exception as e:
        print(f"\n[ERROR] æ“ä½œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
    finally:
        # æ¸…ç†èµ„æº
        await uploader.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
