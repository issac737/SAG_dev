"""
Stage2: ç­”æ¡ˆç”Ÿæˆ

åŠŸèƒ½ï¼š
1. ä» retrieval_results.jsonl åŠ è½½æ£€ç´¢ç»“æœ
2. ä¸ºæ¯ä¸ªé—®é¢˜åŸºäºæ£€ç´¢åˆ°çš„æ®µè½ç”Ÿæˆç­”æ¡ˆï¼ˆä½¿ç”¨ LLMï¼‰
3. ä¿å­˜ç”Ÿæˆçš„ç­”æ¡ˆåˆ° generated_answers.jsonl

è¾“å…¥ï¼š
- retrieval_results.jsonlï¼ˆStage1 è¾“å‡ºï¼‰

è¾“å‡ºï¼š
- generated_answers.jsonlï¼ˆç”Ÿæˆçš„ç­”æ¡ˆï¼Œæ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼‰

ä½¿ç”¨æ–¹æ³•ï¼š
    # åŸºæœ¬ç”¨æ³•
    python stage2_generate.py

    # è‡ªå®šä¹‰è¾“å…¥/è¾“å‡ºè·¯å¾„
    python stage2_generate.py --input data/retrieval_results.jsonl --output data/generated_answers.jsonl

    # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
    python stage2_generate.py --verbose
"""

import asyncio
import argparse
import logging
from pathlib import Path
from typing import List
from datetime import datetime

# æ·»åŠ  evaluation ç›®å½•åˆ°è·¯å¾„
import sys
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
project_root = Path(__file__).parent.parent.parent.parent
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"[OK] å·²åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
else:
    print(f"[WARN] æœªæ‰¾åˆ° .env æ–‡ä»¶: {env_path}")

from hotpotqa_evaluation import config

# å¯¼å…¥ç³»ç»Ÿæ¨¡å—
from dataflow.core.ai.factory import create_llm_client
from dataflow.core.ai.models import LLMMessage, LLMRole

# å¯¼å…¥å…±äº«æ¨¡å—
from shared import (
    RetrievalResult,
    GeneratedAnswer,
    GenerationMetadata,
    read_jsonl,
    write_jsonl,
    validate_generated_answers,
)


async def generate_answer_with_metadata(
    result: RetrievalResult,
    llm_client,
    index: int,
    total: int,
    verbose: bool = False
) -> GeneratedAnswer:
    """
    ä¸ºå•ä¸ªé—®é¢˜ç”Ÿæˆç­”æ¡ˆï¼ˆå¸¦å…ƒæ•°æ®ï¼‰

    Args:
        result: æ£€ç´¢ç»“æœ
        llm_client: LLMå®¢æˆ·ç«¯
        index: å½“å‰ç´¢å¼•
        total: æ€»æ•°
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—

    Returns:
        ç”Ÿæˆçš„ç­”æ¡ˆå¯¹è±¡
    """
    question = result.question
    question_id = result.question_id

    # è·å–æ£€ç´¢åˆ°çš„æ®µè½
    contexts = [
        section.content_preview
        for section in result.retrieved_sections
    ]

    # è·å– oracle ä¿¡æ¯
    oracle_answer = result.answer
    oracle_chunk_ids = result.oracle_chunk_ids

    # æ‰“å°è¿›åº¦ï¼ˆç®€åŒ–æ¨¡å¼ï¼‰
    print(f"[{index}/{total}] ç”Ÿæˆç­”æ¡ˆ: {question[:60]}...")

    # ç”Ÿæˆç­”æ¡ˆ
    if not contexts:
        answer = ""
        if verbose:
            print(f"  [!] è­¦å‘Š: æ²¡æœ‰æ£€ç´¢åˆ°æ®µè½ï¼Œä½¿ç”¨ç©ºç­”æ¡ˆ")
    else:
        try:
            answer = await generate_answer(question, contexts, llm_client)
            if verbose:
                print(f"  [AI] ç­”æ¡ˆ: {answer[:100]}...")
        except Exception as e:
            print(f"  [X] ç”Ÿæˆå¤±è´¥: {e}")
            answer = ""

    # åˆ›å»º GeneratedAnswer å¯¹è±¡
    generation_metadata = GenerationMetadata(
        model=llm_client.model_name if hasattr(llm_client, 'model_name') else "unknown",
        temperature=0.3
    )

    generated_answer = GeneratedAnswer(
        question_id=question_id,
        question=question,
        generated_answer=answer,
        contexts_used=contexts,
        generation_metadata=generation_metadata
    )

    return generated_answer


async def generate_answer(
    question: str,
    contexts: List[str],
    llm_client
) -> str:
    """
    åŸºäºæ£€ç´¢åˆ°çš„æ®µè½ç”Ÿæˆç­”æ¡ˆ

    Args:
        question: ç”¨æˆ·é—®é¢˜
        contexts: æ£€ç´¢åˆ°çš„æ®µè½åˆ—è¡¨
        llm_client: LLMå®¢æˆ·ç«¯

    Returns:
        ç”Ÿæˆçš„ç­”æ¡ˆ
    """
    # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°æ®µè½ï¼Œè¿”å›ç©ºç­”æ¡ˆ
    if not contexts:
        return ""

    # æ„å»ºä¸Šä¸‹æ–‡æ–‡æœ¬
    context_text = "\n\n".join(contexts)

    # ä½¿ç”¨æ ‡å‡†çš„ RAG prompt æ¨¡æ¿
    prompt = f"""You are an assistant for question-answering tasks.
Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know.
Use two sentences maximum and keep the answer concise.
Question: {question}
Context: {context_text}
Answer:"""

    # è°ƒç”¨LLM
    messages = [LLMMessage(role=LLMRole.USER, content=prompt)]
    response = await llm_client.chat(messages, temperature=0.3)

    return response.content.strip()


async def main():
    parser = argparse.ArgumentParser(description='Stage2: ç­”æ¡ˆç”Ÿæˆ')
    parser.add_argument('--input', type=str,
                       default=str(config.DATA_DIR / "retrieval_results.jsonl"),
                       help='æ£€ç´¢ç»“æœè¾“å…¥è·¯å¾„ï¼ˆStage1 è¾“å‡ºï¼‰')
    parser.add_argument('--output', type=str,
                       default=str(config.DATA_DIR / "generated_answers.jsonl"),
                       help='ç”Ÿæˆç­”æ¡ˆè¾“å‡ºè·¯å¾„')
    parser.add_argument('--concurrency', type=int, default=5,
                       help='å¹¶å‘æ•°ï¼ˆé»˜è®¤ 5ï¼Œæ ¹æ® API é™æµè°ƒæ•´ï¼‰')
    parser.add_argument('--verbose', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—')

    args = parser.parse_args()

    # é…ç½®æ—¥å¿—
    if args.verbose:
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%H:%M:%S'
        )
    else:
        logging.basicConfig(level=logging.WARNING)

    print("=" * 60)
    print("ğŸš€ Stage2: ç­”æ¡ˆç”Ÿæˆ")
    print("=" * 60)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # 1. åŠ è½½æ£€ç´¢ç»“æœ
    input_path = Path(args.input)
    print(f"ğŸ“‚ åŠ è½½æ£€ç´¢ç»“æœ: {input_path}")

    try:
        retrieval_results = read_jsonl(input_path, RetrievalResult)
    except FileNotFoundError as e:
        print(f"[X] é”™è¯¯: {e}")
        print("   è¯·å…ˆè¿è¡Œ Stage1 (stage1_retrieve.py)")
        return
    except Exception as e:
        print(f"[X] åŠ è½½å¤±è´¥: {e}")
        return

    print(f"[OK] åŠ è½½äº† {len(retrieval_results)} ä¸ªæ£€ç´¢ç»“æœ")
    print()

    # 2. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
    print("ğŸ¤– åˆå§‹åŒ– LLM å®¢æˆ·ç«¯...")
    try:
        llm_client = await create_llm_client()
        print("[OK] åˆå§‹åŒ–å®Œæˆ\n")
    except Exception as e:
        print(f"[X] åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # 3. ä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆç­”æ¡ˆï¼ˆå¹¶å‘å¤„ç†ï¼‰
    print("ğŸ“ å¼€å§‹ç”Ÿæˆç­”æ¡ˆ...")
    print("=" * 60)
    print(f"å¹¶å‘æ•°: {args.concurrency}")
    print()

    try:
        # ğŸ†• ä½¿ç”¨ asyncio.Semaphore æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(args.concurrency)

        async def generate_with_semaphore(result, index):
            async with semaphore:
                return await generate_answer_with_metadata(
                    result=result,
                    llm_client=llm_client,
                    index=index,
                    total=len(retrieval_results),
                    verbose=args.verbose
                )

        # ğŸ†• åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [
            generate_with_semaphore(result, i)
            for i, result in enumerate(retrieval_results, 1)
        ]

        # ğŸ†• å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        generated_answers = await asyncio.gather(*tasks)

        print(f"\n{'='*60}")
        print(f"[OK] æ‰€æœ‰ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼ˆå…± {len(generated_answers)} ä¸ªï¼‰")
        print(f"{'='*60}\n")

    except Exception as e:
        print(f"\n[X] ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    # 4. éªŒè¯æ•°æ®
    print("âœ“ éªŒè¯ç”Ÿæˆçš„ç­”æ¡ˆ...")
    try:
        validate_generated_answers(generated_answers, retrieval_results)
    except ValueError as e:
        print(f"[X] éªŒè¯å¤±è´¥: {e}")
        return

    # 5. ä¿å­˜ç»“æœ
    output_path = Path(args.output)
    write_jsonl(generated_answers, output_path)

    print()
    print("=" * 60)
    print("âœ… Stage2 å®Œæˆ")
    print("=" * 60)
    print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
