"""
RAGAs æ¨¡å‹é€‚é…å™¨
å°†é¡¹ç›®çš„ LLM å’Œ Embedding é…ç½®é€‚é…åˆ° RAGAs è¯„ä¼°æ¡†æ¶
"""
from typing import Optional
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

from dataflow.core.config import get_settings


def create_ragas_llm(
    model: Optional[str] = None,
    temperature: float = 0.0,
    timeout: Optional[int] = None,
    verbose: bool = True
) -> ChatOpenAI:
    """
    åˆ›å»ºç”¨äº RAGAs è¯„ä¼°çš„ LLM å®¢æˆ·ç«¯

    ä½¿ç”¨é¡¹ç›®é…ç½®çš„ LLM APIï¼Œæ”¯æŒä¸­è½¬ API å’Œ OpenAI å®˜æ–¹ API

    Args:
        model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
        temperature: æ¸©åº¦å‚æ•°ï¼ˆé»˜è®¤ 0.0ï¼Œè¯„ä¼°æ—¶éœ€è¦ç¡®å®šæ€§ï¼‰
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
        verbose: æ˜¯å¦æ‰“å°é…ç½®ä¿¡æ¯

    Returns:
        LangChain ChatOpenAI å®ä¾‹

    Example:
        >>> from shared.ragas_adapters import create_ragas_llm, create_ragas_embeddings
        >>>
        >>> # åˆ›å»ºæ¨¡å‹å®ä¾‹
        >>> llm = create_ragas_llm()
        >>> embeddings = create_ragas_embeddings()
        >>>
        >>> # ä¼ é€’ç»™ RAGAs evaluate()
        >>> results = evaluate(
        ...     dataset,
        ...     metrics=[faithfulness, answer_relevancy, context_precision, context_recall],
        ...     llm=llm,
        ...     embeddings=embeddings
        ... )
    """
    settings = get_settings()

    # ä½¿ç”¨é¡¹ç›®é…ç½®
    model = model or settings.llm_model
    base_url = settings.llm_base_url
    api_key = settings.llm_api_key
    timeout = timeout or settings.llm_timeout

    if verbose:
        print("=" * 60)
        print("ğŸ¤– RAGAs LLM é…ç½®")
        print("=" * 60)
        print(f"  æ¨¡å‹:      {model}")
        print(f"  API åœ°å€:  {base_url or 'OpenAI å®˜æ–¹ API'}")
        print(f"  æ¸©åº¦:      {temperature}")
        print(f"  è¶…æ—¶:      {timeout}s")
        print("=" * 60)
        print()

    # åˆ›å»º LangChain ChatOpenAI å®¢æˆ·ç«¯
    # æ³¨æ„ï¼šlangchain-openai ä½¿ç”¨ä¸åŒçš„å‚æ•°å
    client_kwargs = {
        "model": model,
        "api_key": api_key,
        "temperature": temperature,
        "timeout": timeout,
        "max_retries": settings.llm_max_retries,
    }

    # å¦‚æœæœ‰è‡ªå®šä¹‰ base_urlï¼Œæ·»åŠ é…ç½®
    if base_url:
        client_kwargs["base_url"] = base_url

    llm = ChatOpenAI(**client_kwargs)

    return llm


def create_ragas_embeddings(
    model: Optional[str] = None,
    dimensions: Optional[int] = None,
    verbose: bool = True
) -> OpenAIEmbeddings:
    """
    åˆ›å»ºç”¨äº RAGAs è¯„ä¼°çš„ Embedding å®¢æˆ·ç«¯

    ä½¿ç”¨é¡¹ç›®é…ç½®çš„ Embedding APIï¼Œæ”¯æŒä¸­è½¬ API å’Œ OpenAI å®˜æ–¹ API

    Args:
        model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä»é…ç½®è¯»å–ï¼‰
        dimensions: å‘é‡ç»´åº¦ï¼ˆå¯é€‰ï¼ŒæŸäº›æ¨¡å‹æ”¯æŒè‡ªå®šä¹‰ç»´åº¦ï¼‰
        verbose: æ˜¯å¦æ‰“å°é…ç½®ä¿¡æ¯

    Returns:
        LangChain OpenAIEmbeddings å®ä¾‹

    Example:
        >>> embeddings = create_ragas_embeddings()
        >>> # ä¼ é€’ç»™ RAGAs evaluate()
        >>> results = evaluate(dataset, metrics=[...], embeddings=embeddings)
    """
    settings = get_settings()

    # ä½¿ç”¨é¡¹ç›®é…ç½®
    model = model or settings.embedding_model_name
    dimensions = dimensions or settings.embedding_dimensions
    base_url = settings.embedding_base_url or settings.llm_base_url
    api_key = settings.embedding_api_key or settings.llm_api_key

    if verbose:
        print("=" * 60)
        print("ğŸ¯ RAGAs Embeddings é…ç½®")
        print("=" * 60)
        print(f"  æ¨¡å‹:      {model}")
        print(f"  API åœ°å€:  {base_url or 'OpenAI å®˜æ–¹ API'}")
        if dimensions:
            print(f"  å‘é‡ç»´åº¦:  {dimensions}")
        else:
            # æ ¹æ®æ¨¡å‹æç¤ºé»˜è®¤ç»´åº¦
            default_dims = {
                "Qwen/Qwen3-Embedding-0.6B": 1536,
                "text-embedding-3-large": 3072,
                "text-embedding-ada-002": 1536,
            }
            print(f"  å‘é‡ç»´åº¦:  {default_dims.get(model, 'æ¨¡å‹é»˜è®¤')}")
        print("=" * 60)
        print()

    # åˆ›å»º LangChain OpenAIEmbeddings å®¢æˆ·ç«¯
    client_kwargs = {
        "model": model,
        "api_key": api_key,
    }

    # å¦‚æœæœ‰è‡ªå®šä¹‰ base_urlï¼Œæ·»åŠ é…ç½®
    if base_url:
        client_kwargs["base_url"] = base_url

    # å¦‚æœæŒ‡å®šäº†ç»´åº¦ï¼Œæ·»åŠ é…ç½®ï¼ˆä»… text-embedding-3-* ç³»åˆ—æ”¯æŒï¼‰
    if dimensions:
        client_kwargs["dimensions"] = dimensions

    embeddings = OpenAIEmbeddings(**client_kwargs)

    return embeddings


def print_model_config():
    """
    æ‰“å°å½“å‰é¡¹ç›®çš„æ¨¡å‹é…ç½®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    """
    settings = get_settings()

    print("\n" + "=" * 60)
    print("ğŸ“‹ å½“å‰é¡¹ç›®æ¨¡å‹é…ç½®")
    print("=" * 60)
    print("\n[LLM é…ç½®]")
    print(f"  æ¨¡å‹:        {settings.llm_model}")
    print(f"  API Key:     {settings.llm_api_key[:20]}..." if settings.llm_api_key else "  API Key:     æœªé…ç½®")
    print(f"  Base URL:    {settings.llm_base_url or 'OpenAI å®˜æ–¹ API'}")
    print(f"  è¶…æ—¶:        {settings.llm_timeout}s")
    print(f"  æœ€å¤§é‡è¯•:    {settings.llm_max_retries}")

    print("\n[Embedding é…ç½®]")
    print(f"  æ¨¡å‹:        {settings.embedding_model_name}")
    print(f"  API Key:     {settings.embedding_api_key[:20] + '...' if settings.embedding_api_key else '(ä½¿ç”¨ LLM API Key)'}")
    print(f"  Base URL:    {settings.embedding_base_url or settings.llm_base_url or 'OpenAI å®˜æ–¹ API'}")
    if settings.embedding_dimensions:
        print(f"  å‘é‡ç»´åº¦:    {settings.embedding_dimensions}")

    print("=" * 60 + "\n")


if __name__ == "__main__":
    # æµ‹è¯•è„šæœ¬
    print_model_config()

    # åˆ›å»ºå®ä¾‹
    llm = create_ragas_llm()
    embeddings = create_ragas_embeddings()

    print("âœ… æ¨¡å‹å®ä¾‹åˆ›å»ºæˆåŠŸï¼")
