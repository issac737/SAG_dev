#!/usr/bin/env python3
"""
Query Recall æµ‹è¯•è„šæœ¬

ç®€åŒ–ç‰ˆï¼šåªè¾“å‡ºé—®é¢˜å’Œå¯¹åº”ç­”æ¡ˆ
"""

import json
import sys
import asyncio
from pathlib import Path
from typing import List, Dict, Any
import numpy as np

# Add project root to path
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))

# Load environment variables
from dotenv import load_dotenv
env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)
    print(f"[INFO] Loaded environment variables: {env_path}")
else:
    print(f"[WARN] .env file not found: {env_path}")

# Import ES and Embedding clients
from dataflow.core.storage.elasticsearch import ElasticsearchClient, ESConfig
from dataflow.core.ai.embedding import EmbeddingClient
from dataflow.core.storage.repositories.source_chunk_repository import SourceChunkRepository

# Import recall metrics
from evaluation.hotpotqa_evaluation.scripts.recall_metrics import RecallCalculator, RecallResult


class SimpleVectorSearcher:
    """ç®€å•çš„å‘é‡æœç´¢å™¨ï¼Œç”¨äºæµ‹è¯•å¬å›æ•ˆæœ"""

    def __init__(self):
        """åˆå§‹åŒ–æœç´¢å™¨"""
        # åˆå§‹åŒ– ES å®¢æˆ·ç«¯
        es_client = ElasticsearchClient(config=ESConfig.from_env())

        # åˆå§‹åŒ– SourceChunkRepository
        self.chunk_repo = SourceChunkRepository(es_client=es_client)

        # åˆå§‹åŒ– Embedding å®¢æˆ·ç«¯
        self.embedding_client = EmbeddingClient()

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            vec1: å‘é‡1 (query_vector)
            vec2: å‘é‡2 (content_vector)

        Returns:
            ä½™å¼¦ç›¸ä¼¼åº¦ï¼ŒèŒƒå›´ [-1, 1]
            - 1.0: å®Œå…¨åŒå‘ï¼ˆç›¸ä¼¼ï¼‰
            - 0.0: æ­£äº¤ï¼ˆæ— å…³ï¼‰
            - -1.0: å®Œå…¨åå‘ï¼ˆç›¸åï¼‰
        """
        if not vec1 or not vec2:
            return 0.0

        try:
            v1 = np.array(vec1, dtype=np.float32)
            v2 = np.array(vec2, dtype=np.float32)

            if len(v1) != len(v2):
                print(f"      [WARN] å‘é‡é•¿åº¦ä¸ä¸€è‡´: {len(v1)} vs {len(v2)}")
                return 0.0

            # è®¡ç®—ç‚¹ç§¯
            dot_product = np.dot(v1, v2)
            # è®¡ç®—å‘é‡çš„æ¨¡
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            # ä½™å¼¦ç›¸ä¼¼åº¦ = ç‚¹ç§¯ / (æ¨¡1 Ã— æ¨¡2)
            similarity = dot_product / (norm1 * norm2)
            # âœ… ä¿ç•™å®Œæ•´èŒƒå›´ [-1, 1]ï¼Œåªé™åˆ¶æµ®ç‚¹è¯¯å·®
            return float(np.clip(similarity, -1.0, 1.0))

        except Exception as e:
            print(f"      [ERROR] ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0

    async def check_es_data(self):
        """æ£€æŸ¥ESä¸­çš„æ•°æ®æƒ…å†µ"""
        es_client = self.chunk_repo.es_client.client

        # 1. è·å–æ€»æ•°
        count_result = await es_client.count(index="source_chunks")
        total_count = count_result['count']
        print(f"      [DEBUG] ESä¸­source_chunksæ€»æ•°: {total_count}")

        # 2. èšåˆæŸ¥è¯¢ï¼Œè·å–æ‰€æœ‰ä¸åŒçš„source_config_id
        agg_query = {
            "size": 0,
            "aggs": {
                "source_configs": {
                    "terms": {
                        "field": "source_config_id",
                        "size": 100
                    }
                }
            }
        }

        response = await es_client.search(index="source_chunks", body=agg_query)

        if 'aggregations' in response and 'source_configs' in response['aggregations']:
            buckets = response['aggregations']['source_configs']['buckets']
            print(f"      [DEBUG] ESä¸­çš„source_config_idåˆ—è¡¨:")
            for bucket in buckets:
                print(f"         - {bucket['key']}: {bucket['doc_count']} ä¸ªæ–‡æ¡£")

        # 3. å¦‚æœæ€»æ•°ä¸º0ï¼Œç»™å‡ºæç¤º
        if total_count == 0:
            print(f"      [WARNING] ESä¸­æ²¡æœ‰ä»»ä½•æ•°æ®ï¼è¯·å…ˆå¯¼å…¥æ•°æ®ã€‚")

        return total_count

    async def search(
        self,
        query: str,
        source_config_id: str,
        es_retrieve_k: int = 50
    ) -> List[Dict[str, Any]]:
        """
        å‘é‡æœç´¢æ®µè½ï¼ˆä» ES å¬å›æŒ‡å®šæ•°é‡ï¼‰

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            source_config_id: æ•°æ®æºé…ç½®ID
            es_retrieve_k: ä» ES å¬å›çš„æ®µè½æ•°é‡ï¼ˆåç»­ä¼šå»é‡å’Œç­›é€‰ï¼‰

        Returns:
            æ®µè½åˆ—è¡¨ï¼Œæ¯ä¸ªæ®µè½åŒ…å« chunk_id, heading, content, score, cosine_similarity ç­‰å­—æ®µ
        """
        # 1. ç”Ÿæˆ query çš„ embedding
        query_vector = await self.embedding_client.generate(query)

        # 2. ä½¿ç”¨ SourceChunkRepository æœç´¢ï¼ˆå¬å› es_retrieve_k ä¸ªæ®µè½ï¼‰
        print(f"      [DEBUG] source_config_id: {source_config_id}, es_retrieve_k: {es_retrieve_k}")
        print(f"      [DEBUG] query_vectorç»´åº¦: {len(query_vector)}")

        results = await self.chunk_repo.search_similar_by_content(
            query_vector=query_vector,
            k=es_retrieve_k,
            source_config_id=source_config_id
        )

        print(f"      [DEBUG] æœç´¢è¿”å›ç»“æœæ•°: {len(results)}")
        if results:
            print(f"      [DEBUG] ç¬¬ä¸€ä¸ªç»“æœå­—æ®µ: {list(results[0].keys())}")

        # 3. è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼ï¼Œå¹¶æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        sections = []
        for hit in results:
            chunk_id = hit.get('id', '')
            es_score = hit.get('_score', 0.0)

            # ğŸ”‘ è·å–æ®µè½çš„ content_vectorï¼ˆä» ES ç»“æœä¸­ï¼‰
            content_vector = hit.get('content_vector', None)

            # ğŸ”‘ æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            if content_vector:
                manual_cosine_similarity = self._cosine_similarity(query_vector, content_vector)
            else:
                manual_cosine_similarity = None
                print(f"      [WARN] chunk {chunk_id[:8]}... æ²¡æœ‰ content_vector")

            sections.append({
                'chunk_id': chunk_id,
                'heading': hit.get('heading', ''),
                'content': hit.get('content', ''),
                'score': es_score,  # ES è¿”å›çš„ _score
                'cosine_similarity': manual_cosine_similarity,  # æ‰‹åŠ¨è®¡ç®—çš„ä½™å¼¦ç›¸ä¼¼åº¦
                'weight': es_score  # ä¿æŒå…¼å®¹æ€§
            })

        return sections

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self.chunk_repo, 'es_client') and hasattr(self.chunk_repo.es_client, 'client'):
            await self.chunk_repo.es_client.client.close()


def load_corpus(corpus_path: Path) -> Dict[str, Dict[str, str]]:
    """åŠ è½½è¯­æ–™åº“"""
    print(f"[INFO] åŠ è½½è¯­æ–™åº“: {corpus_path}")
    corpus_dict = {}
    with open(corpus_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if line.strip():
                try:
                    chunk = json.loads(line)
                    chunk_id = chunk['id']

                    # å¤„ç†åˆå¹¶çš„ID
                    if "//" in chunk_id:
                        original_ids = chunk_id.split("//")
                        for original_id in original_ids:
                            corpus_dict[original_id] = {
                                'title': chunk.get('title', ''),
                                'text': chunk.get('text', '')
                            }
                    else:
                        corpus_dict[chunk_id] = {
                            'title': chunk.get('title', ''),
                            'text': chunk.get('text', '')
                        }
                except json.JSONDecodeError as e:
                    print(f"[WARN] ç¬¬ {line_num} è¡ŒJSONè§£æå¤±è´¥: {e}")

    print(f"[INFO] è¯­æ–™åº“åŠ è½½å®Œæˆï¼Œå…± {len(corpus_dict)} ä¸ªæ®µè½")
    return corpus_dict


def load_questions(questions_path: Path) -> List[Dict[str, Any]]:
    """åŠ è½½é—®é¢˜åˆ—è¡¨"""
    print(f"[INFO] åŠ è½½é—®é¢˜åˆ—è¡¨: {questions_path}")
    questions = []
    with open(questions_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if line.strip():
                try:
                    q = json.loads(line)
                    questions.append(q)
                except json.JSONDecodeError as e:
                    print(f"[WARN] ç¬¬ {line_num} è¡ŒJSONè§£æå¤±è´¥: {e}")

    print(f"[INFO] é—®é¢˜åŠ è½½å®Œæˆï¼Œå…± {len(questions)} ä¸ªé—®é¢˜")
    return questions


def find_latest_source_dir(base_dir: Path) -> Path:
    """
    æŸ¥æ‰¾æœ€æ–°çš„æºæ•°æ®æ–‡ä»¶å¤¹

    æ–‡ä»¶å¤¹æ ¼å¼ï¼šYYYYMMDD_HHMMSS
    """
    print(f"[INFO] æŸ¥æ‰¾æœ€æ–°çš„æºæ•°æ®æ–‡ä»¶å¤¹: {base_dir}")

    if not base_dir.exists():
        print(f"[ERROR] æºæ•°æ®ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        sys.exit(1)

    # è·å–æ‰€æœ‰æ–‡ä»¶å¤¹
    dirs = [d for d in base_dir.iterdir() if d.is_dir()]

    if not dirs:
        print(f"[ERROR] æºæ•°æ®ç›®å½•ä¸‹æ²¡æœ‰æ–‡ä»¶å¤¹: {base_dir}")
        sys.exit(1)

    # æŒ‰åç§°æ’åºï¼ˆYYYYMMDD_HHMMSS æ ¼å¼å¯ä»¥ç›´æ¥å­—ç¬¦ä¸²æ’åºï¼‰
    dirs.sort(key=lambda x: x.name, reverse=True)

    latest_dir = dirs[0]
    print(f"[INFO] æ‰¾åˆ°æœ€æ–°æ–‡ä»¶å¤¹: {latest_dir.name}")

    return latest_dir


async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='Query Recall æµ‹è¯•è„šæœ¬')
    parser.add_argument('--source-config-id', type=str, required=False, default=None,
                       help='æ•°æ®æºé…ç½®ID (å¦‚æœä¸æŒ‡å®šï¼Œå°†ä»æºç›®å½•çš„ process_result.json ä¸­è¯»å–)')
    parser.add_argument('--input', type=Path, required=False, default=None,
                       help='é—®é¢˜åˆ—è¡¨æ–‡ä»¶è·¯å¾„ (JSONLæ ¼å¼)ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨æœ€æ–°æºæ•°æ®ç›®å½•ä¸‹çš„ oracle.jsonl')
    parser.add_argument('--corpus', type=Path, required=False, default=None,
                       help='è¯­æ–™åº“æ–‡ä»¶è·¯å¾„ (JSONLæ ¼å¼)ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†ä½¿ç”¨æœ€æ–°æºæ•°æ®ç›®å½•ä¸‹çš„ corpus.jsonl')
    parser.add_argument('--source-dir', type=Path, required=False, default=None,
                       help='æºæ•°æ®ç›®å½•è·¯å¾„ (åŒ…å« oracle.jsonl)ã€‚å¦‚æœæœªæŒ‡å®šï¼Œå°†è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°ç›®å½•')
    parser.add_argument('--max-questions', type=int, default=None,
                       help='æœ€å¤§å¤„ç†é—®é¢˜æ•°ï¼ˆé»˜è®¤: å…¨éƒ¨ï¼‰')
    parser.add_argument('--es-retrieve-k', type=int, default=50,
                       help='ä» ES å¬å›çš„æ®µè½æ•°é‡ï¼ˆé»˜è®¤: 50ï¼‰ï¼Œå¬å›åä¼šå»é‡å¹¶ç­›é€‰ top-k')
    parser.add_argument('--top-k', type=int, default=5,
                       help='å»é‡åé€‰æ‹©çš„æ®µè½æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰')
    parser.add_argument('--output', type=Path, required=False, default=None,
                       help='ä¿å­˜æœç´¢ç»“æœçš„æ–‡ä»¶è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼‰')

    args = parser.parse_args()

    # è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ä½œä¸ºåŸºå‡†
    script_dir = Path(__file__).parent  # scripts/
    eval_base_dir = script_dir.parent   # evaluation/hotpotqa_evaluation/

    # ç¡®å®šæºæ•°æ®ç›®å½•
    if args.source_dir:
        # ä½¿ç”¨æŒ‡å®šçš„æºæ•°æ®ç›®å½•
        source_dir = Path(args.source_dir)
        print(f"[INFO] ä½¿ç”¨æŒ‡å®šçš„æºæ•°æ®ç›®å½•: {source_dir}")
    else:
        # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„æºæ•°æ®ç›®å½•
        base_dir = eval_base_dir / "data" / "source"
        source_dir = find_latest_source_dir(base_dir)
        print(f"[INFO] è‡ªåŠ¨é€‰æ‹©æœ€æ–°æºæ•°æ®ç›®å½•: {source_dir}")

    # ç¡®å®š source_config_id
    if args.source_config_id:
        source_config_id = args.source_config_id
        print(f"[INFO] ä½¿ç”¨æŒ‡å®šçš„ source_config_id: {source_config_id}")
    else:
        # ä» process_result.json ä¸­è¯»å–
        process_result_file = source_dir / "process_result.json"
        if process_result_file.exists():
            with open(process_result_file, 'r', encoding='utf-8') as f:
                process_result = json.load(f)
                source_config_id = process_result.get('source_config_id')
                if source_config_id:
                    print(f"[INFO] ä» process_result.json è¯»å– source_config_id: {source_config_id}")
                else:
                    print(f"[ERROR] process_result.json ä¸­æ²¡æœ‰ source_config_id å­—æ®µ")
                    sys.exit(1)
        else:
            print(f"[ERROR] æœªæ‰¾åˆ° process_result.json: {process_result_file}")
            print(f"[ERROR] è¯·ä½¿ç”¨ --source-config-id å‚æ•°æ‰‹åŠ¨æŒ‡å®š")
            sys.exit(1)

    # ç¡®å®šè¾“å…¥æ–‡ä»¶
    if args.input:
        input_file = Path(args.input)
    else:
        input_file = source_dir / "oracle.jsonl"
        if not input_file.exists():
            print(f"[ERROR] æºæ•°æ®ç›®å½•ä¸‹æ‰¾ä¸åˆ° oracle.jsonl: {input_file}")
            sys.exit(1)
        print(f"[INFO] ä½¿ç”¨æºæ•°æ®ç›®å½•ä¸‹çš„ oracle.jsonl: {input_file}")

    # ç¡®å®šè¯­æ–™åº“æ–‡ä»¶
    if args.corpus:
        corpus_file = Path(args.corpus)
    else:
        corpus_file = source_dir / "corpus.jsonl"
        if not corpus_file.exists():
            print(f"[ERROR] æºæ•°æ®ç›®å½•ä¸‹æ‰¾ä¸åˆ° corpus.jsonl: {corpus_file}")
            sys.exit(1)
        print(f"[INFO] ä½¿ç”¨æºæ•°æ®ç›®å½•ä¸‹çš„ corpus.jsonl: {corpus_file}")

    # éªŒè¯æ–‡ä»¶
    if not input_file.exists():
        print(f"[ERROR] é—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        sys.exit(1)

    if not corpus_file.exists():
        print(f"[ERROR] è¯­æ–™åº“æ–‡ä»¶ä¸å­˜åœ¨: {corpus_file}")
        sys.exit(1)

    # åŠ è½½æ•°æ®
    corpus_dict = load_corpus(corpus_file)
    questions = load_questions(input_file)

    # é™åˆ¶é—®é¢˜æ•°
    if args.max_questions:
        questions = questions[:args.max_questions]
        print(f"[INFO] é™åˆ¶å¤„ç†å‰ {args.max_questions} ä¸ªé—®é¢˜")

    # è¾“å‡ºé—®é¢˜å’Œç­”æ¡ˆ
    print("\n" + "="*80)
    print("å¼€å§‹å¤„ç†é—®é¢˜å¹¶æ‰§è¡Œå‘é‡æœç´¢")
    print("="*80)

    # åˆå§‹åŒ–æœç´¢å™¨
    print("\n[INFO] åˆå§‹åŒ–å‘é‡æœç´¢å™¨...")
    searcher = SimpleVectorSearcher()
    print("[INFO] æœç´¢å™¨åˆå§‹åŒ–å®Œæˆ")

    # æ£€æŸ¥ESæ•°æ®
    print("\n[INFO] æ£€æŸ¥ESä¸­çš„æ•°æ®...")
    await searcher.check_es_data()
    print()

    # ç”¨äºä¿å­˜æ‰€æœ‰ç»“æœ
    all_results = []
    total_recall = 0.0
    total_questions_with_oracle = 0

    for i, q in enumerate(questions, 1):
        question_id = q.get('id', 'unknown')
        question = q.get('question', '')
        answer = q.get('answer', '')
        oracle_chunk_ids = q.get('oracle_chunk_ids', [])

        print(f"\n{'='*80}")
        print(f"[{i}/{len(questions)}] ID: {question_id}")
        print(f"â“ é—®é¢˜: {question}")
        print(f"âœ… ç­”æ¡ˆ: {answer}")
        print(f"ğŸ“Œ æ ‡å‡†ç­”æ¡ˆæ®µè½æ•°: {len(oracle_chunk_ids)}")

        # æ˜¾ç¤ºæ ‡å‡†ç­”æ¡ˆæ®µè½çš„è¯¦ç»†å†…å®¹
        if oracle_chunk_ids and corpus_dict:
            print(f"\n   æ ‡å‡†ç­”æ¡ˆæ®µè½:")
            for j, chunk_id in enumerate(oracle_chunk_ids, 1):
                if chunk_id in corpus_dict:
                    chunk = corpus_dict[chunk_id]
                    title = chunk.get('title', 'N/A')
                    text = chunk.get('text', '')[:200]  # é™åˆ¶é¢„è§ˆé•¿åº¦

                    print(f"   [{j}] {title}")
                    print(f"       {text}...")
                else:
                    print(f"   [{j}] chunk_id: {chunk_id} (æœªæ‰¾åˆ°å¯¹åº”æ®µè½)")

        # æ‰§è¡Œå‘é‡æœç´¢
        print(f"\nğŸ” æ‰§è¡Œå‘é‡æœç´¢ (ESå¬å›: {args.es_retrieve_k}, å»é‡åé€‰æ‹©: top-{args.top_k})...")
        try:
            import time
            start_time = time.time()
            search_results = await searcher.search(
                query=question,
                source_config_id=source_config_id,  # ä½¿ç”¨ä»æ–‡ä»¶è¯»å–çš„ source_config_id
                es_retrieve_k=args.es_retrieve_k  # ä» ES å¬å›çš„æ®µè½æ•°é‡
            )
            search_time = time.time() - start_time

            print(f"   ES å¬å›å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} ä¸ªæ®µè½ (è€—æ—¶: {search_time:.3f}ç§’)")

            # å‡†å¤‡å¬å›ç‡è®¡ç®—æ‰€éœ€çš„æ•°æ®
            # 1. å‡†å¤‡ oracle chunksï¼ˆæ ‡å‡†ç­”æ¡ˆæ®µè½ï¼‰
            oracle_chunks_for_calc = []
            for chunk_id in oracle_chunk_ids:
                if chunk_id in corpus_dict:
                    chunk = corpus_dict[chunk_id]
                    oracle_chunks_for_calc.append({
                        'chunk_id': chunk_id,
                        'title': chunk.get('title', ''),
                        'text': chunk.get('text', '')
                    })

            # 2. å¯¹æ£€ç´¢ç»“æœå»é‡ï¼ˆåŸºäºå†…å®¹ï¼‰
            seen_contents = set()
            deduped_results = []
            duplicates_count = 0

            for section in search_results:
                content = section.get('content', '')
                # ä½¿ç”¨å†…å®¹çš„å½’ä¸€åŒ–ç‰ˆæœ¬ä½œä¸ºå»é‡é”®
                content_key = ' '.join(content.strip().split())

                if content_key not in seen_contents:
                    seen_contents.add(content_key)
                    deduped_results.append(section)
                else:
                    duplicates_count += 1

            if duplicates_count > 0:
                print(f"   âš ï¸  æ£€æµ‹åˆ° {duplicates_count} ä¸ªé‡å¤æ®µè½ï¼Œå·²è‡ªåŠ¨è¿‡æ»¤")

            # 3. ä»å»é‡ç»“æœä¸­é€‰æ‹© top-k ä¸ªæ®µè½
            final_results = deduped_results[:args.top_k]
            if len(deduped_results) > args.top_k:
                print(f"   âœ‚ï¸  å»é‡åä» {len(deduped_results)} ä¸ªæ®µè½ä¸­é€‰æ‹© top-{args.top_k}")

            # 4. ä½¿ç”¨ RecallCalculator è®¡ç®—å¬å›ç‡ï¼ˆï¿½ï¿½ï¿½ç”¨æœ€ç»ˆç­›é€‰åçš„ç»“æœï¼‰
            recall_result = RecallCalculator.calculate(
                question_id=question_id,
                oracle_chunks=oracle_chunks_for_calc,
                retrieved_sections=final_results,
                verbose=False
            )

            # æ˜¾ç¤ºæœç´¢ç»“æœï¼ˆæœ€ç»ˆç­›é€‰åï¼‰
            print(f"\n   æ£€ç´¢ç»“æœ (æœ€ç»ˆ: {len(final_results)} ä¸ªæ®µè½):")
            for idx, section in enumerate(final_results, 1):
                chunk_id = section.get('chunk_id', '')
                heading = section.get('heading', 'N/A')
                content = section.get('content', '')[:200]  # é™åˆ¶é•¿åº¦
                es_score = section.get('score', 0.0)
                manual_cosine = section.get('cosine_similarity', None)

                # æ£€æŸ¥æ˜¯å¦å‘½ä¸­æ ‡å‡†ç­”æ¡ˆï¼ˆåŸºäºæ–‡æœ¬åŒ¹é…ï¼‰
                is_hit = ''
                for detail in recall_result.recalled_details:
                    if detail.get('retrieved_chunk_id') == chunk_id:
                        is_hit = 'âœ… å‘½ä¸­'
                        break

                print(f"   [{idx}] {heading} {is_hit}")
                print(f"       ES Score: {es_score:.4f}")
                if manual_cosine is not None:
                    print(f"       Manual Cosine Similarity: {manual_cosine:.4f}")
                    # ğŸ”‘ æ˜¾ç¤ºä¸¤è€…çš„å·®å¼‚
                    diff = abs(es_score - manual_cosine)
                    print(f"       å·®å¼‚: {diff:.4f} (ES Score - Manual Cosine)")
                else:
                    print(f"       Manual Cosine Similarity: N/A (æ— å‘é‡)")
                print(f"       å†…å®¹: {content}...")

            # æ˜¾ç¤ºå¬å›ç»Ÿè®¡
            print(f"\n   ğŸ“Š å¬å›ç»Ÿè®¡: {recall_result.recalled}/{recall_result.total_oracle} ({recall_result.recall:.2%})")
            if recall_result.recalled_details:
                print(f"   åŒ¹é…è¯¦æƒ…:")
                for detail in recall_result.recalled_details[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"      - {detail['oracle_title']} <-> {detail['retrieved_heading']}")
                if len(recall_result.recalled_details) > 5:
                    print(f"      ... è¿˜æœ‰ {len(recall_result.recalled_details) - 5} ä¸ªåŒ¹é…")

            # ç´¯è®¡ç»Ÿè®¡
            if recall_result.total_oracle > 0:
                total_recall += recall_result.recall
                total_questions_with_oracle += 1

            # ä¿å­˜ç»“æœï¼ˆä½¿ç”¨æœ€ç»ˆç­›é€‰åçš„ç»“æœï¼‰
            result = {
                'question_id': question_id,
                'question': question,
                'answer': answer,
                'oracle_chunk_ids': oracle_chunk_ids,
                'es_retrieved': len(search_results),
                'duplicates_filtered': duplicates_count,
                'deduped_count': len(deduped_results),
                'final_count': len(final_results),
                'search_results': [
                    {
                        'chunk_id': s.get('chunk_id', ''),
                        'heading': s.get('heading', ''),
                        'content': s.get('content', ''),
                        'score': s.get('score', 0.0),
                        'cosine_similarity': s.get('cosine_similarity', None)  # ğŸ”‘ æ·»åŠ æ‰‹åŠ¨è®¡ç®—çš„ä½™å¼¦ç›¸ä¼¼åº¦
                    }
                    for s in final_results
                ],
                'recall': recall_result.recall,
                'total_oracle': recall_result.total_oracle,
                'recalled': recall_result.recalled,
                'retrieved': recall_result.retrieved,
                'recalled_details': recall_result.recalled_details,
                'search_time': search_time
            }
            all_results.append(result)

        except Exception as e:
            print(f"   âš ï¸ æœç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

            # ä¿å­˜å¤±è´¥è®°å½•
            all_results.append({
                'question_id': question_id,
                'question': question,
                'answer': answer,
                'oracle_chunk_ids': oracle_chunk_ids,
                'error': str(e),
                'search_results': []
            })

    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    print("\n" + "="*80)
    print("ğŸ“Š æ€»ä½“ç»Ÿè®¡")
    print("="*80)
    print(f"æ€»é—®é¢˜æ•°: {len(questions)}")

    if total_questions_with_oracle > 0:
        avg_recall = total_recall / total_questions_with_oracle
        print(f"å¹³å‡å¬å›ç‡: {avg_recall:.2%}")
        print(f"  è®¡ç®—æ–¹å¼: å®å¹³å‡ (Macro Average)")
        print(f"  å…¬å¼: Î£(æ¯ä¸ªé—®é¢˜çš„å¬å›ç‡) / é—®é¢˜æ€»æ•°")
        print(f"  è¯´æ˜: æ¯ä¸ªé—®é¢˜æƒé‡ç›¸åŒï¼Œä¸è®ºå…¶æ ‡å‡†ç­”æ¡ˆæ•°é‡")

        # ç»Ÿè®¡ä¸åŒå¬å›æƒ…å†µçš„é—®é¢˜æ•°
        perfect_recall = 0  # å®Œç¾å¬å›ï¼ˆ100%ï¼‰
        partial_recall = 0  # éƒ¨åˆ†å¬å›ï¼ˆ0% < recall < 100%ï¼‰
        zero_recall = 0     # é›¶å¬å›ï¼ˆ0%ï¼‰

        # ç»Ÿè®¡éƒ¨åˆ†å¬å›çš„è¯¦ç»†æƒ…å†µ
        from collections import defaultdict
        partial_recall_details = defaultdict(int)  # {å¬å›æ•°é‡: é—®é¢˜æ•°}

        # ç»Ÿè®¡æ€»çš„æ ‡å‡†ç­”æ¡ˆæ•°å’Œå¬å›æ•°ï¼ˆç”¨äºè®¡ç®—å¾®å¹³å‡ï¼‰
        total_oracle_chunks = 0
        total_recalled_chunks = 0

        for result in all_results:
            if 'error' in result:
                continue

            recall = result.get('recall', 0.0)
            recalled_count = result.get('recalled', 0)
            total_oracle_count = result.get('total_oracle', 0)

            total_oracle_chunks += total_oracle_count
            total_recalled_chunks += recalled_count

            if recall >= 1.0:
                perfect_recall += 1
            elif recall > 0.0:
                partial_recall += 1
                # è®°å½•éƒ¨åˆ†å¬å›çš„è¯¦ç»†æƒ…å†µ
                partial_recall_details[recalled_count] += 1
            else:
                zero_recall += 1



        print(f"\nå¬å›æƒ…å†µåˆ†å¸ƒ:")
        print(f"  âœ… å®Œç¾å¬å› (100%): {perfect_recall} ä¸ªé—®é¢˜ ({perfect_recall/total_questions_with_oracle:.2%})")
        print(f"  ğŸ”¶ éƒ¨åˆ†å¬å› (1%-99%): {partial_recall} ä¸ªé—®é¢˜ ({partial_recall/total_questions_with_oracle:.2%})")

        # æ˜¾ç¤ºéƒ¨åˆ†å¬å›çš„è¯¦ç»†åˆ†å¸ƒ
        if partial_recall_details:
            print(f"     éƒ¨åˆ†å¬å›è¯¦æƒ…:")
            for recalled_count in sorted(partial_recall_details.keys()):
                count = partial_recall_details[recalled_count]
                print(f"       - å¬å› {recalled_count} ä¸ªç­”æ¡ˆ: {count} ä¸ªé—®é¢˜")

        print(f"  âŒ é›¶å¬å› (0%): {zero_recall} ä¸ªé—®é¢˜ ({zero_recall/total_questions_with_oracle:.2%})")

    print(f"\næœ‰æ ‡å‡†ç­”æ¡ˆçš„é—®é¢˜æ•°: {total_questions_with_oracle}")
    print("="*80)

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    if args.output:
        print(f"\n[INFO] ä¿å­˜ç»“æœåˆ°: {args.output}")
        args.output.parent.mkdir(parents=True, exist_ok=True)
        with open(args.output, 'w', encoding='utf-8') as f:
            for result in all_results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        print(f"[INFO] ç»“æœå·²ä¿å­˜")

    # æ¸…ç†èµ„æº
    print("\n[INFO] æ¸…ç†æœç´¢å™¨èµ„æº...")
    await searcher.cleanup()


if __name__ == "__main__":
    asyncio.run(main())

