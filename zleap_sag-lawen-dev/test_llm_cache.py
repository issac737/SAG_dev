"""
LLM ç¼“å­˜å¿«é€Ÿæµ‹è¯•

åœ¨ VSCode ä¸­å³é”® -> "åœ¨ç»ˆç«¯ä¸­è¿è¡Œ Python æ–‡ä»¶" æˆ–æŒ‰ Ctrl+F5
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from dataflow.core.ai.factory import create_llm_client
from dataflow.core.ai.models import LLMMessage, LLMRole
from dataflow.core.cache import clear_llm_cache
from dataflow.core.config import get_settings
from dataflow.utils import get_logger

logger = get_logger(__name__)


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""

    # æ˜¾ç¤ºå½“å‰é…ç½®
    settings = get_settings()
    print("\n" + "=" * 70)
    print("ğŸ“‹ LLM ç¼“å­˜é…ç½®")
    print("=" * 70)
    print(f"âœ… ç¼“å­˜å¯ç”¨: {settings.llm_cache_enabled}")
    print(f"ğŸ”‘ é”®å‰ç¼€: {settings.llm_cache_prefix}")
    print(f"â° TTL: {settings.cache_llm_ttl} ç§’ ({settings.cache_llm_ttl // 86400} å¤©)")
    print(f"ğŸ¤– æ¨¡å‹: {settings.llm_model}")
    print(f"ğŸŒ API åœ°å€: {settings.llm_base_url or 'OpenAI å®˜æ–¹'}")
    print("=" * 70 + "\n")

    # åˆ›å»º LLM å®¢æˆ·ï¿½ï¿½
    print("æ­£åœ¨åˆ›å»º LLM å®¢æˆ·ç«¯...")
    try:
        client = await create_llm_client()
        print("âœ… LLM å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ\n")
    except Exception as e:
        print(f"âŒ åˆ›å»º LLM å®¢æˆ·ç«¯å¤±è´¥: {e}")
        print("\nè¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„é…ç½®:")
        print("  - llm_api_key")
        print("  - llm_model")
        print("  - llm_base_url")
        return

    # å‡†å¤‡æµ‹è¯•æ¶ˆæ¯
    messages = [
        LLMMessage(role=LLMRole.USER, content="è¯·ç”¨ä¸€å¥è¯ä»‹ç» Python ç¼–ç¨‹è¯­è¨€")
    ]

    print("=" * 70)
    print("ğŸš€ ç¬¬ä¸€æ¬¡è°ƒç”¨ - åº”è¯¥è°ƒç”¨ LLM APIï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰")
    print("=" * 70)

    try:
        response1 = await client.chat(messages=messages)
        print(f"âœ… å“åº”æˆåŠŸ")
        print(f"ğŸ“ å†…å®¹: {response1.content[:100]}...")
        print(f"ğŸ“Š Token ä½¿ç”¨: è¾“å…¥={response1.usage.prompt_tokens}, "
              f"è¾“å‡º={response1.usage.completion_tokens}, "
              f"æ€»è®¡={response1.usage.total_tokens}")
        print(f"ğŸ å®ŒæˆåŸå› : {response1.finish_reason}\n")
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤±è´¥: {e}\n")
        return

    print("=" * 70)
    print("ğŸ¯ ç¬¬äºŒæ¬¡è°ƒç”¨ - åº”è¯¥ä»ç¼“å­˜è¿”å›ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰")
    print("=" * 70)

    try:
        response2 = await client.chat(messages=messages)
        print(f"âœ… å“åº”æˆåŠŸ")
        print(f"ğŸ“ å†…å®¹: {response2.content[:100]}...")
        print(f"ğŸ“Š Token ä½¿ç”¨: è¾“å…¥={response2.usage.prompt_tokens}, "
              f"è¾“å‡º={response2.usage.completion_tokens}, "
              f"æ€»è®¡={response2.usage.total_tokens}")
        print(f"ğŸ å®ŒæˆåŸå› : {response2.finish_reason}\n")
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤±è´¥: {e}\n")
        return

    # éªŒè¯ç¼“å­˜
    if response1.content == response2.content:
        print("âœ… ç¼“å­˜éªŒè¯é€šè¿‡ - ä¸¤æ¬¡å“åº”å†…å®¹å®Œå…¨ä¸€è‡´")
    else:
        print("âŒ ç¼“å­˜éªŒè¯å¤±è´¥ - ä¸¤æ¬¡å“åº”å†…å®¹ä¸ä¸€è‡´")
        print(f"å“åº”1: {response1.content[:50]}...")
        print(f"å“åº”2: {response2.content[:50]}...")

    print("\n" + "=" * 70)
    print("ğŸ§¹ æ¸…ç†ç¼“å­˜æµ‹è¯•")
    print("=" * 70)

    try:
        deleted_count = await clear_llm_cache()
        print(f"âœ… å·²æ¸…ç† {deleted_count} ä¸ªç¼“å­˜æ¡ç›®\n")
    except Exception as e:
        print(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}\n")

    print("=" * 70)
    print("âœ¨ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("=" * 70)


if __name__ == "__main__":
    # è¿è¡Œå¼‚æ­¥ä¸»å‡½æ•°
    asyncio.run(main())
