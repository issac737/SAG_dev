"""
æ–‡æ¡£è§£æTokenç»Ÿè®¡æµ‹è¯•è„šæœ¬

æµ‹è¯•10ä¸ªæ–‡æ¡£çš„è§£æè¿‡ç¨‹,ç»Ÿè®¡æ¯ç¯‡æ–‡ç« è§£ææ¶ˆè€—çš„è¾“å…¥tokenå’Œè¾“å‡ºtoken

ä½¿ç”¨æ–¹æ³•:
    python -m tests.token_stats.test_document_parsing_tokens
"""

import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime

from dataflow.modules.load.config import DocumentLoadConfig
from dataflow.modules.load.loader import DocumentLoader
from dataflow.modules.extract.config import ExtractConfig
from dataflow.modules.extract.extractor import EventExtractor
from dataflow.core.prompt.manager import PromptManager
from dataflow.db import get_session_factory, Article
from dataflow.utils import get_logger, setup_logging

# é…ç½®æ—¥å¿— - æ˜¾ç¤ºè¯¦ç»†çš„å¤„ç†è¿‡ç¨‹
# è®¾ç½®æ ¹loggerå’Œdataflow loggeréƒ½è¾“å‡º
import sys
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    stream=sys.stdout,
    force=True  # å¼ºåˆ¶é‡æ–°é…ç½®
)
setup_logging(level="INFO")  # é…ç½®dataflowå‘½åç©ºé—´

logger = get_logger("test.token_stats")


# ============================================================
# Tokenè¿½è¸ªå™¨ - è¿½è¸ªæ‰€æœ‰LLMè°ƒç”¨
# ============================================================

class LLMCallTracker:
    """è¿½è¸ªæ‰€æœ‰LLMè°ƒç”¨çš„tokenæ¶ˆè€—"""

    def __init__(self):
        self.calls = []
        self.total = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }

    def record(self, stage: str, purpose: str, usage):
        """è®°å½•ä¸€æ¬¡LLMè°ƒç”¨"""
        if usage is None:
            return

        tokens = {
            "stage": stage,
            "purpose": purpose,
            "prompt_tokens": getattr(usage, 'prompt_tokens', 0),
            "completion_tokens": getattr(usage, 'completion_tokens', 0),
            "total_tokens": getattr(usage, 'total_tokens', 0)
        }

        self.calls.append(tokens)
        self.total["prompt_tokens"] += tokens["prompt_tokens"]
        self.total["completion_tokens"] += tokens["completion_tokens"]
        self.total["total_tokens"] += tokens["total_tokens"]

        logger.info(
            f"ğŸ¤– [{stage}] {purpose}: "
            f"è¾“å…¥={tokens['prompt_tokens']:,}, "
            f"è¾“å‡º={tokens['completion_tokens']:,}, "
            f"æ€»è®¡={tokens['total_tokens']:,}"
        )

    def reset(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.calls = []
        self.total = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }

    def get_stats_by_stage(self):
        """æŒ‰é˜¶æ®µåˆ†ç»„ç»Ÿè®¡"""
        stats = {}
        for call in self.calls:
            stage = call['stage']
            if stage not in stats:
                stats[stage] = {
                    "calls": [],
                    "total": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
                }
            stats[stage]["calls"].append(call)
            stats[stage]["total"]["prompt_tokens"] += call["prompt_tokens"]
            stats[stage]["total"]["completion_tokens"] += call["completion_tokens"]
            stats[stage]["total"]["total_tokens"] += call["total_tokens"]
        return stats


# å…¨å±€è¿½è¸ªå™¨
_llm_tracker = LLMCallTracker()


def enable_llm_tracking():
    """å¯ç”¨LLMè°ƒç”¨è¿½è¸ª"""
    from dataflow.core.ai import llm

    original_chat = llm.OpenAIClient.chat

    async def tracked_chat(self, messages, **kwargs):
        # è°ƒç”¨åŸå§‹æ–¹æ³•
        result = await original_chat(self, messages, **kwargs)

        # åˆ¤æ–­è°ƒç”¨æ¥æº
        import inspect
        frame = inspect.currentframe()
        stage = "UNKNOWN"
        purpose = "LLMè°ƒç”¨"

        # å‘ä¸ŠæŸ¥æ‰¾è°ƒç”¨æ ˆ
        try:
            for _ in range(15):
                if frame is None:
                    break
                frame = frame.f_back
                if frame and 'self' in frame.f_locals:
                    obj = frame.f_locals['self']
                    class_name = obj.__class__.__name__

                    if 'SumySummarizer' in class_name or 'DocumentProcessor' in class_name:
                        stage = "LOAD"
                        purpose = "ç”Ÿæˆå…ƒæ•°æ®(æ ‡é¢˜/æ‘˜è¦/åˆ†ç±»/æ ‡ç­¾)"
                        break
                    elif 'ExtractorAgent' in class_name:
                        stage = "EXTRACT"
                        purpose = "æå–äº‹é¡¹"
                        break
                    elif 'EventExtractor' in class_name:
                        stage = "EXTRACT"
                        purpose = "æå–äº‹é¡¹"
                        break
        except:
            pass

        # è®°å½•usage
        if hasattr(result, 'usage'):
            _llm_tracker.record(stage, purpose, result.usage)

        return result

    # æ›¿æ¢æ–¹æ³•
    llm.OpenAIClient.chat = tracked_chat
    logger.info("âœ… LLMè°ƒç”¨è¿½è¸ªå·²å¯ç”¨")


# åœ¨æ¨¡å—åŠ è½½æ—¶å¯ç”¨è¿½è¸ª
enable_llm_tracking()


class DocumentTokenStats:
    """æ–‡æ¡£è§£æTokenç»Ÿè®¡å™¨"""

    def __init__(self, source_config_id: str):
        """
        åˆå§‹åŒ–ç»Ÿè®¡å™¨

        Args:
            source_config_id: æºé…ç½®ID (å­—ç¬¦ä¸²ç±»å‹)
        """
        self.source_config_id = source_config_id
        self.doc_stats: List[Dict] = []
        self.total_usage = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
        }
        self.session_factory = get_session_factory()

    async def process_document(self, file_path: str, doc_index: int, background: Optional[str] = None):
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£å¹¶ç»Ÿè®¡token

        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            doc_index: æ–‡æ¡£ç´¢å¼•(ä»0å¼€å§‹)
            background: èƒŒæ™¯ä¿¡æ¯(å¯é€‰)
        """
        print(f"\n{'=' * 80}")
        print(f"ğŸ“„ å¤„ç†æ–‡æ¡£ {doc_index + 1}: {Path(file_path).name}")
        print(f"{'=' * 80}")

        start_time = datetime.now()

        # é‡ç½®å…¨å±€è¿½è¸ªå™¨
        _llm_tracker.reset()

        try:
            # 1. åŠ è½½æ–‡æ¡£
            logger.info("æ­¥éª¤1: åŠ è½½æ–‡æ¡£...")

            # ä½¿ç”¨DocumentLoaderç›´æ¥åŠ è½½
            # æ³¨æ„: DocumentLoaderä¼šè‡ªåŠ¨åˆ›å»ºsource_config(å¦‚æœä¸å­˜åœ¨)
            loader = DocumentLoader()

            # æ„å»ºåŠ è½½é…ç½®
            load_config = DocumentLoadConfig(
                source_config_id=str(self.source_config_id),  # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
                path=file_path,
                background=background or "æµ‹è¯•æ–‡æ¡£è§£ætokenæ¶ˆè€—",
                auto_vector=True,  # è‡ªåŠ¨ç”Ÿæˆå‘é‡
            )

            # åŠ è½½æ–‡æ¡£
            load_result = await loader.load(load_config)
            article_id = load_result.source_id

            # ä»æ•°æ®åº“è·å–articleå¯¹è±¡
            async with self.session_factory() as session:
                article = await session.get(Article, article_id)
                if not article:
                    raise Exception(f"æœªæ‰¾åˆ°æ–‡ç« : {article_id}")

            logger.info(f"æ–‡æ¡£å·²åŠ è½½: article_id={article.id}, title={article.title}")

            # 2. æå–äº‹é¡¹(è¿™é‡Œä¼šè°ƒç”¨LLM)
            logger.info("æ­¥éª¤2: æå–äº‹é¡¹...")

            # åˆ›å»ºæå–å™¨
            prompt_manager = PromptManager()
            extractor = EventExtractor(
                prompt_manager=prompt_manager,
                model_config=None  # ä½¿ç”¨é»˜è®¤é…ç½®
            )

            # è·å–è¯¥æ–‡æ¡£çš„æ‰€æœ‰chunks
            async with self.session_factory() as session:
                from sqlalchemy import select
                from dataflow.db import SourceChunk

                result = await session.execute(
                    select(SourceChunk)
                    .where(SourceChunk.source_id == article.id)
                    .where(SourceChunk.source_type == "ARTICLE")
                    .order_by(SourceChunk.rank)
                )
                chunks = list(result.scalars().all())

            if not chunks:
                logger.warning("æœªæ‰¾åˆ°chunks,è·³è¿‡æå–")
                return

            logger.info(f"æ‰¾åˆ° {len(chunks)} ä¸ªchunks")

            # æ„å»ºæå–é…ç½®
            extract_config = ExtractConfig(
                source_config_id=str(self.source_config_id),  # æ·»åŠ å¿…éœ€çš„source_config_id
                chunk_ids=[chunk.id for chunk in chunks],
                parallel=True,
                max_concurrency=5,
            )

            # æ‰§è¡Œæå–
            events = await extractor.extract(extract_config)

            # è®°å½•è€—æ—¶
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()

            # è·å–æŒ‰é˜¶æ®µåˆ†ç»„çš„ç»Ÿè®¡ - ä½¿ç”¨trackerçš„å®Œæ•´ç»Ÿè®¡
            stage_stats = _llm_tracker.get_stats_by_stage()

            # ç»Ÿè®¡ä¿¡æ¯ - ä½¿ç”¨trackerçš„æ€»è®¡(åŒ…å«Loadå’ŒExtract)
            doc_stat = {
                "index": doc_index + 1,
                "filename": Path(file_path).name,
                "article_id": article.id,
                "article_title": article.title,
                "chunks_count": len(chunks),
                "events_count": len(events),
                "prompt_tokens": _llm_tracker.total["prompt_tokens"],
                "completion_tokens": _llm_tracker.total["completion_tokens"],
                "total_tokens": _llm_tracker.total["total_tokens"],
                "duration_seconds": duration,
                "stage_stats": stage_stats,  # æ·»åŠ é˜¶æ®µç»Ÿè®¡
            }

            self.doc_stats.append(doc_stat)

            # ç´¯åŠ æ€»è®¡
            self.total_usage["prompt_tokens"] += _llm_tracker.total["prompt_tokens"]
            self.total_usage["completion_tokens"] += _llm_tracker.total["completion_tokens"]
            self.total_usage["total_tokens"] += _llm_tracker.total["total_tokens"]

            # æ‰“å°è¯¦ç»†ç»Ÿè®¡
            print(f"\n{'â”€' * 80}")
            print("ğŸ“Š Tokenæ¶ˆè€—è¯¦ç»†ç»Ÿè®¡:")
            print(f"{'â”€' * 80}")

            # Loadé˜¶æ®µç»Ÿè®¡
            if "LOAD" in stage_stats:
                load_total = stage_stats["LOAD"]["total"]
                print(f"\nğŸ“‚ Loadé˜¶æ®µ (æ–‡æ¡£åŠ è½½+å…ƒæ•°æ®ç”Ÿæˆ):")
                print(f"   è¾“å…¥Token:  {load_total['prompt_tokens']:>12,}")
                print(f"   è¾“å‡ºToken:  {load_total['completion_tokens']:>12,}")
                print(f"   æ€»è®¡Token:  {load_total['total_tokens']:>12,}")

            # Extracté˜¶æ®µç»Ÿè®¡
            if "EXTRACT" in stage_stats:
                extract_total = stage_stats["EXTRACT"]["total"]
                print(f"\nğŸ” Extracté˜¶æ®µ (äº‹é¡¹æå–):")
                print(f"   è¾“å…¥Token:  {extract_total['prompt_tokens']:>12,}")
                print(f"   è¾“å‡ºToken:  {extract_total['completion_tokens']:>12,}")
                print(f"   æ€»è®¡Token:  {extract_total['total_tokens']:>12,}")

            # æ€»è®¡
            print(f"\n{'â”€' * 80}")
            print(f"âœ… æ€»è¾“å…¥Token:  {_llm_tracker.total['prompt_tokens']:>12,}")
            print(f"âœ… æ€»è¾“å‡ºToken:  {_llm_tracker.total['completion_tokens']:>12,}")
            print(f"âœ… æ€»è®¡Token:    {_llm_tracker.total['total_tokens']:>12,}")
            print(f"{'â”€' * 80}")
            print(f"ğŸ“Š æå–äº‹é¡¹:  {len(events):>12} ä¸ª")
            print(f"ğŸ“Š æ–‡æ¡£ç‰‡æ®µ:  {len(chunks):>12} ä¸ª")
            print(f"â±ï¸  å¤„ç†è€—æ—¶:  {duration:>12.2f} ç§’")

        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥: {e}", exc_info=True)
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")

            # è®°å½•å¤±è´¥çš„æ–‡æ¡£
            doc_stat = {
                "index": doc_index + 1,
                "filename": Path(file_path).name,
                "error": str(e),
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0,
            }
            self.doc_stats.append(doc_stat)

    def print_summary(self):
        """æ‰“å°æ±‡æ€»ç»Ÿè®¡"""
        print(f"\n{'=' * 120}")
        print("ğŸ“Š Tokenæ¶ˆè€—ç»Ÿè®¡æ±‡æ€»")
        print(f"{'=' * 120}\n")

        # è¡¨å¤´
        print(
            f"{'åºå·':>4} | {'æ–‡ä»¶å':<35} | {'äº‹é¡¹æ•°':>8} | "
            f"{'Load Token':>14} | {'Extract Token':>14} | {'æ€»Token':>14} | {'è€—æ—¶(ç§’)':>10}"
        )
        print("-" * 120)

        # æ˜ç»†
        for stat in self.doc_stats:
            if "error" in stat:
                print(
                    f"{stat['index']:>4} | {stat['filename']:<35} | "
                    f"{'å¤±è´¥':>8} | {'-':>14} | {'-':>14} | {'-':>14} | {'-':>10}"
                )
            else:
                # è®¡ç®—Loadå’ŒExtractçš„token
                stage_stats = stat.get('stage_stats', {})
                load_tokens = stage_stats.get('LOAD', {}).get('total', {}).get('total_tokens', 0)
                extract_tokens = stage_stats.get('EXTRACT', {}).get('total', {}).get('total_tokens', 0)

                print(
                    f"{stat['index']:>4} | {stat['filename']:<35} | "
                    f"{stat['events_count']:>8,} | "
                    f"{load_tokens:>14,} | "
                    f"{extract_tokens:>14,} | "
                    f"{stat['total_tokens']:>14,} | "
                    f"{stat['duration_seconds']:>10.2f}"
                )

        print("-" * 120)

        # ç»Ÿè®¡æˆåŠŸçš„æ–‡æ¡£æ•°
        success_docs = [s for s in self.doc_stats if "error" not in s]
        total_docs = len(self.doc_stats)
        success_count = len(success_docs)

        # æ€»è®¡
        total_events = sum(s.get("events_count", 0) for s in success_docs)
        total_duration = sum(s.get("duration_seconds", 0) for s in success_docs)

        # è®¡ç®—å„é˜¶æ®µæ€»token
        total_load_tokens = 0
        total_extract_tokens = 0
        for stat in success_docs:
            stage_stats = stat.get('stage_stats', {})
            total_load_tokens += stage_stats.get('LOAD', {}).get('total', {}).get('total_tokens', 0)
            total_extract_tokens += stage_stats.get('EXTRACT', {}).get('total', {}).get('total_tokens', 0)

        print(
            f"{'æ€»è®¡':>4} | {f'{success_count}/{total_docs} ä¸ªæ–‡æ¡£':<35} | "
            f"{total_events:>8,} | "
            f"{total_load_tokens:>14,} | "
            f"{total_extract_tokens:>14,} | "
            f"{self.total_usage['total_tokens']:>14,} | "
            f"{total_duration:>10.2f}"
        )

        if success_count > 0:
            # å¹³å‡å€¼
            avg_events = total_events / success_count
            avg_load = total_load_tokens / success_count
            avg_extract = total_extract_tokens / success_count
            avg_total = self.total_usage["total_tokens"] / success_count
            avg_duration = total_duration / success_count

            print(
                f"{'å¹³å‡':>4} | {'':<35} | "
                f"{avg_events:>8,.1f} | "
                f"{avg_load:>14,.0f} | "
                f"{avg_extract:>14,.0f} | "
                f"{avg_total:>14,.0f} | "
                f"{avg_duration:>10.2f}"
            )

        print(f"\n{'=' * 120}\n")

        # é˜¶æ®µå æ¯”åˆ†æ
        if total_load_tokens + total_extract_tokens > 0:
            print("ğŸ“Š å„é˜¶æ®µTokenå æ¯”:")
            print(f"   Loadé˜¶æ®µ (å…ƒæ•°æ®ç”Ÿæˆ):  {total_load_tokens:>12,} tokens ({total_load_tokens/(total_load_tokens+total_extract_tokens)*100:>5.1f}%)")
            print(f"   Extracté˜¶æ®µ (äº‹é¡¹æå–): {total_extract_tokens:>12,} tokens ({total_extract_tokens/(total_load_tokens+total_extract_tokens)*100:>5.1f}%)")
            print(f"   æ€»è®¡:                    {total_load_tokens+total_extract_tokens:>12,} tokens\n")

        # æˆæœ¬ä¼°ç®—(åŸºäº302.AIçš„Qwen3-30Bå®šä»·,å‡è®¾æ¯ç™¾ä¸‡tokenä»·æ ¼)
        # æ³¨æ„: è¯·æ ¹æ®å®é™…APIå®šä»·ä¿®æ”¹è¿™é‡Œçš„ä»·æ ¼
        input_price_per_m = 1.0  # æ¯ç™¾ä¸‡è¾“å…¥tokençš„ä»·æ ¼(ç¾å…ƒ)
        output_price_per_m = 1.0  # æ¯ç™¾ä¸‡è¾“å‡ºtokençš„ä»·æ ¼(ç¾å…ƒ)

        input_cost = (self.total_usage["prompt_tokens"] / 1_000_000) * input_price_per_m
        output_cost = (self.total_usage["completion_tokens"] / 1_000_000) * output_price_per_m
        total_cost = input_cost + output_cost

        print("ğŸ’° æˆæœ¬ä¼°ç®— (åŸºäºå‡è®¾å®šä»·: $1.0/M tokens)")
        print(f"   è¾“å…¥æˆæœ¬: ${input_cost:.4f}")
        print(f"   è¾“å‡ºæˆæœ¬: ${output_cost:.4f}")
        print(f"   æ€»æˆæœ¬:   ${total_cost:.4f}")
        print()

    def export_to_csv(self, output_path: str = "token_stats.csv"):
        """
        å¯¼å‡ºç»Ÿè®¡ç»“æœä¸ºCSVæ–‡ä»¶

        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        import csv

        with open(output_path, "w", newline="", encoding="utf-8") as f:
            fieldnames = [
                "åºå·",
                "æ–‡ä»¶å",
                "æ–‡ç« ID",
                "æ–‡ç« æ ‡é¢˜",
                "ç‰‡æ®µæ•°",
                "äº‹é¡¹æ•°",
                "Loadé˜¶æ®µToken",
                "Extracté˜¶æ®µToken",
                "è¾“å…¥Token",
                "è¾“å‡ºToken",
                "æ€»Token",
                "è€—æ—¶(ç§’)",
            ]
            writer = csv.DictWriter(f, fieldnames=fieldnames)

            writer.writeheader()
            for stat in self.doc_stats:
                if "error" not in stat:
                    # è®¡ç®—å„é˜¶æ®µtoken
                    stage_stats = stat.get('stage_stats', {})
                    load_tokens = stage_stats.get('LOAD', {}).get('total', {}).get('total_tokens', 0)
                    extract_tokens = stage_stats.get('EXTRACT', {}).get('total', {}).get('total_tokens', 0)

                    writer.writerow(
                        {
                            "åºå·": stat["index"],
                            "æ–‡ä»¶å": stat["filename"],
                            "æ–‡ç« ID": stat.get("article_id", ""),
                            "æ–‡ç« æ ‡é¢˜": stat.get("article_title", ""),
                            "ç‰‡æ®µæ•°": stat.get("chunks_count", 0),
                            "äº‹é¡¹æ•°": stat.get("events_count", 0),
                            "Loadé˜¶æ®µToken": load_tokens,
                            "Extracté˜¶æ®µToken": extract_tokens,
                            "è¾“å…¥Token": stat["prompt_tokens"],
                            "è¾“å‡ºToken": stat["completion_tokens"],
                            "æ€»Token": stat["total_tokens"],
                            "è€—æ—¶(ç§’)": f"{stat.get('duration_seconds', 0):.2f}",
                        }
                    )

        logger.info(f"ç»Ÿè®¡ç»“æœå·²å¯¼å‡ºåˆ°: {output_path}")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""

    # ============================================================
    # é…ç½®åŒºåŸŸ - è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
    # ============================================================

    # 1. æºé…ç½®ID (éœ€è¦å…ˆåˆ›å»ºsource_config)
    # æ³¨æ„: source_config_id å¿…é¡»æ˜¯å­—ç¬¦ä¸²ç±»å‹
    # æ¯æ¬¡æµ‹è¯•ä½¿ç”¨æ–°çš„IDé¿å…æ•°æ®å†²çª
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    SOURCE_CONFIG_ID = f"token-test-source"  # ä½¿ç”¨æ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€ID

    # 2. æµ‹è¯•æ–‡æ¡£åˆ—è¡¨ - è‡ªåŠ¨éå†ç›®å½•ä¸‹æ‰€æœ‰mdæ–‡ä»¶
    # æ”¯æŒæ ¼å¼: .md, .pdf, .html, .txt
    data_dir = Path("tests/token_stats/data")

    # è‡ªåŠ¨è·å–æ‰€æœ‰ .md æ–‡ä»¶
    test_documents = [str(f) for f in data_dir.glob("*.md")]

    # æˆ–è€…æ‰‹åŠ¨æŒ‡å®šæ–‡ä»¶åˆ—è¡¨:
    # test_documents = [
    #     "tests\\token_stats\\data\\æ–‡ä»¶1.md",
    #     "tests\\token_stats\\data\\æ–‡ä»¶2.md",
    # ]
    print(test_documents)
    # 3. èƒŒæ™¯ä¿¡æ¯ (å¯é€‰)
    background = "è¿™æ˜¯ä¸€æ‰¹æµ‹è¯•æ–‡æ¡£,ç”¨äºç»Ÿè®¡æ–‡æ¡£è§£æçš„tokenæ¶ˆè€—"

    # 4. é™åˆ¶å¤„ç†æ–‡æ¡£æ•°é‡ (ç”¨äºæµ‹è¯•,Noneè¡¨ç¤ºå¤„ç†å…¨éƒ¨)
    MAX_DOCS = 10

    # ============================================================

    print("=" * 80)
    print("ğŸ“Š æ–‡æ¡£è§£æTokenç»Ÿè®¡æµ‹è¯•")
    print("=" * 80)
    print(f"æºé…ç½®ID: {SOURCE_CONFIG_ID}")
    print(f"æ–‡æ¡£æ•°é‡: {min(len(test_documents), MAX_DOCS) if MAX_DOCS else len(test_documents)}")
    print("=" * 80)

    # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
    valid_docs = []
    for doc_path in test_documents:
        if Path(doc_path).exists():
            valid_docs.append(doc_path)
        else:
            logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨,è·³è¿‡: {doc_path}")

    if not valid_docs:
        logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æµ‹è¯•æ–‡æ¡£,è¯·æ£€æŸ¥æ–‡æ¡£è·¯å¾„é…ç½®")
        return

    # é™åˆ¶æ–‡æ¡£æ•°é‡
    if MAX_DOCS:
        valid_docs = valid_docs[:MAX_DOCS]

    print(f"\næ‰¾åˆ° {len(valid_docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£\n")

    # åˆ›å»ºç»Ÿè®¡å™¨
    stats = DocumentTokenStats(source_config_id=SOURCE_CONFIG_ID)

    # ä¾æ¬¡å¤„ç†æ–‡æ¡£
    for idx, doc_path in enumerate(valid_docs):
        await stats.process_document(doc_path, idx, background)

    # æ‰“å°æ±‡æ€»
    stats.print_summary()

    # å¯¼å‡ºCSV
    output_csv = "token_stats_result.csv"
    stats.export_to_csv(output_csv)
    print(f"ğŸ“„ è¯¦ç»†æ•°æ®å·²å¯¼å‡ºåˆ°: {output_csv}\n")


if __name__ == "__main__":
    asyncio.run(main())
