"""
æµ‹è¯•Stage1æœç´¢ä¸­çš„å®ä½“æå–åŠŸèƒ½

åŒ…å«å•å…ƒæµ‹è¯•ï¼ˆæ¨¡æ‹Ÿï¼‰å’Œé›†æˆæµ‹è¯•ï¼ˆçœŸå®LLMï¼‰
"""

import pytest
import asyncio
from unittest.mock import AsyncMock, MagicMock
from typing import List, Dict, Any

from dataflow.core.ai.base import BaseLLMClient
from dataflow.core.prompt.manager import PromptManager
from dataflow.modules.search.stage1 import Stage1Searcher
from dataflow.modules.search.config import SearchConfig

# æ ‡è®°é›†æˆæµ‹è¯•
pytestmark = pytest.mark.asyncio


@pytest.fixture
def mock_llm_client():
    """æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯"""
    client = MagicMock(spec=BaseLLMClient)
    client.chat_with_schema = AsyncMock()
    return client


@pytest.fixture
def mock_prompt_manager():
    """æ¨¡æ‹Ÿæç¤ºè¯ç®¡ç†å™¨"""
    manager = MagicMock(spec=PromptManager)

    # åˆ›å»ºä¸€ä¸ªçœŸæ­£çš„ MagicMock å¯¹è±¡æ¥æ¨¡æ‹Ÿ render æ–¹æ³•
    render_mock = MagicMock()

    # é…ç½® render æ–¹æ³•çš„è¡Œä¸º
    def mock_render_impl(template, **kwargs):
        query = kwargs.get('query', '')
        return (
            f"è¯·ä»æŸ¥è¯¢ '{query}' ä¸­æå–å®ä½“ã€‚"
            f"å®ä½“ç±»å‹åŒ…æ‹¬ï¼špersonï¼ˆäººç‰©ï¼‰ã€locationï¼ˆåœ°ç‚¹ï¼‰ã€timeï¼ˆæ—¶é—´ï¼‰ã€"
            f"topicï¼ˆè¯é¢˜ï¼‰ã€actionï¼ˆåŠ¨ä½œï¼‰ã€organizationï¼ˆç»„ç»‡ï¼‰ã€productï¼ˆäº§å“ï¼‰ç­‰ã€‚"
            f"è¯·æŒ‰ç…§ JSON Schema æ ¼å¼è¿”å›ç»“æ„åŒ–ç»“æœã€‚"
        )

    # ä½¿ç”¨ side_effect è®©å‡½æ•°è¿”å›å®é™…å­—ç¬¦ä¸²ï¼ŒåŒæ—¶ä¿æŒ mock åŠŸèƒ½
    render_mock.side_effect = mock_render_impl
    manager.render = render_mock

    return manager


@pytest.fixture
def stage1_searcher(mock_llm_client, mock_prompt_manager):
    """åˆ›å»ºStage1æœç´¢å™¨å®ä¾‹"""
    return Stage1Searcher(mock_llm_client, mock_prompt_manager)


@pytest.mark.asyncio
async def test_extract_attributes_with_new_method(stage1_searcher, mock_llm_client):
    """æµ‹è¯•æ–°çš„å±æ€§æå–æ–¹æ³•"""

    # æ¨¡æ‹ŸLLMå“åº” - ä½¿ç”¨ attributes æ ¼å¼
    mock_response = {
        "attributes": [
            {"name": "å¼ ä¸‰", "type": "person", "context": "å¼ ä¸‰åœ¨åŒ—äº¬ä»äº‹äººå·¥æ™ºèƒ½ç›¸å…³å·¥ä½œ", "importance": "high"},
            {"name": "åŒ—äº¬", "type": "location", "context": "å¼ ä¸‰åœ¨åŒ—äº¬ä»äº‹äººå·¥æ™ºèƒ½ç›¸å…³å·¥ä½œ", "importance": "high"},
            {"name": "äººå·¥æ™ºèƒ½", "type": "topic", "context": "å¼ ä¸‰åœ¨åŒ—äº¬ä»äº‹äººå·¥æ™ºèƒ½ç›¸å…³å·¥ä½œ", "importance": "high"}
        ]
    }
    mock_llm_client.chat_with_schema.return_value = mock_response

    # æµ‹è¯•æŸ¥è¯¢
    query = "å¼ ä¸‰åœ¨åŒ—äº¬ä»äº‹äººå·¥æ™ºèƒ½ç›¸å…³å·¥ä½œ"

    # è°ƒç”¨å±æ€§æå–æ–¹æ³•
    attributes = await stage1_searcher._extract_attributes_from_query(query)

    # éªŒè¯ç»“æœ
    assert len(attributes) == 3

    # éªŒè¯äººå‘˜å®ä½“
    person_attrs = [attr for attr in attributes if attr["type"] == "person"]
    assert len(person_attrs) == 1
    assert person_attrs[0]["name"] == "å¼ ä¸‰"
    assert person_attrs[0]["importance"] == "high"
    assert person_attrs[0]["confidence"] == 0.9  # high importance å¯¹åº” 0.9 ç½®ä¿¡åº¦

    # éªŒè¯åœ°ç‚¹å®ä½“
    location_attrs = [attr for attr in attributes if attr["type"] == "location"]
    assert len(location_attrs) == 1
    assert location_attrs[0]["name"] == "åŒ—äº¬"
    assert location_attrs[0]["importance"] == "high"
    assert location_attrs[0]["confidence"] == 0.9  # high importance å¯¹åº” 0.9 ç½®ä¿¡åº¦

    # éªŒè¯è¯é¢˜å®ä½“
    topic_attrs = [attr for attr in attributes if attr["type"] == "topic"]
    assert len(topic_attrs) == 1
    assert topic_attrs[0]["name"] == "äººå·¥æ™ºèƒ½"
    assert topic_attrs[0]["importance"] == "high"
    assert topic_attrs[0]["confidence"] == 0.9  # high importance å¯¹åº” 0.9 ç½®ä¿¡åº¦


@pytest.mark.asyncio
async def test_extract_attributes_with_string_format(stage1_searcher, mock_llm_client):
    """æµ‹è¯•å­—ç¬¦ä¸²æ ¼å¼çš„å®ä½“å“åº”"""

    # æ¨¡æ‹ŸLLMå“åº” - ä½¿ç”¨ attributes æ ¼å¼
    mock_response = {
        "attributes": [
            {"name": "æå››", "type": "person", "context": "æå››åœ¨ä¸Šæµ·å’Œæ·±åœ³ç ”ç©¶åŒºå—é“¾æŠ€æœ¯", "importance": "high"},
            {"name": "ä¸Šæµ·", "type": "location", "context": "æå››åœ¨ä¸Šæµ·å’Œæ·±åœ³ç ”ç©¶åŒºå—é“¾æŠ€æœ¯", "importance": "high"},
            {"name": "æ·±åœ³", "type": "location", "context": "æå››åœ¨ä¸Šæµ·å’Œæ·±åœ³ç ”ç©¶åŒºå—é“¾æŠ€æœ¯", "importance": "high"},
            {"name": "åŒºå—é“¾æŠ€æœ¯", "type": "topic", "context": "æå››åœ¨ä¸Šæµ·å’Œæ·±åœ³ç ”ç©¶åŒºå—é“¾æŠ€æœ¯", "importance": "medium"}
        ]
    }
    mock_llm_client.chat_with_schema.return_value = mock_response

    # æµ‹è¯•æŸ¥è¯¢
    query = "æå››åœ¨ä¸Šæµ·å’Œæ·±åœ³ç ”ç©¶åŒºå—é“¾æŠ€æœ¯"

    # è°ƒç”¨å±æ€§æå–æ–¹æ³•
    attributes = await stage1_searcher._extract_attributes_from_query(query)

    # éªŒè¯ç»“æœ
    assert len(attributes) == 4

    # éªŒè¯äººå‘˜å®ä½“
    person_attrs = [attr for attr in attributes if attr["type"] == "person"]
    assert len(person_attrs) == 1
    assert person_attrs[0]["name"] == "æå››"

    # éªŒè¯åœ°ç‚¹å®ä½“
    location_attrs = [attr for attr in attributes if attr["type"] == "location"]
    assert len(location_attrs) == 2
    location_names = [attr["name"] for attr in location_attrs]
    assert "ä¸Šæµ·" in location_names
    assert "æ·±åœ³" in location_names

    # éªŒè¯è¯é¢˜å®ä½“
    topic_attrs = [attr for attr in attributes if attr["type"] == "topic"]
    assert len(topic_attrs) == 1
    assert topic_attrs[0]["name"] == "åŒºå—é“¾æŠ€æœ¯"


@pytest.mark.asyncio
async def test_extract_attributes_empty_response(stage1_searcher, mock_llm_client):
    """æµ‹è¯•ç©ºå“åº”çš„å¤„ç†"""

    # æ¨¡æ‹Ÿç©ºå“åº”
    mock_response = {"attributes": []}
    mock_llm_client.chat_with_schema.return_value = mock_response

    # æµ‹è¯•æŸ¥è¯¢
    query = "è¿™æ˜¯ä¸€ä¸ªæ²¡æœ‰å®ä½“çš„æŸ¥è¯¢"

    # è°ƒç”¨å±æ€§æå–æ–¹æ³•
    attributes = await stage1_searcher._extract_attributes_from_query(query)

    # éªŒè¯ç»“æœï¼ˆåº”è¯¥ä½¿ç”¨å›é€€æ–¹æ¡ˆï¼‰
    assert isinstance(attributes, list)
    # å›é€€æ–¹æ¡ˆå¯èƒ½è¿”å›ä¸€äº›åŸºç¡€å±æ€§ï¼Œæˆ–è€…ç©ºåˆ—è¡¨


@pytest.mark.asyncio
async def test_extract_attributes_llm_failure(stage1_searcher, mock_llm_client):
    """æµ‹è¯•LLMè°ƒç”¨å¤±è´¥æ—¶çš„å›é€€å¤„ç†"""

    # æ¨¡æ‹ŸLLMè°ƒç”¨å¤±è´¥
    mock_llm_client.chat_with_schema.side_effect = Exception("LLMè°ƒç”¨å¤±è´¥")

    # æµ‹è¯•æŸ¥è¯¢
    query = "å¼ ä¸‰åœ¨åŒ—äº¬å·¥ä½œ"

    # è°ƒç”¨å±æ€§æå–æ–¹æ³•
    attributes = await stage1_searcher._extract_attributes_from_query(query)

    # éªŒè¯ç»“æœï¼ˆåº”è¯¥ä½¿ç”¨å›é€€æ–¹æ¡ˆï¼‰
    assert isinstance(attributes, list)
    # å›é€€æ–¹æ¡ˆåº”è¯¥èƒ½å¤Ÿæå–åˆ°ä¸€äº›åŸºç¡€å±æ€§


async def test_build_attribute_extraction_prompt(stage1_searcher):
    """æµ‹è¯•æç¤ºè¯æ„å»º"""

    query = "å¼ ä¸‰åœ¨åŒ—äº¬ä»äº‹AIç ”ç©¶"

    prompt = stage1_searcher._build_attribute_extraction_prompt(query)

    # éªŒè¯æç¤ºè¯åŒ…å«å…³é”®å…ƒç´ 
    assert "å¼ ä¸‰åœ¨åŒ—äº¬ä»äº‹AIç ”ç©¶" in prompt
    assert "time" in prompt or "person" in prompt  # è‡³å°‘åŒ…å«ä¸€äº›å®ä½“ç±»å‹æç¤º
    assert "JSON Schema" in prompt

    # éªŒè¯ render æ–¹æ³•è¢«æ­£ç¡®è°ƒç”¨
    stage1_searcher.prompt_manager.render.assert_called_once_with("search/extract_attributes", query=query)


async def test_build_attribute_extraction_schema(stage1_searcher):
    """æµ‹è¯•JSON Schemaæ„å»º"""

    schema = stage1_searcher._build_attribute_extraction_schema()

    # éªŒè¯Schemaç»“æ„
    assert schema["type"] == "object"
    assert "attributes" in schema["properties"]
    assert schema["properties"]["attributes"]["type"] == "array"
    assert "items" in schema["properties"]["attributes"]

    # éªŒè¯å±æ€§é¡¹çš„schema
    item_schema = schema["properties"]["attributes"]["items"]
    assert "name" in item_schema["properties"]
    assert "type" in item_schema["properties"]
    assert "context" in item_schema["properties"]
    assert "importance" in item_schema["properties"]


async def test_parse_attribute_extraction_response(stage1_searcher):
    """æµ‹è¯•å“åº”è§£æ"""

    # æ¨¡æ‹Ÿå“åº”
    response = {
        "attributes": [
            {"name": "ç‹äº”", "type": "person", "context": "ç‹äº”åœ¨æ­å·å·¥ä½œ", "importance": "high"},
            {"name": "æ­å·", "type": "location", "context": "ç‹äº”åœ¨æ­å·å·¥ä½œ", "importance": "medium"}
        ]
    }

    attributes = stage1_searcher._parse_attribute_extraction_response(response)

    # éªŒè¯è§£æç»“æœ
    assert len(attributes) == 2

    person_attrs = [attr for attr in attributes if attr["type"] == "person"]
    assert len(person_attrs) == 1
    assert person_attrs[0]["name"] == "ç‹äº”"
    assert person_attrs[0]["importance"] == "high"
    assert person_attrs[0]["confidence"] == 0.9  # high importance å¯¹åº” 0.9

    location_attrs = [attr for attr in attributes if attr["type"] == "location"]
    assert len(location_attrs) == 1
    assert location_attrs[0]["name"] == "æ­å·"
    assert location_attrs[0]["importance"] == "medium"
    assert location_attrs[0]["confidence"] == 0.7  # medium importance å¯¹åº” 0.7


async def test_parse_attribute_extraction_response_string_format(stage1_searcher):
    """æµ‹è¯•å­—ç¬¦ä¸²æ ¼å¼çš„å“åº”è§£æ"""

    # æ¨¡æ‹Ÿå­—ç¬¦ä¸²æ ¼å¼çš„å“åº”
    response = {
        "attributes": [
            {"name": "èµµå…­", "type": "person", "context": "èµµå…­ç ”ç©¶æœºå™¨å­¦ä¹ ", "importance": "medium"},
            {"name": "æœºå™¨å­¦ä¹ ", "type": "topic", "context": "èµµå…­ç ”ç©¶æœºå™¨å­¦ä¹ ", "importance": "medium"}
        ]
    }

    attributes = stage1_searcher._parse_attribute_extraction_response(response)

    # éªŒè¯è§£æç»“æœ
    assert len(attributes) == 2

    person_attrs = [attr for attr in attributes if attr["type"] == "person"]
    assert len(person_attrs) == 1
    assert person_attrs[0]["name"] == "èµµå…­"
    assert person_attrs[0]["importance"] == "medium"
    assert person_attrs[0]["confidence"] == 0.7  # medium importance å¯¹åº” 0.7

    topic_attrs = [attr for attr in attributes if attr["type"] == "topic"]
    assert len(topic_attrs) == 1
    assert topic_attrs[0]["name"] == "æœºå™¨å­¦ä¹ "


# ==================== é›†æˆæµ‹è¯•ï¼ˆçœŸå®LLMï¼‰ ====================

@pytest.mark.integration
async def test_real_llm_entity_extraction():
    """ä½¿ç”¨çœŸå®LLMæµ‹è¯•å®ä½“æå–åŠŸèƒ½"""
    pytest.skip("éœ€è¦é…ç½®APIå¯†é’¥æ‰èƒ½è¿è¡Œé›†æˆæµ‹è¯•")

@pytest.mark.integration
async def test_real_llm_simple_query():
    """æµ‹è¯•ç®€å•æŸ¥è¯¢çš„çœŸå®LLMå®ä½“æå–"""
    pytest.skip("éœ€è¦é…ç½®APIå¯†é’¥æ‰èƒ½è¿è¡Œé›†æˆæµ‹è¯•")

@pytest.mark.integration
async def test_real_llm_complex_query():
    """æµ‹è¯•å¤æ‚æŸ¥è¯¢çš„çœŸå®LLMå®ä½“æå–"""
    pytest.skip("éœ€è¦é…ç½®APIå¯†é’¥æ‰èƒ½è¿è¡Œé›†æˆæµ‹è¯•")


# æ‰‹åŠ¨è¿è¡Œé›†æˆæµ‹è¯•çš„è¾…åŠ©å‡½æ•°
async def run_integration_tests():
    """æ‰‹åŠ¨è¿è¡Œé›†æˆæµ‹è¯•ï¼ˆä¸é€šè¿‡pytestï¼‰"""
    print("ğŸ§ª å¼€å§‹è¿è¡ŒçœŸå®LLMé›†æˆæµ‹è¯•")
    print("=" * 50)

    try:
        from dataflow.core.ai.llm import OpenAIClient
        from dataflow.core.config import get_settings

        # æ£€æŸ¥é…ç½®
        settings = get_settings()
        if not settings.llm_api_key:
            print("âŒ æœªé…ç½®LLM APIå¯†é’¥ï¼Œè·³è¿‡é›†æˆæµ‹è¯•")
            return

        # åˆå§‹åŒ–ç»„ä»¶
        llm_client = OpenAIClient(
            api_key=settings.llm_api_key,
            base_url=settings.llm_base_url,
            model=settings.llm_model,
        )
        prompt_manager = PromptManager()
        searcher = Stage1Searcher(llm_client, prompt_manager)

        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {
                "name": "ç®€å•å®ä½“æŸ¥è¯¢",
                "query": "å¼ ä¸‰åœ¨åŒ—äº¬ä»äº‹äººå·¥æ™ºèƒ½ç›¸å…³å·¥ä½œ",
                "expected_entities": ["å¼ ä¸‰", "åŒ—äº¬", "äººå·¥æ™ºèƒ½"]
            },
            {
                "name": "å¤šåœ°ç‚¹æŸ¥è¯¢",
                "query": "æå››çš„å…¬å¸åœ¨ä¸Šæµ·å’Œæ·±åœ³éƒ½æœ‰åˆ†å…¬å¸ï¼Œä¸»è¦åšåŒºå—é“¾æŠ€æœ¯",
                "expected_entities": ["æå››", "ä¸Šæµ·", "æ·±åœ³", "åŒºå—é“¾æŠ€æœ¯"]
            },
            {
                "name": "æ—¶é—´ç›¸å…³æŸ¥è¯¢",
                "query": "2024å¹´å¤å­£ï¼Œç‹äº”åœ¨æ­å·å‚åŠ äº†AIæŠ€æœ¯å¤§ä¼š",
                "expected_entities": ["2024å¹´å¤å­£", "ç‹äº”", "æ­å·", "AIæŠ€æœ¯å¤§ä¼š"]
            },
            {
                "name": "æŠ€æœ¯ä¸»é¢˜æŸ¥è¯¢",
                "query": "å¦‚ä½•æé«˜æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‡†ç¡®ç‡ï¼Ÿ",
                "expected_entities": ["æœºå™¨å­¦ä¹ ", "æ¨¡å‹", "å‡†ç¡®ç‡"]
            },
            {
                "name": "å¤æ‚æŠ€æœ¯æŸ¥è¯¢",
                "query": "æ·±åº¦å­¦ä¹ æ¡†æ¶TensorFlowå’ŒPyTorchåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨å¯¹æ¯”",
                "expected_entities": ["æ·±åº¦å­¦ä¹ ", "TensorFlow", "PyTorch", "è‡ªç„¶è¯­è¨€å¤„ç†"]
            }
        ]

        # è¿è¡Œæµ‹è¯•
        results = []
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nğŸ“ æµ‹è¯•ç”¨ä¾‹ {i}: {test_case['name']}")
            print(f"æŸ¥è¯¢: {test_case['query']}")
            print("-" * 40)

            try:
                # è®°å½•å¼€å§‹æ—¶é—´
                import time
                start_time = time.time()

                # è°ƒç”¨å®ä½“æå–
                attributes = await searcher._extract_attributes_from_query(test_case['query'])

                # è®¡ç®—å“åº”æ—¶é—´
                response_time = time.time() - start_time

                # åˆ†æç»“æœ
                extracted_names = [attr['name'] for attr in attributes]
                found_entities = []
                missing_entities = []

                for expected in test_case['expected_entities']:
                    if any(expected in extracted or extracted in expected for extracted in extracted_names):
                        found_entities.append(expected)
                    else:
                        missing_entities.append(expected)

                # è¾“å‡ºç»“æœ
                print(f"âœ… å“åº”æ—¶é—´: {response_time:.2f}ç§’")
                print(f"ğŸ“Š æå–åˆ° {len(attributes)} ä¸ªå±æ€§:")

                for attr in attributes:
                    confidence_bar = "â–ˆ" * int(attr['confidence'] * 5) + "â–‘" * (5 - int(attr['confidence'] * 5))
                    print(f"  â€¢ {attr['name']} [{attr['type']}] {confidence_bar} {attr['confidence']:.2f}")

                # åˆ†æè¦†ç›–ç‡
                coverage = len(found_entities) / len(test_case['expected_entities']) * 100
                print(f"ğŸ¯ å®ä½“è¦†ç›–ç‡: {coverage:.1f}% ({len(found_entities)}/{len(test_case['expected_entities'])})")

                if missing_entities:
                    print(f"âš ï¸ æœªæå–åˆ°: {', '.join(missing_entities)}")

                # è®°å½•ç»“æœ
                results.append({
                    "test_case": test_case['name'],
                    "query": test_case['query'],
                    "extracted_count": len(attributes),
                    "expected_count": len(test_case['expected_entities']),
                    "coverage": coverage,
                    "response_time": response_time,
                    "attributes": attributes
                })

            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
                results.append({
                    "test_case": test_case['name'],
                    "query": test_case['query'],
                    "error": str(e)
                })

        # æ€»ç»“æŠ¥å‘Š
        print("\n" + "=" * 50)
        print("ğŸ“ˆ æµ‹è¯•æ€»ç»“æŠ¥å‘Š")
        print("=" * 50)

        successful_tests = [r for r in results if 'error' not in r]
        failed_tests = [r for r in results if 'error' in r]

        if successful_tests:
            avg_coverage = sum(r['coverage'] for r in successful_tests) / len(successful_tests)
            avg_response_time = sum(r['response_time'] for r in successful_tests) / len(successful_tests)
            total_extracted = sum(r['extracted_count'] for r in successful_tests)

            print(f"âœ… æˆåŠŸæµ‹è¯•: {len(successful_tests)}/{len(test_cases)}")
            print(f"ğŸ“Š å¹³å‡å®ä½“è¦†ç›–ç‡: {avg_coverage:.1f}%")
            print(f"â±ï¸ å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ç§’")
            print(f"ğŸ” æ€»æå–å®ä½“æ•°: {total_extracted}")

        if failed_tests:
            print(f"âŒ å¤±è´¥æµ‹è¯•: {len(failed_tests)}")
            for failed in failed_tests:
                print(f"  â€¢ {failed['test_case']}: {failed['error']}")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        save_test_results(results)

    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•åˆå§‹åŒ–å¤±è´¥: {e}")


def save_test_results(results: List[Dict[str, Any]]):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
    import json
    import os
    from datetime import datetime

    # åˆ›å»ºresultsç›®å½•
    os.makedirs("test_results", exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"test_results/entity_extraction_{timestamp}.json"

    # ä¿å­˜ç»“æœ
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {filename}")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--integration":
        # è¿è¡Œé›†æˆæµ‹è¯•
        asyncio.run(run_integration_tests())
    else:
        # è¿è¡Œå•å…ƒæµ‹è¯•
        pytest.main([__file__])