"""
å…³é”®è¯æå–æµ‹è¯• - CSV è¾“å‡ºæ¨¡å¼

ä»…ä½¿ç”¨é LLM æ¨¡å¼ï¼ˆtokenizer æ¨¡å¼ï¼‰æå–å…³é”®è¯ï¼Œå¹¶å°†ç»“æœä¿å­˜ä¸º CSV æ–‡ä»¶ã€‚

è¿è¡Œæ–¹å¼ï¼š
    python tests/ai/test_tokenizer_entities_csv.py
"""

import csv
import os
import sys
import time

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from dataflow.core.ai.tokensize import get_keyword_extractor


def read_test_file(file_path):
    """è¯»å–æµ‹è¯•æ–‡ä»¶å†…å®¹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return None


def save_keyword_counts_to_csv(keyword_counts, output_file):
    """å°†å…³é”®è¯é¢‘æ¬¡ä¿å­˜ä¸º CSV æ ¼å¼"""
    try:
        with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
            writer = csv.writer(csvfile)
            # å†™å…¥è¡¨å¤´
            writer.writerow(['å…³é”®è¯', 'å‡ºç°é¢‘æ¬¡'])

            # å†™å…¥æ•°æ®ï¼ˆæŒ‰é¢‘æ¬¡é™åºæ’åºï¼‰
            for keyword, count in sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True):
                writer.writerow([keyword, count])

        print(f"âœ… å…³é”®è¯é¢‘æ¬¡å·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜ CSV å¤±è´¥: {e}")


def split_into_paragraphs(text):
    """å°†æ–‡æœ¬æ‹†åˆ†ä¸ºæ®µè½"""
    # æŒ‰ç©ºè¡Œåˆ†å‰²æ®µè½
    paragraphs = []
    current_paragraph = []

    for line in text.split('\n'):
        line = line.strip()
        if line:
            current_paragraph.append(line)
        else:
            if current_paragraph:
                paragraphs.append(' '.join(current_paragraph))
                current_paragraph = []

    if current_paragraph:
        paragraphs.append(' '.join(current_paragraph))

    return paragraphs


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 70)
    print("å…³é”®è¯æå– - CSV è¾“å‡ºæµ‹è¯•")
    print("ä»…ä½¿ç”¨ tokenizer æ¨¡å¼ï¼ˆé LLM æ¨¡å¼ï¼‰")
    print("=" * 70)

    # å¼€å§‹è®¡æ—¶
    start_time = time.time()

    # è·å–æå–å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    extractor = get_keyword_extractor()

    # æµ‹è¯•æ–‡ä»¶è·¯å¾„
    test_file = "tests/load/fixtures/LLM_Architecture.md"
    output_file = "test_tokenizer_keywords_output.csv"

    # è¯»å–æµ‹è¯•æ–‡ä»¶
    print(f"\nğŸ“– æ­£åœ¨è¯»å–æ–‡ä»¶: {test_file}")
    read_start = time.time()
    text = read_test_file(test_file)
    read_time = time.time() - read_start

    if not text:
        print("âŒ æ— æ³•è¯»å–æµ‹è¯•æ–‡ä»¶")
        return

    print(f"âœ… æ–‡ä»¶è¯»å–æˆåŠŸï¼Œå…± {len(text)} ä¸ªå­—ç¬¦ (è€—æ—¶: {read_time:.3f}s)")

    # å°†æ–‡æœ¬æ‹†åˆ†ä¸ºæ®µè½
    split_start = time.time()
    paragraphs = split_into_paragraphs(text)
    split_time = time.time() - split_start
    print(f"âœ… å…±æ‹†åˆ†ä¸º {len(paragraphs)} ä¸ªæ®µè½ (è€—æ—¶: {split_time:.3f}s)")

    # æå–å…³é”®è¯å¹¶ç»Ÿè®¡é¢‘æ¬¡
    print("\nğŸ” å¼€å§‹æå–å…³é”®è¯...")
    extract_start = time.time()
    keyword_counts = {}
    total_keywords = 0

    for idx, paragraph in enumerate(paragraphs, 1):
        if not paragraph.strip():
            continue

        # åªä½¿ç”¨ tokenizer æ¨¡å¼æå–å…³é”®è¯
        keywords = extractor.extract(paragraph)

        # ç»Ÿè®¡å…³é”®è¯é¢‘æ¬¡
        for keyword in keywords:
            keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
            total_keywords += 1

        # æ˜¾ç¤ºè¿›åº¦
        if idx % 10 == 0:
            print(f"  å·²å¤„ç† {idx}/{len(paragraphs)} ä¸ªæ®µè½...", end="\r")

    extract_time = time.time() - extract_start
    print(f"\nâœ… å…³é”®è¯æå–å®Œæˆ")
    print(f"  - å¤„ç†æ®µè½æ•°: {len(paragraphs)}")
    print(f"  - æå–å…³é”®è¯æ€»æ•°: {total_keywords}")
    print(f"  - å”¯ä¸€å…³é”®è¯æ•°: {len(keyword_counts)}")
    print(f"  - æå–è€—æ—¶: {extract_time:.3f}s")

    # å°†ç»“æœä¿å­˜ä¸º CSV
    print("\nğŸ’¾ ä¿å­˜ç»“æœåˆ° CSV æ–‡ä»¶...")
    save_start = time.time()
    save_keyword_counts_to_csv(keyword_counts, output_file)
    save_time = time.time() - save_start
    print(f"  - ä¿å­˜è€—æ—¶: {save_time:.3f}s")

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 70)
    print("ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - æ€»æ®µè½æ•°: {len(paragraphs)}")
    print(f"  - æ€»å…³é”®è¯æ•°: {total_keywords}")
    print(f"  - å”¯ä¸€å…³é”®è¯æ•°: {len(keyword_counts)}")
    print(f"  - å¹³å‡æ¯æ®µè½å…³é”®è¯æ•°: {total_keywords / len(paragraphs):.2f}")
    print("=" * 70)

    # æ˜¾ç¤ºå‰20ä¸ªé«˜é¢‘å…³é”®è¯
    sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)

    print("\nTop 20 é«˜é¢‘å…³é”®è¯:")
    for i, (keyword, count) in enumerate(sorted_keywords[:20], 1):
        print(f"  {i:2d}. {keyword} ({count} æ¬¡)")
    print("=" * 70)

    # æ€»è€—æ—¶
    total_time = time.time() - start_time
    print(f"\nâ±ï¸  æ€»è€—æ—¶: {total_time:.3f}s")
    print("=" * 70)


if __name__ == "__main__":
    main()
