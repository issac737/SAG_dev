"""
Sumy å„ç®—æ³•æ¨¡å—æ€§èƒ½æµ‹è¯•å’Œå…³é”®è¯ç»Ÿè®¡

æµ‹è¯•å†…å®¹ï¼š
1. LexRank ç®—æ³•ï¼ˆåŸºäºå›¾çš„æ–¹æ³•ï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
2. Luhn ç®—æ³•ï¼ˆåŸºäºè¯é¢‘å’Œå¥å­èšç±»ï¼‰
3. TextRank ç®—æ³•ï¼ˆåŸºäº PageRank çš„å›¾ç®—æ³•ï¼‰
4. Latent Semantic Analysis (LSA) ç®—æ³•ï¼ˆåŸºäºå¥‡å¼‚å€¼åˆ†è§£ï¼‰
5. KL-Sum ç®—æ³•ï¼ˆåŸºäº KL æ•£åº¦çš„æ–¹æ³•ï¼‰

æ€§èƒ½æŒ‡æ ‡ï¼š
- ç®—æ³•æ‰§è¡Œè€—æ—¶
- æå–çš„å…³é”®å¥å­æ•°é‡
- å…³é”®è¯ç»Ÿè®¡å’Œåˆ†æ

ä½¿ç”¨æ–¹æ³•:
    # ä½¿ç”¨å†…ç½®ç¤ºä¾‹æ–‡æœ¬
    python test_sumy_algorithms_benchmark.py

    # ä½¿ç”¨æŒ‡å®šæ–‡ä»¶
    python test_sumy_algorithms_benchmark.py path/to/file.txt

    # æŒ‡å®šæå–çš„å¥å­æ•°é‡
    python test_sumy_algorithms_benchmark.py path/to/file.txt 10
"""

import sys
import time
from pathlib import Path
from collections import Counter
from typing import List, Dict, Tuple
import re

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from sumy.nlp.tokenizers import Tokenizer
from sumy.parsers.plaintext import PlaintextParser
from sumy.summarizers.lex_rank import LexRankSummarizer
from sumy.summarizers.luhn import LuhnSummarizer
from sumy.summarizers.text_rank import TextRankSummarizer
from sumy.summarizers.lsa import LsaSummarizer
from sumy.summarizers.kl import KLSummarizer
from sumy.nlp.stemmers import Stemmer
from sumy.utils import get_stop_words

from dataflow.utils import setup_logging

setup_logging()


# å†…ç½®æµ‹è¯•æ–‡æœ¬
BUILTIN_TEXT = """äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼Œç®€ç§°AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œæ—¨åœ¨ç ”ç©¶å’Œå¼€å‘èƒ½å¤Ÿæ¨¡æ‹Ÿã€å»¶ä¼¸å’Œæ‰©å±•äººç±»æ™ºèƒ½çš„ç†è®ºã€æ–¹æ³•ã€æŠ€æœ¯åŠåº”ç”¨ç³»ç»Ÿã€‚

è‡ª20ä¸–çºª50å¹´ä»£è¯ç”Ÿä»¥æ¥ï¼Œäººå·¥æ™ºèƒ½ç»å†äº†å¤šæ¬¡å‘å±•é«˜æ½®å’Œä½è°·ã€‚è¿‘å¹´æ¥ï¼Œéšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„çªç ´ï¼ŒAIåœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†å·¨å¤§è¿›å±•ã€‚

æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒé€šè¿‡æ„å»ºå¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„é«˜çº§ç‰¹å¾è¡¨ç¤ºã€‚æ·±åº¦å­¦ä¹ çš„æˆåŠŸå¾—ç›Šäºä¸‰ä¸ªå…³é”®å› ç´ ï¼šå¤§è§„æ¨¡æ•°æ®é›†çš„å¯ç”¨æ€§ã€å¼ºå¤§çš„è®¡ç®—èƒ½åŠ›ï¼ˆç‰¹åˆ«æ˜¯GPUçš„åº”ç”¨ï¼‰ï¼Œä»¥åŠæ”¹è¿›çš„ç®—æ³•å’Œç½‘ç»œæ¶æ„ã€‚

åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸï¼ŒTransformeræ¶æ„çš„æå‡ºæ ‡å¿—ç€ä¸€ä¸ªé‡è¦çš„é‡Œç¨‹ç¢‘ã€‚åŸºäºTransformerçš„æ¨¡å‹å¦‚BERTã€GPTç³»åˆ—åœ¨å„ç§NLPä»»åŠ¡ä¸Šåˆ·æ–°äº†æ€§èƒ½è®°å½•ã€‚è¿™äº›å¤§è¯­è¨€æ¨¡å‹ä¸ä»…èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆæ–‡æœ¬ï¼Œè¿˜å±•ç°å‡ºäº†ä»¤äººæƒŠè®¶çš„æ¨ç†å’Œé—®é¢˜è§£å†³èƒ½åŠ›ã€‚

ç„¶è€Œï¼Œäººå·¥æ™ºèƒ½çš„å‘å±•ä¹Ÿå¸¦æ¥äº†ä¸€äº›æŒ‘æˆ˜å’Œæ‹…å¿§ã€‚ç®—æ³•åè§ã€éšç§ä¿æŠ¤ã€å°±ä¸šå½±å“ã€AIå®‰å…¨ç­‰é—®é¢˜éœ€è¦è¢«è®¤çœŸå¯¹å¾…ã€‚ç ”ç©¶è€…å’Œæ”¿ç­–åˆ¶å®šè€…æ­£åœ¨åŠªåŠ›åˆ¶å®šç›¸å…³çš„ä¼¦ç†å‡†åˆ™å’Œç›‘ç®¡æ¡†æ¶ã€‚

å±•æœ›æœªæ¥ï¼Œäººå·¥æ™ºèƒ½å°†ç»§ç»­å¿«é€Ÿå‘å±•ï¼Œå¹¶åœ¨åŒ»ç–—ã€æ•™è‚²ã€äº¤é€šã€é‡‘èç­‰å„ä¸ªé¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚é€šç”¨äººå·¥æ™ºèƒ½ï¼ˆAGIï¼‰çš„å®ç°ä»ç„¶æ˜¯ä¸€ä¸ªé•¿æœŸç›®æ ‡ï¼Œä½†å³ä½¿æ˜¯å½“å‰çš„å¼±äººå·¥æ™ºèƒ½æŠ€æœ¯ä¹Ÿå·²ç»åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ã€‚

äººå·¥æ™ºèƒ½çš„å‘å±•éœ€è¦è·¨å­¦ç§‘çš„åˆä½œï¼ŒåŒ…æ‹¬è®¡ç®—æœºç§‘å­¦ã€æ•°å­¦ã€è®¤çŸ¥ç§‘å­¦ã€ç¥ç»ç§‘å­¦ã€è¯­è¨€å­¦ç­‰å¤šä¸ªé¢†åŸŸã€‚åªæœ‰é€šè¿‡æŒç»­çš„ç ”ç©¶å’Œè´Ÿè´£ä»»çš„åº”ç”¨ï¼Œæˆ‘ä»¬æ‰èƒ½å……åˆ†å‘æŒ¥AIæŠ€æœ¯çš„æ½œåŠ›ï¼ŒåŒæ—¶é¿å…å…¶æ½œåœ¨çš„è´Ÿé¢å½±å“ã€‚

æœºå™¨å­¦ä¹ ä½œä¸ºäººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ç­‰å¤šç§æ–¹æ³•ã€‚ç›‘ç£å­¦ä¹ é€šè¿‡æ ‡æ³¨æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œæ— ç›‘ç£å­¦ä¹ ä»æœªæ ‡æ³¨æ•°æ®ä¸­å‘ç°æ¨¡å¼ï¼Œå¼ºåŒ–å­¦ä¹ åˆ™é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚

è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„å¦ä¸€ä¸ªé‡è¦åº”ç”¨é¢†åŸŸã€‚é€šè¿‡å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç­‰æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œè®¡ç®—æœºå·²ç»èƒ½å¤Ÿåœ¨å›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²ç­‰ä»»åŠ¡ä¸Šè¾¾åˆ°ç”šè‡³è¶…è¶Šäººç±»çš„è¡¨ç°ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚ä»æ—©æœŸçš„è§„åˆ™ç³»ç»Ÿåˆ°ç°åœ¨çš„å¤§è¯­è¨€æ¨¡å‹ï¼ŒNLPæŠ€æœ¯ç»å†äº†é©å‘½æ€§çš„å‘å±•ã€‚ç°ä»£NLPç³»ç»Ÿå¯ä»¥è¿›è¡Œæœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æã€é—®ç­”ç³»ç»Ÿã€æ–‡æœ¬æ‘˜è¦ç­‰å¤šç§ä»»åŠ¡ã€‚"""


class AlgorithmBenchmark:
    """ç®—æ³•æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""

    def __init__(self, text: str, language: str = "chinese", sentence_count: int = 5):
        """
        åˆå§‹åŒ–åŸºå‡†æµ‹è¯•

        Args:
            text: æµ‹è¯•æ–‡æœ¬
            language: è¯­è¨€ç±»å‹ï¼ˆchinese/englishï¼‰
            sentence_count: è¦æå–çš„å¥å­æ•°é‡
        """
        self.text = text
        self.language = language
        self.sentence_count = sentence_count
        self.parser = PlaintextParser.from_string(text, Tokenizer(language))
        self.stemmer = Stemmer(language)
        self.stop_words = get_stop_words(language)

        # ç»Ÿè®¡åŸæ–‡ä¿¡æ¯
        self.char_count = len(text)
        self.total_sentences = len(list(self.parser.document.sentences))

    def extract_keywords(self, sentences: List[str], top_n: int = 20) -> List[Tuple[str, int]]:
        """
        ä»å¥å­ä¸­æå–å…³é”®è¯

        Args:
            sentences: å¥å­åˆ—è¡¨
            top_n: è¿”å›å‰Nä¸ªå…³é”®è¯

        Returns:
            å…³é”®è¯åŠå…¶é¢‘ç‡çš„åˆ—è¡¨
        """
        text = " ".join(sentences)

        # æ ¹æ®è¯­è¨€é€‰æ‹©åˆ†è¯æ–¹å¼
        if self.language == "chinese":
            # ä¸­æ–‡ï¼šæŒ‰å­—ç¬¦åˆ‡åˆ†ï¼ˆç®€å•æ–¹å¼ï¼‰
            words = re.findall(r'[\u4e00-\u9fff]+', text)
            # æå–2-4å­—çš„è¯è¯­
            words = [w for w in words if 2 <= len(w) <= 4]
        else:
            # è‹±æ–‡ï¼šæŒ‰å•è¯åˆ‡åˆ†
            words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
            # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
            words = [w for w in words if len(w) > 3 and w not in self.stop_words]

        # ç»Ÿè®¡è¯é¢‘
        word_counts = Counter(words)
        return word_counts.most_common(top_n)

    def test_lexrank(self) -> Dict:
        """æµ‹è¯• LexRank ç®—æ³•"""
        print(f"\n{'='*70}")
        print("æµ‹è¯• LexRank ç®—æ³•ï¼ˆåŸºäºå›¾çš„æ–¹æ³•ï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰")
        print(f"{'='*70}")

        start_time = time.time()

        summarizer = LexRankSummarizer(self.stemmer)
        summarizer.stop_words = self.stop_words

        summary_sentences = summarizer(
            self.parser.document,
            sentences_count=self.sentence_count
        )

        elapsed_time = time.time() - start_time

        sentences = [str(s) for s in summary_sentences]
        keywords = self.extract_keywords(sentences)

        # è¾“å‡ºç»“æœ
        print(f"\nâ±ï¸  æ‰§è¡Œæ—¶é—´: {elapsed_time:.4f} ç§’")
        print(f"ğŸ“ æå–å¥å­æ•°: {len(sentences)}/{self.sentence_count}")

        print(f"\næå–çš„å…³é”®å¥å­:")
        for i, sentence in enumerate(sentences, 1):
            print(f"  {i}. {sentence[:100]}{'...' if len(sentence) > 100 else ''}")

        print(f"\nğŸ”‘ Top 10 å…³é”®è¯:")
        for word, count in keywords[:10]:
            print(f"  {word}: {count}")

        return {
            "algorithm": "LexRank",
            "time": elapsed_time,
            "sentences": sentences,
            "keywords": keywords
        }

    def test_luhn(self) -> Dict:
        """æµ‹è¯• Luhn ç®—æ³•"""
        print(f"\n{'='*70}")
        print("æµ‹è¯• Luhn ç®—æ³•ï¼ˆåŸºäºè¯é¢‘å’Œå¥å­èšç±»ï¼‰")
        print(f"{'='*70}")

        start_time = time.time()

        summarizer = LuhnSummarizer(self.stemmer)
        summarizer.stop_words = self.stop_words

        summary_sentences = summarizer(
            self.parser.document,
            sentences_count=self.sentence_count
        )

        elapsed_time = time.time() - start_time

        sentences = [str(s) for s in summary_sentences]
        keywords = self.extract_keywords(sentences)

        # è¾“å‡ºç»“æœ
        print(f"\nâ±ï¸  æ‰§è¡Œæ—¶é—´: {elapsed_time:.4f} ç§’")
        print(f"ğŸ“ æå–å¥å­æ•°: {len(sentences)}/{self.sentence_count}")

        print(f"\næå–çš„å…³é”®å¥å­:")
        for i, sentence in enumerate(sentences, 1):
            print(f"  {i}. {sentence[:100]}{'...' if len(sentence) > 100 else ''}")

        print(f"\nğŸ”‘ Top 10 å…³é”®è¯:")
        for word, count in keywords[:10]:
            print(f"  {word}: {count}")

        return {
            "algorithm": "Luhn",
            "time": elapsed_time,
            "sentences": sentences,
            "keywords": keywords
        }

    def test_textrank(self) -> Dict:
        """æµ‹è¯• TextRank ç®—æ³•"""
        print(f"\n{'='*70}")
        print("æµ‹è¯• TextRank ç®—æ³•ï¼ˆåŸºäº PageRank çš„å›¾ç®—æ³•ï¼‰")
        print(f"{'='*70}")

        start_time = time.time()

        summarizer = TextRankSummarizer(self.stemmer)
        summarizer.stop_words = self.stop_words

        summary_sentences = summarizer(
            self.parser.document,
            sentences_count=self.sentence_count
        )

        elapsed_time = time.time() - start_time

        sentences = [str(s) for s in summary_sentences]
        keywords = self.extract_keywords(sentences)

        # è¾“å‡ºç»“æœ
        print(f"\nâ±ï¸  æ‰§è¡Œæ—¶é—´: {elapsed_time:.4f} ç§’")
        print(f"ğŸ“ æå–å¥å­æ•°: {len(sentences)}/{self.sentence_count}")

        print(f"\næå–çš„å…³é”®å¥å­:")
        for i, sentence in enumerate(sentences, 1):
            print(f"  {i}. {sentence[:100]}{'...' if len(sentence) > 100 else ''}")

        print(f"\nğŸ”‘ Top 10 å…³é”®è¯:")
        for word, count in keywords[:10]:
            print(f"  {word}: {count}")

        return {
            "algorithm": "TextRank",
            "time": elapsed_time,
            "sentences": sentences,
            "keywords": keywords
        }

    def test_lsa(self) -> Dict:
        """æµ‹è¯• LSA ç®—æ³•"""
        print(f"\n{'='*70}")
        print("æµ‹è¯• LSA ç®—æ³•ï¼ˆLatent Semantic Analysis - åŸºäºå¥‡å¼‚å€¼åˆ†è§£ï¼‰")
        print(f"{'='*70}")

        start_time = time.time()

        summarizer = LsaSummarizer(self.stemmer)
        summarizer.stop_words = self.stop_words

        summary_sentences = summarizer(
            self.parser.document,
            sentences_count=self.sentence_count
        )

        elapsed_time = time.time() - start_time

        sentences = [str(s) for s in summary_sentences]
        keywords = self.extract_keywords(sentences)

        # è¾“å‡ºç»“æœ
        print(f"\nâ±ï¸  æ‰§è¡Œæ—¶é—´: {elapsed_time:.4f} ç§’")
        print(f"ğŸ“ æå–å¥å­æ•°: {len(sentences)}/{self.sentence_count}")

        print(f"\næå–çš„å…³é”®å¥å­:")
        for i, sentence in enumerate(sentences, 1):
            print(f"  {i}. {sentence[:100]}{'...' if len(sentence) > 100 else ''}")

        print(f"\nğŸ”‘ Top 10 å…³é”®è¯:")
        for word, count in keywords[:10]:
            print(f"  {word}: {count}")

        return {
            "algorithm": "LSA",
            "time": elapsed_time,
            "sentences": sentences,
            "keywords": keywords
        }

    def run_all_tests(self) -> List[Dict]:
        """è¿è¡Œæ‰€æœ‰ç®—æ³•æµ‹è¯•"""
        print(f"\n{'#'*70}")
        print("Sumy ç®—æ³•æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print(f"{'#'*70}")
        print(f"\næ–‡æœ¬ä¿¡æ¯:")
        print(f"  å­—ç¬¦æ•°: {self.char_count}")
        print(f"  å¥å­æ•°: {self.total_sentences}")
        print(f"  è¯­è¨€: {self.language}")
        print(f"  æå–å¥å­æ•°: {self.sentence_count}")

        results = []

        # æµ‹è¯•æ‰€æœ‰ç®—æ³•
        results.append(self.test_luhn())
        results.append(self.test_textrank())

        # è¾“å‡ºæ€§èƒ½å¯¹æ¯”
        self.print_performance_comparison(results)

        return results

    def print_performance_comparison(self, results: List[Dict]):
        """è¾“å‡ºæ€§èƒ½å¯¹æ¯”è¡¨"""
        print(f"\n{'='*70}")
        print("æ€§èƒ½å¯¹æ¯”æ€»ç»“")
        print(f"{'='*70}")

        # æŒ‰æ‰§è¡Œæ—¶é—´æ’åº
        sorted_results = sorted(results, key=lambda x: x['time'])

        print(f"\nâ±ï¸  æ‰§è¡Œæ—¶é—´æ’åï¼ˆä»å¿«åˆ°æ…¢ï¼‰:")
        for i, result in enumerate(sorted_results, 1):
            print(f"  {i}. {result['algorithm']:15s} - {result['time']:.4f} ç§’")

        print(f"\nğŸ“Š ç®—æ³•ç‰¹ç‚¹æ€»ç»“:")
        print(f"  LexRank:  åŸºäºå›¾çš„æ–¹æ³•ï¼Œè®¡ç®—å¥å­é—´ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œé€‚åˆå¤šæ–‡æ¡£æ‘˜è¦")
        print(f"  Luhn:     åŸºäºè¯é¢‘ç»Ÿè®¡ï¼Œé€Ÿåº¦å¿«ï¼Œé€‚åˆå¿«é€Ÿæ‘˜è¦")
        print(f"  TextRank: åŸºäº PageRankï¼Œè€ƒè™‘å¥å­é‡è¦æ€§ä¼ æ’­")
        print(f"  LSA:      åŸºäºçŸ©é˜µåˆ†è§£ï¼Œæ•æ‰æ½œåœ¨è¯­ä¹‰å…³ç³»")
        print(f"  KL-Sum:   åŸºäº KL æ•£åº¦ï¼Œé€‰æ‹©ä¿¡æ¯é‡æœ€å¤§çš„å¥å­")

        # å…³é”®è¯é‡å åˆ†æ
        print(f"\nğŸ”‘ å…³é”®è¯ç»Ÿè®¡:")
        for result in results:
            top_5_keywords = [w for w, _ in result['keywords'][:5]]
            print(f"  {result['algorithm']:15s}: {', '.join(top_5_keywords)}")


def detect_language(text: str) -> str:
    """è‡ªåŠ¨æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    total_chars = chinese_chars + english_chars

    if total_chars == 0:
        return "english"

    chinese_ratio = chinese_chars / total_chars
    return "chinese" if chinese_ratio > 0.3 else "english"


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) == 1:
        # ä½¿ç”¨å†…ç½®æ–‡æœ¬
        text = BUILTIN_TEXT
        sentence_count = 3
        print("\nä½¿ç”¨å†…ç½®ç¤ºä¾‹æ–‡æœ¬")
    else:
        # ä»æ–‡ä»¶è¯»å–
        file_path = sys.argv[1]
        sentence_count = int(sys.argv[2]) if len(sys.argv) > 2 else 500

        print(f"\nä»æ–‡ä»¶è¯»å–: {file_path}")
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()

    # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
    language = detect_language(text)
    print(f"æ£€æµ‹åˆ°è¯­è¨€: {language}")

    # åˆ›å»ºåŸºå‡†æµ‹è¯•å®ä¾‹
    benchmark = AlgorithmBenchmark(
        text=text,
        language=language,
        sentence_count=sentence_count
    )

    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    benchmark.run_all_tests()

    print(f"\n{'='*70}")
    print("æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\næµ‹è¯•è¢«ä¸­æ–­")
    except Exception as e:
        print(f"\n\næµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
