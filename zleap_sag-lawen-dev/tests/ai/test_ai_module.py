"""
AI æ¨¡å—ç»¼åˆæµ‹è¯•è„šæœ¬

æµ‹è¯•å†…å®¹:
- æµå¼è¾“å‡º vs éæµå¼è¾“å‡º
- å¸¦é‡è¯• vs ä¸å¸¦é‡è¯•
- æ¸©åº¦ç­‰é…ç½®å‚æ•°
- è§’è‰²å®šä¹‰ (system, user, assistant)
- ä»»åŠ¡æ‰§è¡Œ
- JSON è¾“å‡ºéªŒè¯

é…ç½®æ–¹å¼:
    1. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶:
       LLM_API_KEY=sk-your-api-key
       LLM_MODEL=sophnet/Qwen3-30B-A3B-Thinking-2507
       LLM_BASE_URL=https://your-proxy-api.com/v1  # å¯é€‰ï¼Œä¸­è½¬API

    2. æˆ–è®¾ç½®ç¯å¢ƒå˜é‡:
       export LLM_API_KEY='your-api-key'
       export LLM_MODEL='sophnet/Qwen3-30B-A3B-Thinking-2507'
       export LLM_BASE_URL='https://your-proxy-api.com/v1'

è¿è¡Œæ–¹å¼:
    python tests/test_ai_module.py

æ³¨æ„: factory.create_llm_client() ä¼šè‡ªåŠ¨ä» settings è¯»å– .env é…ç½®
"""

import asyncio
import json
import os
import sys
from typing import Any, Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dataflow.core.ai.factory import create_llm_client
from dataflow.core.ai.models import LLMMessage, LLMRole
from dataflow.exceptions import LLMError, LLMTimeoutError, LLMRateLimitError


class Colors:
    """ç»ˆç«¯é¢œè‰²"""

    HEADER = "\033[95m"
    OKBLUE = "\033[94m"
    OKCYAN = "\033[96m"
    OKGREEN = "\033[92m"
    WARNING = "\033[93m"
    FAIL = "\033[91m"
    ENDC = "\033[0m"
    BOLD = "\033[1m"


def print_section(title: str) -> None:
    """æ‰“å°æµ‹è¯•ç« èŠ‚æ ‡é¢˜"""
    print(f"\n{Colors.HEADER}{Colors.BOLD}{'=' * 70}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{title:^70}{Colors.ENDC}")
    print(f"{Colors.HEADER}{Colors.BOLD}{'=' * 70}{Colors.ENDC}\n")


def print_success(message: str) -> None:
    """æ‰“å°æˆåŠŸæ¶ˆæ¯"""
    print(f"{Colors.OKGREEN}âœ“ {message}{Colors.ENDC}")


def print_error(message: str) -> None:
    """æ‰“å°é”™è¯¯æ¶ˆæ¯"""
    print(f"{Colors.FAIL}âœ— {message}{Colors.ENDC}")


def print_info(message: str) -> None:
    """æ‰“å°ä¿¡æ¯æ¶ˆæ¯"""
    print(f"{Colors.OKCYAN}â„¹ {message}{Colors.ENDC}")


def print_warning(message: str) -> None:
    """æ‰“å°è­¦å‘Šæ¶ˆæ¯"""
    print(f"{Colors.WARNING}âš  {message}{Colors.ENDC}")


async def test_basic_chat() -> bool:
    """æµ‹è¯• 1: åŸºç¡€èŠå¤©åŠŸèƒ½"""
    print_section("æµ‹è¯• 1: åŸºç¡€èŠå¤©åŠŸèƒ½")

    try:
        # åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä¸å¸¦é‡è¯•ï¼‰
        client = create_llm_client(with_retry=False)
        print_info("åˆ›å»º LLM å®¢æˆ·ç«¯ï¼ˆä¸å¸¦é‡è¯•ï¼‰")

        # å‡†å¤‡æ¶ˆæ¯
        messages = [
            LLMMessage(role=LLMRole.SYSTEM, content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"),
            LLMMessage(role=LLMRole.USER, content="è¯·ç”¨ä¸€å¥è¯ä»‹ç» Pythonã€‚"),
        ]
        print_info(f"å‘é€ {len(messages)} æ¡æ¶ˆæ¯")

        # è°ƒç”¨ API
        response = await client.chat(messages)

        # éªŒè¯å“åº”
        print_success("æˆåŠŸè·å–å“åº”")
        print(f"  æ¨¡å‹: {response.model}")
        print(f"  å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
        print(f"  ä½¿ç”¨ token: {response.total_tokens}")
        print(f"  ç»“æŸåŸå› : {response.finish_reason}")
        print(f"\n  å“åº”å†…å®¹:\n  {response.content[:200]}...")

        return True

    except Exception as e:
        print_error(f"æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_streaming_chat() -> bool:
    """æµ‹è¯• 2: æµå¼è¾“å‡º"""
    print_section("æµ‹è¯• 2: æµå¼è¾“å‡º")

    try:
        # åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆä¸å¸¦é‡è¯•ï¼‰
        client = create_llm_client(with_retry=False)
        print_info("åˆ›å»º LLM å®¢æˆ·ç«¯ï¼ˆæµå¼æ¨¡å¼ï¼‰")

        # å‡†å¤‡æ¶ˆæ¯
        messages = [
            LLMMessage(role=LLMRole.SYSTEM, content="ä½ æ˜¯ä¸€ä¸ªè¯—äººã€‚"),
            LLMMessage(role=LLMRole.USER, content="å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„çŸ­è¯—ï¼ˆ4è¡Œï¼‰ã€‚"),
        ]
        print_info("å‘é€æµå¼è¯·æ±‚")

        # æµå¼è°ƒç”¨
        chunks = []
        print("\n  æµå¼è¾“å‡º: ", end="", flush=True)

        async for chunk in client.chat_stream(messages):
            print(chunk, end="", flush=True)
            chunks.append(chunk)

        print()  # æ¢è¡Œ

        full_content = "".join(chunks)
        print_success(f"æˆåŠŸæ¥æ”¶ {len(chunks)} ä¸ªæµå¼ç‰‡æ®µ")
        print(f"  æ€»é•¿åº¦: {len(full_content)} å­—ç¬¦")

        return True

    except Exception as e:
        print_error(f"æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_with_retry() -> bool:
    """æµ‹è¯• 3: å¸¦é‡è¯•æœºåˆ¶"""
    print_section("æµ‹è¯• 3: å¸¦é‡è¯•æœºåˆ¶")

    try:
        # åˆ›å»ºå¸¦é‡è¯•çš„å®¢æˆ·ç«¯
        client = create_llm_client(with_retry=True)
        print_info("åˆ›å»º LLM å®¢æˆ·ç«¯ï¼ˆå¸¦æ™ºèƒ½é‡è¯•ï¼‰")

        # å‡†å¤‡æ¶ˆæ¯
        messages = [
            LLMMessage(role=LLMRole.SYSTEM, content="ä½ æ˜¯ä¸€ä¸ªæ•°å­¦è€å¸ˆã€‚"),
            LLMMessage(role=LLMRole.USER, content="ä»€ä¹ˆæ˜¯æ–æ³¢é‚£å¥‘æ•°åˆ—ï¼Ÿ"),
        ]
        print_info("å‘é€è¯·æ±‚ï¼ˆå¸¦é‡è¯•ä¿æŠ¤ï¼‰")

        # è°ƒç”¨ API
        response = await client.chat(messages)

        print_success("æˆåŠŸè·å–å“åº”")
        print(f"  é‡è¯•é…ç½®: æœ€å¤š {client.max_retries} æ¬¡")
        print(f"  å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")

        # æµ‹è¯•é‡è¯•é€»è¾‘
        print_info("\néªŒè¯é‡è¯•é€»è¾‘:")
        print(f"  è¶…æ—¶é”™è¯¯ä¼šé‡è¯•: {client._should_retry(LLMTimeoutError('test'))}")
        print(f"  é€Ÿç‡é™åˆ¶ä¼šé‡è¯•: {client._should_retry(LLMRateLimitError('test'))}")
        print(f"  LLMé”™è¯¯ä¼šé‡è¯•: {client._should_retry(LLMError('test'))}")

        return True

    except Exception as e:
        print_error(f"æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_temperature_configs() -> bool:
    """æµ‹è¯• 4: æ¸©åº¦ç­‰é…ç½®å‚æ•°"""
    print_section("æµ‹è¯• 4: æ¸©åº¦ç­‰é…ç½®å‚æ•°")

    try:
        client = create_llm_client(with_retry=False)

        # æµ‹è¯•ä¸åŒçš„æ¸©åº¦
        temperatures = [0.0, 0.5, 1.0]

        for temp in temperatures:
            print_info(f"\næµ‹è¯•æ¸©åº¦: {temp}")

            messages = [
                LLMMessage(role=LLMRole.USER, content="è¯´ä¸€ä¸ªè¯æ¥å½¢å®¹å¤©ç©ºçš„é¢œè‰²ã€‚"),
            ]

            response = await client.chat(
                messages,
                temperature=temp,
                max_tokens=50,
            )

            print(f"  å“åº” (temp={temp}): {response.content[:100]}")
            print(f"  ä½¿ç”¨ token: {response.total_tokens}")

        print_success("\næˆåŠŸæµ‹è¯•ä¸åŒæ¸©åº¦é…ç½®")
        return True

    except Exception as e:
        print_error(f"æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_role_definitions() -> bool:
    """æµ‹è¯• 5: è§’è‰²å®šä¹‰"""
    print_section("æµ‹è¯• 5: è§’è‰²å®šä¹‰ (System, User, Assistant)")

    try:
        client = create_llm_client(with_retry=False)
        print_info("æµ‹è¯•å¤šè½®å¯¹è¯ä¸è§’è‰²å®šä¹‰")

        # å¤šè½®å¯¹è¯
        messages = [
            LLMMessage(role=LLMRole.SYSTEM, content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œä¸“æ³¨äº Python ç¼–ç¨‹ã€‚"),
            LLMMessage(role=LLMRole.USER, content="å¦‚ä½•åœ¨ Python ä¸­è¯»å–æ–‡ä»¶ï¼Ÿ"),
            LLMMessage(
                role=LLMRole.ASSISTANT, content="åœ¨ Python ä¸­å¯ä»¥ä½¿ç”¨ open() å‡½æ•°è¯»å–æ–‡ä»¶ã€‚"
            ),
            LLMMessage(role=LLMRole.USER, content="èƒ½ç»™ä¸ªå…·ä½“ä¾‹å­å—ï¼Ÿ"),
        ]

        print_info(f"å‘é€ {len(messages)} æ¡æ¶ˆæ¯ï¼ˆåŒ…å«å†å²å¯¹è¯ï¼‰")
        for i, msg in enumerate(messages, 1):
            print(f"  {i}. [{msg.role.value}] {msg.content[:50]}...")

        response = await client.chat(messages, max_tokens=200)

        print_success("\næˆåŠŸå¤„ç†å¤šè½®å¯¹è¯")
        print(f"  æœ€ç»ˆå“åº”:\n  {response.content[:300]}...")

        return True

    except Exception as e:
        print_error(f"æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_json_output() -> bool:
    """æµ‹è¯• 6: JSON è¾“å‡ºéªŒè¯"""
    print_section("æµ‹è¯• 6: JSON è¾“å‡ºä¸ Schema éªŒè¯")

    try:
        client = create_llm_client(with_retry=False)
        print_info("æµ‹è¯•ç»“æ„åŒ– JSON è¾“å‡º")

        # å®šä¹‰ JSON Schema
        response_schema = {
            "type": "object",
            "properties": {
                "name": {"type": "string"},
                "age": {"type": "number"},
                "skills": {"type": "array", "items": {"type": "string"}},
            },
            "required": ["name", "age", "skills"],
        }

        print_info("JSON Schema:")
        print(f"  {json.dumps(response_schema, indent=2, ensure_ascii=False)}")

        # å‡†å¤‡æ¶ˆæ¯
        messages = [
            LLMMessage(
                role=LLMRole.USER, content="åˆ›å»ºä¸€ä¸ªè™šæ„çš„ç¨‹åºå‘˜æ¡£æ¡ˆï¼ŒåŒ…å«å§“åã€å¹´é¾„å’ŒæŠ€èƒ½åˆ—è¡¨ã€‚"
            ),
        ]

        # è°ƒç”¨å¸¦ Schema çš„æ¥å£
        result = await client.chat_with_schema(
            messages,
            response_schema=response_schema,
            temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„ JSON
        )

        print_success("\næˆåŠŸè·å–ç»“æ„åŒ– JSON è¾“å‡º")
        print(f"  JSON ç»“æœ:")
        print(f"  {json.dumps(result, indent=2, ensure_ascii=False)}")

        # éªŒè¯å­—æ®µ
        assert "name" in result, "ç¼ºå°‘ name å­—æ®µ"
        assert "age" in result, "ç¼ºå°‘ age å­—æ®µ"
        assert "skills" in result, "ç¼ºå°‘ skills å­—æ®µ"
        assert isinstance(result["skills"], list), "skills ä¸æ˜¯åˆ—è¡¨"

        print_success("JSON å­—æ®µéªŒè¯é€šè¿‡")

        return True

    except Exception as e:
        print_error(f"æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_task_execution() -> bool:
    """æµ‹è¯• 7: å¤æ‚ä»»åŠ¡æ‰§è¡Œ"""
    print_section("æµ‹è¯• 7: å¤æ‚ä»»åŠ¡æ‰§è¡Œ")

    try:
        client = create_llm_client(with_retry=True)
        print_info("æµ‹è¯•ä»£ç ç”Ÿæˆä»»åŠ¡")

        messages = [
            LLMMessage(
                role=LLMRole.SYSTEM, content="ä½ æ˜¯ä¸€ä¸ª Python ä¸“å®¶ï¼Œæ“…é•¿ç¼–å†™ç®€æ´é«˜æ•ˆçš„ä»£ç ã€‚"
            ),
            LLMMessage(
                role=LLMRole.USER,
                content="""
å†™ä¸€ä¸ª Python å‡½æ•°æ¥åˆ¤æ–­ä¸€ä¸ªæ•°æ˜¯å¦ä¸ºè´¨æ•°ã€‚
è¦æ±‚:
1. å‡½æ•°åä¸º is_prime
2. åŒ…å«å®Œæ•´çš„æ–‡æ¡£å­—ç¬¦ä¸²
3. å¤„ç†è¾¹ç•Œæƒ…å†µ
4. ä½¿ç”¨é«˜æ•ˆçš„ç®—æ³•
""",
            ),
        ]

        print_info("å‘é€ä»£ç ç”Ÿæˆè¯·æ±‚")

        response = await client.chat(
            messages,
            temperature=0.3,  # ä½æ¸©åº¦ä»¥è·å¾—æ›´å‡†ç¡®çš„ä»£ç 
            max_tokens=500,
        )

        print_success("æˆåŠŸç”Ÿæˆä»£ç ")
        print(f"  å“åº”é•¿åº¦: {len(response.content)} å­—ç¬¦")
        print(f"  ä½¿ç”¨ token: {response.total_tokens}")
        print(f"\n  ç”Ÿæˆçš„ä»£ç :\n{response.content}")

        return True

    except Exception as e:
        print_error(f"æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_error_handling() -> bool:
    """æµ‹è¯• 8: é”™è¯¯å¤„ç†"""
    print_section("æµ‹è¯• 8: é”™è¯¯å¤„ç†")

    try:
        print_info("æµ‹è¯•æ— æ•ˆ API å¯†é’¥")

        # ä½¿ç”¨æ— æ•ˆçš„ API å¯†é’¥
        client = create_llm_client(
            api_key="invalid-key-test",
            with_retry=False,
        )

        messages = [
            LLMMessage(role=LLMRole.USER, content="Hello"),
        ]

        try:
            await client.chat(messages)
            print_error("åº”è¯¥æŠ›å‡ºå¼‚å¸¸ä½†æ²¡æœ‰")
            return False
        except LLMError as e:
            print_success(f"æ­£ç¡®æ•è· LLMError: {str(e)[:100]}")
            return True

    except Exception as e:
        print_error(f"æµ‹è¯•å¤±è´¥: {e}")
        return False


async def run_all_tests() -> None:
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print(f"\n{Colors.BOLD}{Colors.HEADER}")
    print("=" * 70)
    print("DataFlow AI æ¨¡å—ç»¼åˆæµ‹è¯•".center(70))
    print("=" * 70)
    print(f"{Colors.ENDC}\n")

    print_info("é…ç½®æ¥æº: .env æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡")
    print_info("factory.create_llm_client() è‡ªåŠ¨è¯»å– settings é…ç½®")
    print()

    # å°è¯•åˆ›å»ºå®¢æˆ·ç«¯ä»¥éªŒè¯é…ç½®
    try:
        test_client = create_llm_client(with_retry=False)
        print_success("é…ç½®æ£€æŸ¥é€šè¿‡")
    except Exception as e:
        print_error(f"é…ç½®é”™è¯¯: {e}")
        print()
        print_info("è¯·æ£€æŸ¥é…ç½®:")
        print("  1. åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶:")
        print("     LLM_API_KEY=sk-your-api-key")
        print("     LLM_MODEL=sophnet/Qwen3-30B-A3B-Thinking-2507")
        print("  2. æˆ–è®¾ç½®ç¯å¢ƒå˜é‡:")
        print("     export LLM_API_KEY='your-api-key'")
        return

    # è¿è¡Œæµ‹è¯•
    tests = [
        ("åŸºç¡€èŠå¤©åŠŸèƒ½", test_basic_chat),
        ("æµå¼è¾“å‡º", test_streaming_chat),
        ("å¸¦é‡è¯•æœºåˆ¶", test_with_retry),
        ("æ¸©åº¦ç­‰é…ç½®å‚æ•°", test_temperature_configs),
        ("è§’è‰²å®šä¹‰", test_role_definitions),
        ("JSON è¾“å‡ºéªŒè¯", test_json_output),
        ("å¤æ‚ä»»åŠ¡æ‰§è¡Œ", test_task_execution),
        ("é”™è¯¯å¤„ç†", test_error_handling),
    ]

    results = []

    for name, test_func in tests:
        try:
            result = await test_func()
            results.append((name, result))
        except Exception as e:
            print_error(f"æµ‹è¯• '{name}' å‘ç”Ÿå¼‚å¸¸: {e}")
            results.append((name, False))

        # æµ‹è¯•é—´éš”
        await asyncio.sleep(1)

    # æ‰“å°æ€»ç»“
    print_section("æµ‹è¯•æ€»ç»“")

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for name, result in results:
        if result:
            print_success(f"{name}")
        else:
            print_error(f"{name}")

    print(f"\n{Colors.BOLD}æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡{Colors.ENDC}")

    if passed == total:
        print(f"\n{Colors.OKGREEN}{Colors.BOLD}ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼{Colors.ENDC}")
    else:
        print(f"\n{Colors.WARNING}{Colors.BOLD}âš  éƒ¨åˆ†æµ‹è¯•å¤±è´¥{Colors.ENDC}")


def main() -> None:
    """ä¸»å‡½æ•°"""
    try:
        asyncio.run(run_all_tests())
    except KeyboardInterrupt:
        print(f"\n{Colors.WARNING}æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­{Colors.ENDC}")
    except Exception as e:
        print_error(f"æµ‹è¯•è¿è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
