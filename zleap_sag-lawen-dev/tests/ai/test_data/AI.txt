MoE简史
混合专家模型 (MoE) 的理念起源于 1991 年的论文 “Adaptive Mixture of Local Experts”。这个概念与集成学习方法相似，旨在为由多个单独网络组成的系统建立一个监管机制。在这种系统中，每个网络 (被称为“专家”) 处理训练样本的不同子集，专注于输入空间的特定区域。那么，如何选择哪个专家来处理特定的输入呢？这就是门控网络发挥作用的地方，它决定了分配给每个专家的权重。在训练过程中，这些专家和门控网络都同时接受训练，以优化它们的性能和决策能力。

在 2010 至 2015 年间，两个独立的研究领域为混合专家模型 (MoE) 的后续发展做出了显著贡献:

组件专家: 在传统的 MoE 设置中，整个系统由一个门控网络和多个专家组成。在支持向量机 (SVMs) 、高斯过程和其他方法的研究中，MoE 通常被视为整个模型的一部分。然而，“Learning Factored Representations in a Deep Mixture of Experts”一文中探索了将 MoE 作为更深层网络的一个组件。这种方法允许将 MoE 嵌入到多层网络中的某一层，使得模型既大又高效。
条件计算: 传统的神经网络通过每一层处理所有输入数据。在这一时期，Yoshua Bengio 等研究人员开始探索基于输入Token动态激活或停用网络组件的方法。
这些研究的融合促进了在自然语言处理 (NLP) 领域对混合专家模型的探索。特别是在 2017 年，Shazeer 等人将这一概念应用于 137B 的 LSTM (当时被广泛应用于 NLP 的架构)。通过引入稀疏性，这项工作在保持极高规模的同时实现了快速的推理速度，相关架构如下图所示。这项工作主要集中在翻译领域，但面临着如高通信成本和训练不稳定性等多种挑战。
混合专家模型 (MoE) 的引入使得训练具有数千亿甚至万亿参数的模型成为可能，如开源的 1.6 万亿参数的 Switch Transformers 等。这种技术不仅在自然语言处理 (NLP) 领域得到了广泛应用，也开始在计算机视觉领域进行探索。

MoE架构模型
大模型时代，模型规模是提升模型性能的关键因素之一。在有限的计算资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较小的模型效果更佳。

混合专家模型 (MoE) 的一个显著优势是它们能够在远少于稠密模型所需的计算资源下进行有效的预训练。这意味着在相同的计算预算条件下，您可以显著扩大模型或数据集的规模。特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。

那么，究竟什么是一个混合专家模型 (MoE) 呢？作为一种基于 Transformer 架构的模型，混合专家模型主要由两个关键部分组成。

稀疏 MoE 层: 这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构，甚至可以是 MoE 层本身，从而形成层级式的 MoE 结构。
门控网络或路由: 这个部分用于决定哪些Token (token) 被发送到哪个专家。例如，在下图中，“More”这个Token可能被发送到第二个专家，而“Parameters”这个Token被发送到第一个专家。有时，一个Token甚至可以被发送到多个专家。Token的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。
上图来自Switch Transformers论文的 MoE layer图示。总结来说，在混合专家模型 (MoE) 中，是将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个门控网络和若干数量的专家。

尽管混合专家模型 (MoE) 提供了若干显著优势，例如更高效的预训练和与稠密模型相比更快的推理速度，但它们也伴随着一些挑战:

训练挑战: 虽然 MoE 能够实现更高效的计算预训练，但它们在微调阶段往往面临泛化能力不足的问题，长期以来易于引发过拟合现象。
推理挑战: MoE 模型虽然可能拥有大量参数，但在推理过程中只使用其中的一部分，这使得它们的推理速度快于具有相同数量参数的稠密模型。然而，这种模型需要将所有参数加载到内存中，因此对内存的需求非常高。以 Mixtral 8x7B 这样的 MoE 为例，需要足够的 VRAM 来容纳一个 47B 参数的稠密模型。之所以是 47B 而不是 8 x 7B = 56B，是因为在 MoE 模型中，只有 FFN 层被视为独立的专家，而模型的其他参数是共享的。此外，假设每个Token只使用两个专家，那么推理速度 (以 FLOPs 计算) 类似于使用 12B 模型 (而不是 14B 模型)，因为虽然它进行了 2x7B 的矩阵乘法计算，但某些层是共享的。
什么是稀疏性?
稀疏性的概念采用了条件计算的思想。在传统的稠密模型中，所有的参数都会对所有输入数据进行处理。相比之下，稀疏性允许我们仅针对整个系统的某些特定部分执行计算。这意味着并非所有参数都会在处理每个输入时被激活或使用，而是根据输入的特定特征或需求，只有部分参数集合被调用和运行。

让我们深入分析 Shazeer 对混合专家模型 (MoE) 在翻译应用中的贡献。条件计算的概念 (即仅在每个样本的基础上激活网络的不同部分) 使得在不增加额外计算负担的情况下扩展模型规模成为可能。这一策略在每个 MoE 层中实现了数以千计甚至更多的专家的有效利用。

这种稀疏性设置确实带来了一些挑战。例如，在混合专家模型 (MoE) 中，尽管较大的批量大小通常有利于提高性能，但当数据通过激活的专家时，实际的批量大小可能会减少。比如，假设我们的输入批量包含 10 个Token， 可能会有5个Token被路由到同一个专家，而剩下的5个Token分别被路由到不同的专家。这导致了批量大小的不均匀分配和资源利用效率不高的问题。在接下来的部分中，将会介绍让MoE高效运行的其他挑战以及相应的解决方案。

那我们应该如何解决这个问题呢？一个可学习的门控网络 (G) 决定将输入的哪一部分发送给哪些专家 (E):


在这种设置下，虽然所有专家都会对所有输入进行运算，但通过门控网络的输出进行加权乘法操作。但是，如果 G (门控网络的输出) 为 0 会发生什么呢？如果是这种情况，就没有必要计算相应的专家操作，因此我们可以节省计算资源。那么一个典型的门控函数是什么呢？一个典型的门控函数通常是一个带有 softmax 函数的简单的网络。这个网络将学习将输入发送给哪个专家。


Shazeer 等人的工作还探索了其他的门控机制，其中包括带噪声的 Top-K 门控 (Noisy Top-K Gating)。这种门控方法引入了一些可调整的噪声，然后保留前 k 个值。具体来说:

添加一些噪声 
选择保留前 K 个值 
应用 Softmax 函数 
这种稀疏性引入了一些有趣的特性。通过使用较低的 k 值 (例如 1 或 2)，我们可以比激活多个专家时更快地进行训练和推理。为什么不仅选择最顶尖的专家呢？最初的假设是，需要将输入路由到不止一个专家，以便门控学会如何进行有效的路由选择，因此至少需要选择两个专家。

我们为什么要添加噪声呢？这是为了专家间的负载均衡！

MoE中的Token负载均衡
正如之前讨论的，如果所有的Token都被发送到只有少数几个受欢迎的专家，那么训练效率将会降低。在通常的混合专家模型 (MoE) 训练中，门控网络往往倾向于主要激活相同的几个专家。这种情况可能会自我加强，因为受欢迎的专家训练得更快，因此它们更容易被选择。

为了缓解这个问题，引入了一个 辅助损失，旨在鼓励给予所有专家相同的重要性。这个损失确保所有专家接收到大致相等数量的训练样本，从而平衡了专家之间的选择。接下来的部分还将探讨专家容量的概念，它引入了一个关于专家可以处理多少Token的阈值。在 transformers 库中，可以通过 aux_loss 参数来控制辅助损失。

MoE扩展Transformers
Transformer 类模型明确表明，增加参数数量可以提高性能，因此谷歌使用 GShard 尝试将 Transformer 模型的参数量扩展到超过 6000 亿并不令人惊讶。

GShard 将在编码器和解码器中的每个前馈网络 (FFN) 层中的替换为使用 Top-2 门控的混合专家模型 (MoE) 层，GShard 的工作对适用于 MoE 的并行计算模式也做出了重要贡献。下图展示了编码器部分的结构。这种架构对于大规模计算非常有效: 当扩展到多个设备时，MoE 层在不同设备间共享，而其他所有层则在每个设备上复制。

为了保持负载平衡和训练效率，GShard 的作者除了引入了上一节中讨论的类似辅助损失外，还引入了一些关键变化:

随机路由: 在 Top-2 设置中，始终选择排名最高的专家，但第二个专家是根据其权重比例随机选择的。
专家容量: 我们可以设定一个阈值，定义一个专家能处理多少Token。如果两个专家的容量都达到上限，Token就会溢出，并通过残差连接传递到下一层，或在某些情况下被完全丢弃。
专家容量是 MoE 中最重要的概念之一。为什么需要专家容量呢？因为所有张量的形状在编译时是静态确定的，无法提前知道多少Token会分配给每个专家，因此需要一个固定的容量因子。

注意: 在推理过程中，只有部分专家被激活。同时，有些计算过程是共享的，例如自注意力 (self-attention) 机制，它适用于所有Token。这就解释了为什么可以使用相当于 12B 稠密模型的计算资源来运行一个包含 8 个专家的 47B 模型。如果我们采用 Top-2 门控，模型会使用高达 14B 的参数。但是，由于自注意力操作 (专家间共享) 的存在，实际上模型运行时使用的参数数量是 12B。

MoE训练与微调
Switch Transformers
尽管混合专家模型 (MoE) 显示出了很大的潜力，但它们在训练和微调过程中存在稳定性问题。Switch Transformers 是一项非常激动人心的工作，它深入研究了这些话题。作者甚至在 Hugging Face 上发布了一个 1.6 万亿参数的 MoE，拥有 2048 个专家，实现了与 T5-XXL 相比 4 倍的预训练速度提升。

就像在 GShard 中一样，作者用混合专家模型 (MoE) 层替换了前馈网络 (FFN) 层。Switch Transformers 提出了一个 Switch Transformer 层，它接收两个输入 (两个不同的Token) 并拥有四个专家。与最初使用至少两个专家的想法相反，Switch Transformers 采用了简化的单专家策略。这种方法的效果包括:

减少门控网络 (路由) 计算负担
每个专家的批量大小至少可以减半
降低通信成本
保持模型质量
Switch Transformers 也对 专家容量 这个概念进行了研究。


上述建议的容量是将批次中的Token数量均匀分配到各个专家。如果我们使用大于 1 的容量因子，我们为Token分配不完全平衡时提供了一个缓冲。增加容量因子会导致更高的设备间通信成本，因此这是一个需要考虑的权衡。特别值得注意的是，Switch Transformers 在低容量因子 (例如 1 至 1.25) 下表现出色。

Switch Transformer 的作者还重新审视并简化了前面章节中提到的负载均衡损失。在训练期间，对于每个 Switch 层的辅助损失被添加到总模型损失中。这种损失鼓励均匀路由，并可以使用超参数进行加权。

作者还尝试了混合精度的方法，例如用 bfloat16 精度训练专家，同时对其余计算使用全精度进行。较低的精度可以减少处理器间的通信成本、计算成本以及存储张量的内存。然而，在最初的实验中，当专家和门控网络都使用 bfloat16 精度训练时，出现了不稳定的训练现象。这种不稳定性特别是由路由计算引起的，因为路由涉及指数函数等操作，这些操作对精度要求较高。因此，为了保持计算的稳定性和精确性，保持更高的精度是重要的。为了减轻不稳定性，路由过程也使用了全精度。


使用混合精度不会降低模型质量并可实现更快的训练

Switch Transformers 采用了编码器 - 解码器的架构，实现了与 T5 类似的混合专家模型 (MoE) 版本。GLaM 这篇工作探索了如何使用仅为原来 1/3 的计算资源 (因为 MoE 模型在训练时需要的计算量较少，从而能够显著降低碳足迹) 来训练与 GPT-3 质量相匹配的模型来提高这些模型的规模。作者专注于仅解码器 (decoder-only) 的模型以及少样本和单样本评估，而不是微调。他们使用了 Top-2 路由和更大的容量因子。此外，他们探讨了将容量因子作为一个动态度量，根据训练和评估期间所使用的计算量进行调整。

稳定模型训练
之前讨论的平衡损失可能会导致稳定性问题。我们可以使用许多方法来稳定稀疏模型的训练，但这可能会牺牲模型质量。例如，引入 dropout 可以提高稳定性，但会导致模型质量下降。另一方面，增加更多的乘法分量可以提高质量，但会降低模型稳定性。

ST-MoE 引入的 Router z-loss 在保持了模型性能的同时显著提升了训练的稳定性。这种损失机制通过惩罚门控网络输入的较大 logits 来起作用，目的是促使数值的绝对大小保持较小，这样可以有效减少计算中的舍入误差。这一点对于那些依赖指数函数进行计算的门控网络尤其重要。为了深入了解这一机制，建议参考原始论文以获得更全面的细节。

专家学习特点
ST-MoE 的研究者们发现，编码器中不同的专家倾向于专注于特定类型的Token或浅层概念。例如，某些专家可能专门处理标点符号，而其他专家则专注于专有名词等。与此相反，解码器中的专家通常具有较低的专业化程度。

此外，研究者们还对这一模型进行了多语言训练。尽管人们可能会预期每个专家处理一种特定语言，但实际上并非如此。由于Token路由和负载均衡的机制，没有任何专家被特定配置以专门处理某一特定语言。

微调策略
1）增加更多专家可以提升处理样本的效率和加速模型的运算速度，但这些优势随着专家数量的增加而递减 (尤其是当专家数量达到 256 或 512 之后更为明显) 。同时，这也意味着在推理过程中，需要更多的显存来加载整个模型。值得注意的是，Switch Transformers 的研究表明，其在大规模模型中的特性在小规模模型下也同样适用，即便是每层仅包含 2、4 或 8 个专家。

2)稠密模型和稀疏模型在过拟合的动态表现上存在显著差异。稀疏模型更易于出现过拟合现象，因此在处理这些模型时，尝试更强的内部正则化措施是有益的，比如使用更高比例的 dropout。例如，我们可以为稠密层设定一个较低的 dropout 率，而为稀疏层设置一个更高的 dropout 率，以此来优化模型性能。

3)在微调过程中是否使用辅助损失是一个需要决策的问题。ST-MoE 的作者尝试关闭辅助损失，发现即使高达 11% 的Token被丢弃，模型的质量也没有显著受到影响。Token丢弃可能是一种正则化形式，有助于防止过拟合。

Switch Transformers 的作者观察到，在相同的预训练困惑度下，稀疏模型在下游任务中的表现不如对应的稠密模型，特别是在重理解任务 (如 SuperGLUE) 上。另一方面，对于知识密集型任务 (如 TriviaQA)，稀疏模型的表现异常出色。作者还观察到，在微调过程中，较少的专家的数量有助于改善性能。另一个关于泛化问题确认的发现是，模型在小型任务上表现较差，但在大型任务上表现良好。


在小任务 (左图) 中，我们可以看到明显的过拟合，因为稀疏模型在验证集中的表现要差得多。在较大的任务 (右图) 中，MoE 则表现良好。

4)一种可行的微调策略是尝试冻结所有非专家层的权重。实践中，这会导致性能大幅下降，但这符合我们的预期，因为混合专家模型 (MoE) 层占据了网络的主要部分。我们可以尝试相反的方法: 仅冻结 MoE 层的参数。实验结果显示，这种方法几乎与更新所有参数的效果相当。这种做法可以加速微调过程，并降低显存需求。


在微调稀疏混合专家模型 (MoE) 时需要考虑的最后一个问题是，它们有特别的微调超参数设置——例如，稀疏模型往往更适合使用较小的批量大小和较高的学习率，这样可以获得更好的训练效果。


更多前沿内容>> 专注于大模型/AIGC、Agent、RAG等前沿技术分享！

MoE转折点
此时，您可能会对人们微调 MoE 中遇到的这些挑战而感到沮丧，但最近的一篇论文 “MoEs Meets Instruction Tuning” (2023 年 7 月) 带来了令人兴奋的发现。这篇论文进行了以下实验:

单任务微调
多任务指令微调
多任务指令微调后接单任务微调
当研究者们对 MoE 和对应性能相当的 T5 模型进行微调时，他们发现 T5 的对应模型表现更为出色。然而，当研究者们对 Flan T5 (一种 T5 的指令优化版本) 的 MoE 版本进行微调时，MoE 的性能显著提升。更值得注意的是，Flan-MoE 相比原始 MoE 的性能提升幅度超过了 Flan T5 相对于原始 T5 的提升，这意味着 MoE 模型可能从指令式微调中获益更多，甚至超过了稠密模型。此外，MoE 在多任务学习中表现更佳。与之前关闭 辅助损失 函数的做法相反，实际上这种损失函数可以帮助防止过拟合。



Deepseek MOE 架构
和基础 MOE 结构的区别是：

更精细地划分专家网络，提升每个专家的专业性，提高知识表达的准确度。
引入部分共享专家，减少不同专家间的知识冗余，提升计算效率；所有 tokens 都会经过的共享专家，每个 token 会用计算的 Router 权重，来选择 topK 个专家，然后和共享的专家的输出一起加权求和。
DeepseekMOE 其实是有两类专家的：

共享专家（Shared Expert）：1 个共享专家，用于捕捉通用、全局的特征信息。
路由专家（Routed Experts）：每个 MoE 层都包含 256 个路由专家，负责精细化处理输入 tokens 的专业特征。
Gate 网络与 DeepseekMOE 计算流程
当一个 token 的向量传入 MoE 层时，首先会经过一个专门的 Gate 网络，该网络负责计算 token 与各个路由专家之间的匹配得分。具体流程如下：

1、计算 tokens 和专家的匹配得分

Gate 网络通过线性变换计算每个 token 与所有路由专家的兼容性得分。得分可以反映 token 和各专家“契合”的程度。
2、选择 Top-K 专家

基于得分，Gate 网络为每个 token 选择 Top-K 个最合适的路由专家。在 DeepSeek‐V3 中，每个 token 通常选择 8 个路由专家（在一些实现中还可能对跨节点路由做限制，如最多路由到 4 个不同节点），从而只激活极少数专家进行计算。
3、专家处理与加权聚合

被选中的专家各自对 token 进行独立处理（专家实际上是小型 FFN 模块，类似于 Transformer 中的 FFN 模块），并产生各自的输出。然后，这些专家的输出会根据 Gate 网络给出的得分权重进行加权聚合，最后再和共享专家的输出进行融合，形成当前 MoE 层的最终输出表示。

门控/路由网络实现
门控网络的作用是，根据输入 tokens 动态的选择 Top-K 个专家，并为每个 Token 分配权重。关键流程如下：

门控分数计算：通过线性层 + Softmax 生成专家选择概率分布。
Top-K 专家选择：支持两种模式（贪婪选择 vs 分组限制贪婪选择），贪婪模式直接使用 torch.topk 函数选取分数张量中的前 k 个分数。
权重归一化：对 Top-K 权重进行归一化或缩放。

DeepseekMOE 实现
1、门控计算
调用门控网络（self.gate），对输入 hidden_states 计算得到 top‑k 专家索引（topk_idx）、对应权重（topk_weight）以及辅助损失（aux_loss，推理时不参与梯度计算）。
2、数据重排

将输入 hidden_states 展平为二维张量（形状 [B * T, d]），并将 topk_idx 也展平。
在推理模式下，通常不需要像训练时那样对每个 token 进行 repeat_interleave，因为每个 token 只会由对应专家处理一次。
3、专家计算

根据展平后的 topk_idx，依次对每个专家负责的 token 子集进行计算。
由于这里可能存在多个 token 被分配给不同专家，实际实现中需要将每个专家的输出按顺序记录下来。
4、输出重构与加权融合

将所有专家计算的输出进行合并。通过将输出重新整理（排序）回原始 token 顺序，并按照 topk_weight 对各个专家输出进行加权求和，从而获得最终输出。
整个过程保证最终输出形状与原始输入保持一致，即 [B, T, d]。
稀疏 VS 稠密，如何选择?
稀疏混合专家模型 (MoE) 适用于拥有多台机器且要求高吞吐量的场景。在固定的预训练计算资源下，稀疏模型往往能够实现更优的效果。相反，在显存较少且吞吐量要求不高的场景，稠密模型则是更合适的选择。

注意: 直接比较稀疏模型和稠密模型的参数数量是不恰当的，因为这两类模型基于的概念和参数量的计算方法完全不同。

MoE计算效率提升
最初的混合专家模型 (MoE) 设计采用了分支结构，这导致了计算效率低下。这种低效主要是因为 GPU 并不是为处理这种结构而设计的，而且由于设备间需要传递数据，网络带宽常常成为性能瓶颈。在接下来的讨论中，我们会讨论一些现有的研究成果，旨在使这些模型在预训练和推理阶段更加高效和实用。我们来看看如何优化 MoE 模型，让 MoE 起飞。

并行计算
让我们简要回顾一下并行计算的几种形式:

数据并行: 相同的权重在所有节点上复制，数据在节点之间分割。
模型并行: 模型在节点之间分割，相同的数据在所有节点上复制。
模型和数据并行: 我们可以在节点之间同时分割模型和数据。注意，不同的节点处理不同批次的数据。
专家并行: 专家被放置在不同的节点上。如果与数据并行结合，每个节点拥有不同的专家，数据在所有节点之间分割。
在专家并行中，专家被放置在不同的节点上，每个节点处理不同批次的训练样本。对于非 MoE 层，专家并行的行为与数据并行相同。对于 MoE 层，序列中的Token被发送到拥有所需专家的节点。Switch Transformers 论文中展示如何使用不同的并行技术在节点上分割数据和模型的插图，如下所示。

容量因子和通信开销
提高容量因子 (Capacity Factor, CF) 可以增强模型的性能，但这也意味着更高的通信成本和对保存激活值的显存的需求。在设备通信带宽有限的情况下，选择较小的容量因子可能是更佳的策略。

一个合理的初始设置是采用 Top-2 路由、1.25 的容量因子，同时每个节点配置一个专家。在评估性能时，应根据需要调整容量因子，以在设备间的通信成本和计算成本之间找到一个平衡点。

部署技术
部署混合专家模型 (MoE) 的一个关键挑战是其庞大的参数规模。对于本地使用情况，我们可能希望使用更小的模型。为了使模型更适合部署，下面是几种有用的技术:

预先蒸馏实验: Switch Transformers 的研究者们进行了预先蒸馏的实验。他们通过将 MoE 模型蒸馏回其对应的稠密模型，成功保留了 30-40%的由稀疏性带来的性能提升。预先蒸馏不仅加快了预训练速度，还使得在推理中使用更小型的模型成为可能。
任务级别路由: 最新的方法中，路由器被修改为将整个句子或任务直接路由到一个专家。这样做可以提取出一个用于服务的子网络，有助于简化模型的结构。
专家网络聚合: 这项技术通过合并各个专家的权重，在推理时减少了所需的参数数量。这样可以在不显著牺牲性能的情况下降低模型的复杂度。
高效训练
FasterMoE 深入分析了 MoE 在不同并行策略下的理论性能极限，并且探索了一系列创新技术，包括用于专家权重调整的方法、减少延迟的细粒度通信调度技术，以及一个基于最低延迟进行专家选择的拓扑感知门控机制。这些技术的结合使得 MoE 运行速度提升高达 17 倍。

Megablocks 则专注于通过开发新的 GPU kernel 来处理 MoE 模型中的动态性，以实现更高效的稀疏预训练。其核心优势在于，它不会丢弃任何Token，并能高效地适应现代硬件架构 (支持块稀疏矩阵乘)，从而达到显著的加速效果。Megablocks 的创新之处在于，它不像传统 MoE 那样使用批量矩阵乘法 (这通常假设所有专家形状相同且处理相同数量的Token)，而是将 MoE 层表示为块稀疏操作，可以灵活适应不均衡的Token分配。下图为不同规模的专家和Token数量的块稀疏矩阵乘法。
开源混合专家模型
目前，Megablocks、Fairseq、OpenMoE等开源项目可以用于训练混合专家模型 (MoE)；对于开源的混合专家模型 (MoE)，你可以关注下面这些：Switch Transformers (Google)、NLLB MoE (Meta)、OpenMoE、Mixtral 8x7B (Mistral)、DeepSeek-R1(DeepSeek)。

研究方向
MoE 的 量化 也是一个有趣的研究领域。例如，QMoE通过将 MoE 量化到每个参数不到 1 位，将 1.6 万亿参数的 Switch Transformer 所需的存储从 3.2TB 压缩到仅 160GB。简而言之，一些值得探索的有趣领域包括:

将 Mixtral 蒸馏成一个稠密模型。
探索合并专家模型的技术及其对推理时间的影响。
尝试对 Mixtral 进行极端量化的实验。
推荐阅读


混合专家模型（Mixture of Experts，MoE）是一种先进的神经网络架构，旨在通过整合多个模型或“专家”的预测来提升整体模型性能。MoE模型的核心思想是将输入数据分配给不同的专家子模型，然后将所有子模型的输出进行合并，以生成最终结果。这种分配可以根据输入数据的特征进行动态调整，确保每个专家处理其最擅长的数据类型或任务方面，从而实现更高效、准确的预测。
MoE模型的主要组成部分包括：

专家（Experts）：模型中的每个专家都是一个独立的神经网络，专门处理输入数据的特定子集或特定任务。例如，在自然语言处理任务中，一个专家可能专注于处理与语言语法相关的内容，而另一个专家可能专注于语义理解。

门控网络（Gating Network）：门控网络的作用是决定每个输入样本应该由哪个专家或哪些专家来处理。它根据输入样本的特征计算出每个专家的权重或重要性，然后根据这些权重将输入样本分配给相应的专家。门控网络通常是一个简单的神经网络，其输出经过softmax激活函数处理，以确保所有专家的权重之和为1。

MoE模型的主要优势在于：

提高模型性能：通过将多个专家的预测结果进行整合，MoE模型可以在不同的数据子集或任务方面发挥每个专家的优势，从而提高整体模型的性能。例如，在图像分类任务中，一个专家可能擅长识别动物图片，而另一个专家可能擅长识别车辆图片，通过门控网络的合理分配，MoE模型可以更准确地对不同类型的图片进行分类。

减少计算成本：与传统的密集模型相比，MoE模型在处理每个输入样本时，只有相关的专家会被激活，而不是整个模型的所有参数都被使用。这意味着MoE模型可以在保持较高性能的同时，显著减少计算资源的消耗，特别是在模型规模较大时，这种优势更为明显。例如，对于一个具有数十亿参数的大型语言模型，采用MoE架构可以在不增加太多计算成本的情况下，通过增加专家的数量来进一步提升模型的性能。

增强模型的可扩展性：MoE模型的架构设计使得它可以很容易地扩展到更多的专家和更大的模型规模。通过增加专家的数量，模型可以覆盖更广泛的数据特征和任务类型，从而在不增加计算复杂度的情况下，提升模型的表达能力和泛化能力。这种可扩展性为处理大规模、复杂的数据集提供了有效的解决方案，例如在处理多模态数据（包含文本、图像、语音等多种类型的数据）时，MoE模型可以通过设置不同的专家来专门处理不同模态的数据，实现更高效的多模态融合。
MoE模型的概念最早可以追溯到1991年的论文“Adaptive Mixture of Local Experts”，由Michael Jordan和Geoffrey Hinton等人提出。这篇开创性的论文为MoE模型奠定了基础，提出了一个系统，在该系统中，单独的网络（专家）在门控网络的指导下处理训练案例的不同子集。这种将多个专家模型组合在一起以提高预测性能的思想，启发了后续众多研究工作的发展。

在随后的几年里，MoE模型的研究逐渐展开，但主要集中在相对较小规模的模型和特定的应用场景中。直到近年来，随着深度学习技术的快速发展和大规模数据集的出现，MoE模型开始在自然语言处理、计算机视觉等领域得到更广泛的应用和关注。

2010年至2015年期间，MoE模型的发展取得了重大进展。一方面，研究人员开始探索将MoE作为更深层次网络中的组件，将其嵌入到多层神经网络的某个层级中，以实现模型的大规模化和高效率并存。例如，Eigen、Ranzato和Ilya等人在他们的研究中，将MoE模型与深层神经网络相结合，通过在不同的网络层级中设置专家模型，使得模型能够更好地处理复杂的输入数据和任务，同时保持较高的计算效率。

另一方面，Yoshua Bengio等研究人员引入了条件计算的概念，这种计算方式可以根据输入数据动态激活网络的某些组件，而关闭其他不相关的组件。这种动态的计算机制为MoE模型的发展提供了新的思路，使得模型能够根据输入数据的特点，灵活地选择和激活最合适的专家进行处理，进一步提高了模型的适应性和效率。

2017年，谷歌的研究团队在论文“Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer”中，将MoE模型与LSTM（长短期记忆网络）相结合，应用于自然语言处理任务，特别是在机器翻译领域取得了显著的性能提升。这项工作不仅展示了MoE模型在处理大规模数据和复杂任务方面的潜力，还提出了稀疏门控机制，即在每次前向传播过程中，只激活一小部分专家来进行计算，而不是激活所有的专家。这种稀疏性的引入，使得MoE模型能够在保持较高性能的同时，显著降低计算成本，为后续MoE模型在更大规模的应用中奠定了基础。

此后，MoE模型的研究不断深入和拓展。2020年，谷歌的GShard项目首次将MoE技术引入Transformer架构中，并提供了高效的分布式并行计算架构，使得MoE模型能够在分布式环境中进行大规模的训练和推理，进一步推动了MoE模型在自然语言处理领域的应用。2021年，谷歌的Switch Transformer和GLaM模型进一步挖掘了MoE技术在自然语言处理中的应用潜力，通过优化门控机制和专家设计，实现了更优秀的性能表现。

近年来，MoE模型的应用范围不断扩大，除了在自然语言处理领域继续取得突破外，还在计算机视觉、多模态学习等领域得到了广泛的研究和应用。例如，2021年6月，V-MoE将MoE架构应用在计算机视觉领域的Transformer架构模型中，通过改进路由算法，在相关任务中实现了更高的训练效率和更好的性能表现。2022年，LIMoE作为首个应用了稀疏混合专家模型技术的多模态模型，其模型性能相较于CLIP等其他多模态模型也有所提升，展示了MoE模型在处理多模态数据方面的强大能力。

总之，MoE模型从最初的理论提出，到如今在多个领域的广泛应用和发展，经历了三十多年的不断探索和创新。随着深度学习技术的不断进步和计算资源的日益丰富，MoE模型将继续发挥其独特的优势，在解决大规模、复杂的人工智能问题中扮演着越来越重要的角色。
专家（Expert）作为混合专家模型（MoE）的核心组件之一，是专门处理输入数据特定子集或特定任务的独立神经网络。每个专家都旨在对特定领域的数据特征或任务类型进行深度学习和优化，以实现对输入数据的高效处理和精准预测。

架构形式：在大多数MoE模型应用中，专家通常采用前馈神经网络（Feed-Forward Neural Network, FFN）的结构。这种结构由多个全连接层组成，每层后通常会跟一个非线性激活函数，如ReLU（Rectified Linear Unit）或GELU（Gaussian Error Linear Unit），以增加模型的非线性表达能力。例如，在自然语言处理任务中，专家网络可以由两层全连接层构成，第一层将输入的词嵌入向量映射到一个高维空间，第二层再将高维特征映射回输出空间，通过ReLU激活函数的非线性变换，模型能够捕捉到复杂的语言模式和语义信息。

任务专一性：每个专家被设计为专注于特定的任务或数据特征。以图像分类任务为例，一个专家可能专门用于识别动物图像的特征，如毛发纹理、眼睛形状等；另一个专家可能专注于识别车辆图像的特征，如车轮、车身轮廓等。这种任务专一性的设计使得专家能够在其擅长的领域内进行深度学习和优化，从而提高对特定类型数据的处理效率和预测准确性。在自然语言处理中，一些专家可能专注于处理语法结构，如句子的主谓宾关系；而另一些专家可能专注于语义理解，如词汇的语义角色标注。通过这种方式，MoE模型能够综合利用多个专家的专业知识，对复杂的输入数据进行全面而深入的分析。

参数量与模型容量：专家的参数量决定了其模型容量，即模型能够学习和表达的信息量。在MoE模型中，通过增加专家的数量，可以在不显著增加整体计算成本的情况下，扩大模型的参数量和模型容量。例如，对于一个具有数十亿参数的大型语言模型，采用MoE架构可以通过设置多个专家，每个专家包含数千万到数亿不等的参数，从而在保持较高性能的同时，实现对更大规模数据的学习和处理。这种参数量的灵活调整为模型的扩展和优化提供了更大的空间，使得MoE模型能够适应不同规模和复杂度的任务需求。

训练与优化：专家的训练过程与传统的神经网络类似，通过反向传播算法根据损失函数的梯度更新网络参数。在MoE模型中，专家的训练不仅依赖于自身的损失函数，还会受到门控网络分配的权重的影响。这意味着专家需要在训练过程中不断调整自身的参数，以提高对分配给它的输入数据的处理效果，同时也需要与门控网络进行协同优化，以更好地适应整个MoE模型的架构和目标。此外，为了提高专家的泛化能力和防止过拟合，通常还会在训练过程中引入正则化技术，如权重衰减、Dropout等。
路由器（Router），也称为门控网络（Gating Network），在混合专家模型（MoE）中扮演着至关重要的角色。它的主要任务是根据输入数据的特征，动态地决定每个输入样本应该由哪个专家或哪些专家来处理，并为每个专家分配相应的权重。合理的路由器设计能够确保输入数据被有效地分配给最适合的专家，从而提高整个MoE模型的性能和效率。

基本原理：路由器通常是一个简单的神经网络，其输入是原始的输入数据或经过预处理的特征向量。网络通过学习输入数据与各个专家之间的相关性，为每个专家计算一个权重或重要性分数。这些权重反映了输入数据在各个专家擅长的领域内的匹配程度，权重越高表示该专家对当前输入数据的处理能力越强。然后，路由器将输入数据按照计算出的权重分配给相应的专家，完成数据的路由任务。例如，在一个文本分类任务中，路由器可能会根据文本的主题、情感倾向等特征，将文本分配给擅长处理相应主题或情感的专家进行处理。

softmax激活函数：为了确保所有专家的权重之和为1，路由器的输出通常会经过softmax激活函数进行处理。softmax函数能够将一个实数向量转换为概率分布，使得每个专家的权重都在[0,1]范围内，并且所有权重的总和为1。这样，权重就可以被解释为输入数据属于各个专家的概率，为输入数据的分配提供了合理的依据。例如，对于一个包含三个专家的MoE模型，路由器计算出的权重可能为[0.2, 0.5, 0.3]，这意味着输入数据有20%的可能性由第一个专家处理，50%的可能性由第二个专家处理，30%的可能性由第三个专家处理。

噪声机制：在路由器的设计中，为了防止某些专家过于频繁地被选择，导致其他专家得不到足够的训练，通常会引入噪声机制。具体方法是在路由器的输出logits上添加一定的噪声，然后应用softmax函数。这种噪声的添加使得路由器的决策更加随机和分散，避免了对特定专家的过度依赖，有助于提高模型的泛化能力和专家之间的均衡发展。例如，可以添加高斯噪声或均匀噪声，噪声的强度可以通过超参数进行调整，以达到合适的负载均衡效果。

Top-K选择策略：除了根据softmax权重进行加权求和外，路由器还可以采用Top-K选择策略来进一步简化计算和提高效率。在这种策略下，路由器只选择权重最高的K个专家来进行后续的计算，而忽略其他权重较低的专家。K的值可以根据具体任务和模型的需求进行调整，通常选择1或2。Top-K选择策略使得MoE模型在每次前向传播过程中只激活一小部分专家，显著降低了计算成本，同时也能保持较高的性能。例如，在一个大规模的自然语言处理模型中，采用Top-2选择策略，每次只激活权重最高的两个专家进行计算，可以在不牺牲太多性能的情况下，大幅减少计算资源的消耗。

负载均衡与辅助损失函数：为了确保各个专家之间的负载均衡，避免某些专家过度负担而其他专家闲置的情况，通常会在MoE模型的训练过程中引入辅助损失函数。辅助损失函数的目标是使每个专家处理的输入数据量大致相等，从而提高模型的整体效率和稳定性。例如，可以定义一个负载均衡损失函数，计算每个专家实际处理的输入数据量与理想处理数据量之间的差异，并将其作为模型总损失的一部分。通过优化辅助损失函数，路由器可以学习到更加均衡的数据分配策略，使得各个专家都能得到充分的训练和利用。

可学习的路由参数：路由器的参数是可学习的，这意味着路由器能够在训练过程中不断调整自身的参数，以更好地适应输入数据的分布和特征。通过与专家网络的协同训练，路由器可以学习到如何根据输入数据的上下文信息，动态地选择最合适的专家进行处理。这种可学习的路由机制使得MoE模型具有很强的自适应能力，能够针对不同的任务和数据集进行个性化的优化。

3. MoE模型优势
unsetunset3.1 高效性与可扩展性unsetunset
MoE模型展现出显著的高效性和可扩展性，这使得它在处理大规模数据和复杂任务时具有独特的优势。

高效性：

计算资源优化：MoE模型通过稀疏激活的特性，在每次前向传播过程中，只有与输入数据最相关的少数专家被激活进行计算，而大部分专家处于未激活状态。例如，在一个具有数百个专家的MoE模型中，每次可能只有2到4个专家参与计算。这种稀疏性显著减少了每次计算所需的资源，与传统的密集模型相比，计算效率大幅提升。据研究，MoE模型在相同的计算资源下，可以实现比密集模型快数倍的训练速度。

推理成本降低：在推理阶段，MoE模型同样只需激活部分专家，这使得推理过程更加高效。对于需要实时响应的应用场景，如在线推荐系统、智能客服等，MoE模型能够快速生成准确的预测结果，减少了等待时间，提高了用户体验。同时，推理成本的降低也有助于降低模型的部署和运营成本。

可扩展性：

参数量灵活调整：MoE模型允许根据任务的需求，在不显著增加计算复杂度的情况下增加更多的专家，从而提高模型的容量和表现。例如，当处理的数据规模增大或任务复杂度提高时，可以通过增加专家的数量来进一步优化模型性能。每个专家可以包含不同数量的参数，通过灵活配置专家的参数量，MoE模型可以在不同的任务和数据集上实现最佳的性能和效率平衡。

轻松应对大规模数据：随着数据量的不断增长，MoE模型展现出强大的可扩展性。它能够有效地处理大规模的数据集，利用稀疏矩阵的高效计算和GPU的并行能力，充分发挥计算资源的优势。例如，在处理包含数十亿甚至数百亿条数据的自然语言处理任务时，MoE模型可以通过分布式训练和推理，将不同的专家分配到不同的计算节点上，实现高效的并行处理，大大缩短了训练时间，提高了模型的训练效率。

适应多样化任务：MoE模型不仅可以扩展到更多的专家，还可以根据任务的特点灵活调整专家的设计和组合。不同的任务可能需要关注不同的数据特征和模式，MoE模型可以通过增加或替换特定领域的专家来适应多样化任务的需求。例如，在计算机视觉领域，可以增加专注于图像边缘检测、纹理分析、目标识别等不同方面的专家；在多模态学习中，可以设置专门处理文本、图像、语音等不同模态数据的专家，实现更高效的多模态融合和处理。

unsetunset3.2 适应性与灵活性unsetunset
MoE模型具有出色的适应性和灵活性，能够根据不同的情境和需求进行动态调整和优化。

适应性：

输入数据适应：门控网络可以根据输入数据的不同特征和上下文信息，动态地选择最合适的专家进行处理。这意味着MoE模型能够自动识别输入数据的类型、主题、情感倾向等，并将其分配给擅长处理该类型数据的专家。例如，在自然语言处理中，对于一段描述科技产品的文本，门控网络可能会将其分配给熟悉科技领域的专家；而对于一段情感丰富的评论文本，则可能分配给擅长情感分析的专家，从而提高模型对不同输入数据的适应能力和预测准确性。

任务场景适应：MoE模型不仅能够适应不同的输入数据，还可以根据任务场景的变化进行调整。在不同的应用场景中，模型可以灵活地选择和组合专家，以满足特定任务的需求。例如，在机器翻译任务中，可以根据源语言和目标语言的特点，选择最适合的语言学专家和语义理解专家进行协作；在推荐系统中，可以根据用户的历史行为和偏好，动态地调整专家的选择，提供更加个性化和精准的推荐结果。

灵活性：

专家设计灵活：MoE模型中的专家可以具有不同的架构和参数配置，这为模型的设计和优化提供了极大的灵活性。根据任务的特点和数据的特性，可以设计专门的专家网络来处理特定的问题。例如，在处理图像分类任务时，专家可以采用卷积神经网络（CNN）的架构来提取图像的特征；在处理序列数据时，专家可以采用循环神经网络（RNN）或Transformer架构来捕捉序列的依赖关系。此外，还可以通过调整专家的层数、隐藏单元数量等参数，进一步优化专家的性能和效率。

门控机制可定制：门控网络的设计和实现也可以根据具体需求进行定制。除了常见的基于softmax权重的门控机制外，还可以采用其他更复杂的门控策略，如基于强化学习的门控、基于注意力机制的门控等。这些可定制的门控机制使得MoE模型能够更好地适应不同的任务和数据分布，提高模型的选择性和适应性。例如，在某些任务中，可能需要门控网络更加强调专家之间的竞争，以选择最优的专家进行处理；而在另一些任务中，则可能需要门控网络更加注重专家之间的协作，综合多个专家的意见来生成最终结果。

4. MoE模型训练与优化
unsetunset4.1 训练挑战与策略unsetunset
训练混合专家模型（MoE）面临诸多挑战，但通过恰当的策略可以有效应对，提升模型性能与稳定性。

挑战
门控网络的优化难题：门控网络需精准地为不同输入分配专家权重，若未准确学习分配策略，会导致过度拟合某些专家或未充分利用特定专家，影响资源利用与模型性能。如在多语言翻译任务中，若门控网络总是偏向几个常用专家，其他专家则得不到足够训练，降低对不同语种的翻译质量。

专家负载不均衡：训练过程中，部分专家可能承担过多任务，而其他专家则相对闲置，造成计算资源浪费，并影响模型收敛速度与性能提升。例如在一个大规模文本分类任务中，若多数样本被分配给少数几个擅长常见主题的专家，其他专家处理的数据量极少，就无法充分发挥模型整体的潜力。

微调阶段的过拟合风险：与稠密模型相比，MoE模型在微调时更易过拟合，导致泛化能力不足。像拥有海量参数的MoE预训练模型，在特定基准测试微调时，可能因过度拟合训练数据，而在实际应用中表现欠佳。

策略
改进门控网络训练方式：采用更复杂的门控网络架构，如引入注意力机制或Transformer结构，增强对输入数据特征的捕捉与理解能力，使分配更精准。还可使用强化学习训练门控网络，将其决策过程视为一个强化学习代理，通过奖励信号优化专家选择，更好地平衡性能与资源利用。

引入辅助损失函数：通过添加辅助损失，如负载均衡损失，促使门控网络在训练中尽量均匀分配输入给专家，避免部分专家过载。例如定义一个损失项，计算每个专家实际处理样本数与理想处理数的差异，并将其纳入总损失函数，引导门控网络优化分配策略。

专家特定正则化：为防止专家过拟合，在训练中对专家施加特定的正则化技术，如Dropout、权重衰减等。还可采用数据增强方法，为专家生成更多样化的训练样本，增强其泛化能力，使模型在微调阶段表现更稳定。

unsetunset4.2 负载均衡与优化unsetunset
负载均衡对MoE模型的训练效率与性能至关重要，合理的优化方法能有效改善负载分配。

优化方法
动态负载均衡策略：根据训练过程中的实时数据动态调整专家的负载。如采用在线学习算法，门控网络可根据当前批次数据的分布和专家的性能反馈，实时更新专家权重分配，使负载更适应数据变化。还可引入反馈机制，让专家根据自身处理难度和资源占用情况，向门控网络反馈调整信号，动态优化负载。

专家容量限制与调整：为每个专家设定容量上限，限制其一次能处理的样本数量。若专家达到容量上限，可，限制其一次能处理的样本数量。若专家达到容量上限，可暂存，待后续处理。同时，根据训练进度和模型性能，适时调整专家容量，初期可适当增加容量快速训练，后期减少容量精细优化。

分布式训练与负载均衡：在分布式环境中，将专家分布在不同计算节点，通过高效的通信机制和任务调度算法，实现专家间的负载均衡。例如采用参数服务器架构，集中管理模型参数，各计算节点从服务器获取所需专家参数，处理本地数据后更新参数，通过合理的任务分配和数据切分，确保各节点负载均衡，提高整体训练效率。

效果与影响
提升训练效率：负载均衡优化后，各专家能更均匀地承担训练任务，避免部分专家过载等待或闲置浪费资源，使训练过程更流畅高效。例如在大规模图像识别任务中，通过优化负载均衡，模型训练时间可缩短30% - 50%，加速模型收敛。

增强模型性能：均匀的负载分配让每个专家都能得到充分训练，挖掘数据特征，提升对不同数据子集的处理能力，进而提高模型整体性能。在自然语言处理任务中，优化后的MoE模型性能可提升10% - 20%，更好地应对复杂多样的文本数据。

提高资源利用率：合理分配负载充分利用计算资源，减少因专家闲置或过载导致的资源浪费，降低训练成本。在分布式训练场景下，优化负载均衡可使计算节点资源利用率提高20% - 30%，更经济地实现大规模模型训练。

5. MoE模型的技术挑战
unsetunset5.1 计算成本与资源管理unsetunset
混合专家模型（MoE）虽然在提高模型性能和处理大规模数据方面具有显著优势，但在计算成本与资源管理方面也面临一些挑战。

内存需求高：MoE模型需要将所有专家的参数都加载到内存中，即使在推理过程中只使用其中一部分专家。例如，以Mixtral 8x7B这样的MoE模型为例，需要有足够的VRAM来容纳一个47B参数的稠密模型。这是因为MoE模型中只有FFN层被视为独立的专家，而模型的其他参数是共享的。高内存需求使得在资源有限的情况下部署和运行MoE模型变得困难，特别是在需要处理大规模参数模型时，对硬件设备的要求更为苛刻。

分布式训练复杂：为了应对大规模模型的训练，通常需要采用分布式训练的方式。但在MoE模型中，由于专家之间的数据交换和并行训练需要机间all-to-all通信来实现，这增加了通信成本。随着模型规模的增大，通信开销也相应增加，可能导致训练效率降低。例如，在一个大规模分布式训练场景中，若模型参数规模达到数十亿甚至更大，通信延迟和网络拥塞问题可能会严重影响训练速度和性能。因此，在实际部署过程中，需要仔细设计通信策略和优化网络拓扑，以降低通信延迟和潜在的网络拥塞。

专家容量限制：为了防止特定专家过载并确保工作负载平衡，通常会对每个专家可以同时处理的输入数量设置阈值。例如，采用top-2路由和1.25的容量因子，这意味着每个输入选择两个专家，每个专家处理其通常容量的1.25倍。这种策略虽然可以在一定程度上平衡负载，但也可能导致部分数据无法及时处理或需要重新分配，影响训练和推理的效率。此外，专家容量的设置需要根据具体的任务和模型规模进行调整，这增加了模型配置和管理的复杂性。

unsetunset5.2 过拟合与泛化问题unsetunset
尽管MoE模型在训练阶段能够通过专家的协同工作快速达到较好的性能，但在微调阶段，过拟合与泛化问题成为了主要挑战。

过拟合风险：与稠密模型相比，MoE模型在微调时更易产生过拟合现象。这是因为MoE模型的参数量虽然大，但在实际应用中只激活部分专家，模型的复杂度相对较高。例如，拥有1.6T参数量的MoE预训练模型Switch Transformer，在SuperGLUE等常见基准上进行微调时，其整体性能却落后于较小的模型。这表明在微调过程中，模型可能会过度拟合训练数据中的噪声和细节，而无法很好地泛化到新的、未见过的数据上。

泛化能力不足：MoE模型的泛化能力在某些任务上表现不佳，尤其是在需要对输入数据进行深入理解和推理的任务中。例如，在重理解任务（如SuperGLUE）上，MoE模型的表现不如对应的稠密模型。这可能是因为MoE模型在训练过程中，专家之间的协作和知识共享不够充分，导致模型对特定任务的理解和处理能力有限。此外，门控网络的设计和训练也可能影响模型的泛化能力，如果门控网络不能准确地将输入数据分配给最合适的专家，就会影响模型的整体性能。

微调策略选择：为了提高MoE模型在微调阶段的泛化能力，需要选择合适的微调策略。一种可行的方法是尝试冻结所有非专家层的权重，只对MoE层的参数进行更新。实验结果显示，这种方法几乎与更新所有参数的效果相当，同时可以加速微调过程并降低显存需求。此外，使用较小的批量大小和较高的学习率进行微调，也有助于提高模型的泛化性能。然而，这些策略的选择需要根据具体的任务和模型情况进行调整，不同的任务可能需要不同的微调策略来达到最佳效果。

6. MoE模型的未来发展方向
unsetunset6.1 研究趋势unsetunset
混合专家模型（MoE）作为一种强大的模型架构，其研究呈现出多维度的发展趋势，以下是一些主要的研究方向：

架构优化与创新：

增加专家数量与细化分工：为了使模型能够处理更复杂的任务和更庞大的数据集，研究者们致力于增加专家的数量。例如，DeepSeekMoE通过拆分专家的方式来增加专家数量，将一个专家拆成两个，从而使得模型的参数规模不变的情况下，计算量和参数量没有明显变化，但专家的专业化程度得到提升。同时，细化专家的分工，让每个专家更加专注于特定的子任务或数据特征，从而提高模型的整体性能和效率。如在自然语言处理中，可以设置专门处理语法、语义、情感等不同方面的专家。

引入多头机制与混合架构：借鉴Transformer中的多头注意力机制，研究者们提出了多头混合专家模型（MH-MoE）。这种模型通过多头机制弥补了传统MoE架构的缺陷，实现了更高的专家激活率，提升了模型的高效扩展能力。此外，还将MoE与其他模型架构进行混合，如与Transformer、CNN等结合，发挥各自的优势，创造出更具竞争力的模型架构。

门控算法改进：

动态路由策略：传统的top-k门控策略存在一定的局限性，如在处理不同难度的任务时，简单的固定k值选择可能不够灵活。因此，研究者们提出了动态路由策略，根据任务的复杂度和输入数据的特性，动态地确定每个输入应该选择的专家数量。例如，对于简单的任务，可能只需要选择较少的专家，而对于复杂的任务，则可以适当增加选择的专家数量。这种动态路由方式能够更好地平衡计算成本和模型性能。

因果分段路由与数据批处理：普林斯顿大学和Meta AI联合提出了一种Lory方法，该方法引入了因果分段路由策略和基于相似性的数据批处理技术。通过将输入数据根据其相似性进行分组，并采用因果分段的方式进行路由，使得模型能够更高效地处理大规模数据，并提高了专家的专业化能力。这种门控算法的改进有助于提升MoE模型在多任务学习和大规模数据处理中的性能。

训练策略优化：

对抗鲁棒性提升：为了增强MoE模型在面对噪声和对抗攻击时的稳定性，研究者们探索了将门控模型和专家模型分开训练的方法。例如，AdvMoE通过算法优化，将门控模型和专家模型的训练过程解耦，使得模型在训练过程中能够更好地抵抗对抗干扰，提高模型的对抗鲁棒性。这种训练策略的优化有助于MoE模型在实际应用中更加稳定可靠。

指令调优与多任务学习：随着对模型控制能力的要求不断提高，指令调优成为了MoE模型训练的一个重要方向。通过使用指令数据对MoE模型进行微调，可以使模型更好地理解和执行人类的指令，从而提高模型在各种任务上的表现。此外，多任务学习也是MoE模型的一个重要应用场景，研究者们致力于优化MoE模型的多任务学习能力，使其能够在同时处理多个任务时，共享知识并提高整体性能。

硬件与软件协同优化：

稀疏计算硬件加速：为了充分利用MoE模型的稀疏性，研究者们正在探索更适合稀疏计算的硬件优化技术。例如，块稀疏Flash Attention注意力机制可以减少GPU内存访问次数，加快计算并节省显存。同时，结合PagedAttention构建vLLM11（一种LLM推理加速系统），实现了KV缓存零浪费且支持各请求间共享KV缓存，从而降低了内存使用，并提高了推理效率。

分布式训练系统与软件工具：随着MoE模型规模的不断扩大，分布式训练成为了必然选择。清华团队发布的FastMoE、FasterMoE、SmartMoE等一系列分布式训练系统，能够显著提升MoE模型的训练速度，并优化模型训练性能。微软的DeepSpeed系统提供了端到端的MoE训练和推理解决方案，结合模型压缩等技术，可提供更快、更便宜的MoE模型推理服务。这些分布式训练系统和软件工具的发展，为MoE模型的大规模应用提供了有力支持。
unsetunset6.2 潜在应用场景拓展unsetunset
混合专家模型（MoE）凭借其独特的架构优势和不断优化的技术，在多个领域展现出了广阔的应用前景，以下是一些潜在的应用场景拓展方向：

自然语言处理领域：

多语言处理与翻译：MoE模型可以针对不同的语言设置专门的专家，每个专家专注于一种语言的处理和翻译。通过门控网络的动态分配，模型能够根据输入的源语言和目标语言，选择最合适的专家进行翻译。例如，在多语言对话系统中，MoE模型可以同时支持多种语言的交流，为用户提供更加流畅、准确的翻译服务，促进不同语言之间的沟通与交流。

文本生成与创作：在文本生成任务中，MoE模型可以根据不同的文本风格、主题和用途，选择相应的专家进行生成。例如，一个专家可以生成新闻报道风格的文本，另一个专家可以生成小说创作风格的文本。此外，MoE模型还可以结合上下文信息和用户指令，生成更加符合用户需求和期望的文本内容，如自动创作诗歌、故事、剧本等，为文学创作、内容生成等提供强大的技术支持。

语义理解与问答：借助MoE模型的多专家协作能力，可以更深入地理解和分析文本的语义信息。在问答系统中，不同的专家可以分别处理问题的不同方面，如语义解析、知识检索、推理判断等，然后综合各个专家的输出，生成更加准确、全面的答案。这将使得问答系统能够更好地应对复杂、多样的问题，提高问答的准确性和可靠性，为用户提供更加满意的问答体验。

计算机视觉领域：

多模态融合与理解：MoE模型可以应用于多模态数据的融合与理解，如同时处理图像、文本、语音等多种模态的信息。
假设一个专家团队共同解决复杂问题。每位专家都拥有独特的技能，团队通过高效分配任务实现了前所未有的成功。这就是混合专家（Mixture-of-Experts，MoE）模型架构背后的基本思想，这种方法允许机器学习系统，特别是神经网络，高效扩展。MoE 不是让一个单一的神经网络处理所有任务，而是将工作分配给多个专门的“专家”，由一个门控网络决定针对每不同输入激活哪些专家。



随着模型变得越来越大、越来越复杂，保持效率和准确性成为最大挑战，尤其是在自然语言处理（NLP）和大型语言模型（LLM）领域中参数扩展到数十亿甚至数万亿的局面下。传统模型对每个输入都激活神经网络中的所有层和神经元，计算成本巨大，减慢了推理速度，并消耗大量内存。在追求速度和可扩展性的实际应用中部署如此庞大的模型是一项艰巨的任务。



混合专家通过一次只激活一小部分专家来解决这个问题，从而在不牺牲性能的情况下减少计算开销。在 MoE 中，这种协作方法在 NLP 和像 OpenAI GPT 这样的 LLM 中变得越来越重要，这些模型需要在保持效率和准确性的同时扩展到数十亿参数。



本文将介绍 MoE 的核心概念、LLM、训练、推理以及 MoE 在现代 AI 模型中的作用。

MoE 的定义及核心概念
简而言之，混合专家（Mixture of Experts，MoE）是一种先进的神经网络架构，它通过动态选择专门的子模型或“专家”来处理输入的不同部分，以提高模型的效率和可扩展性。这个概念可以类比为劳动分工，每个专家专注于某个大问题中的特定一小部分任务，从而生成更快、更准确的结果。



混合专家模型由三个关键组件组成：



专家（Experts）：专门针对特定任务的子模型。

门控网络（Gating Network）：一个选择器，它将输入数据路由到相关的专家。

稀疏激活（Sparse Activation）：只有少数专家针对每个输入被激活的方法，优化了计算效率。

专家
在 MoE 架构中，专家是指训练好的子网络（神经网络或层），它们专门处理特定的数据或任务。例如，在图像分类任务中，一个专家可能专门识别纹理，而另一个专家可能识别边缘或形状。这种分工有助于整个模型更高效地处理问题，因为每个专家只处理它最适合的数据类型。

门控网络或路由器
门控网络充当一个选择器，它决定将哪些输入数据发送给哪些专家。不是所有专家都同时工作，而是门控网络将数据路由到最相关的专家那里。类似于 token 选择路由策略，路由算法为每个 token 选择最佳的一个或两个专家。例如，在下图中，输入令牌 1，“我们”，被发送到第二个专家，而输入令牌 2，“喜欢”，被发送到第一个网络。
Token 选择路由 | 来源



以下是一些主流的 Token 路由技术：



Top-k 路由：这是最简单的方法。门控网络选择亲和力得分（affinity score）最高的 top k 个专家，并将输入数据发送给它们。

专家选择路由：与根据数据选择专家不同，这种方法由专家决定它们最能处理哪些数据。这种策略旨在实现最佳的负载均衡，并支持以多种方式将数据映射到专家。

专家选择路由 | Source

稀疏激活
稀疏激活是 MoE 模型的关键部分和优势之一。与所有专家或参数对输入都活跃的密集模型不同，稀疏激活确保只有一小部分专家根据输入数据被激活。这种方法在保持性能的同时减少了计算需求，因为任何时候只有最相关的专家是活跃的。



稀疏路由：稀疏激活的一种特定技术，其中门控网络针对每个输入只激活少数专家。

MoE 在深度学习中的历史演变
MoE 的来自于 1991 年的论文《Adaptive Mixture of Local Experts》。这篇论文引入了将复杂问题分解为子问题并分配给多个专门模型的思想。这种分而治之的策略成为了 MoE 架构的核心。



接下来，两个关键研究领域进一步塑造了 MoE 的演变：



专家作为组件（Experts as components）：最初，MoE 被应用于支持向量机（Support Vector Machines，SVM）和高斯过程（Gaussian Processes）等模型中。然而，Eigen、Ranzato 和 Ilya 的研究通过将 MoE 集成为深度神经网络中的组件，使它们能够作为更大模型中的层来运作，扩展了这种方法。

条件计算：传统的神经网络处理通过所有层处理输入，但 Yoshua Bengio 的研究引入了条件计算，根据输入选择性地激活或停用网络组件。这种动态方法提升了计算效率，因为每个输入只使用了模型中必要的部分。

大规模 NLP 模型中的 MoE：GShard 和 Switch Transformer
MoE 对自然语言处理（NLP）的影响随着 GShard 和 Switch Transformer 等模型的出现而得到巩固。2021 年，谷歌的 Switch Transformer 模型拥有 1.6 万亿参数，证明了 MoE 能够处理需要大量计算资源的任务。通过每个输入只激活少数专家，模型在参数数量增长的同时保持了效率。



另一个重要的里程碑是在 2017 年，Shazeer 等人引入了稀疏门控混合专家层（Sparsely Gated Mixture-of-Experts Layer），实现深度学习中的稀疏激活。这使得模型能够通过高达 1370 亿参数处理机器翻译等任务，同时通过每个输入只激活最相关的专家来维持较低的推理成本。

不只 NLP：视觉和多模态模型中的 MoE
MoE 不仅限于 NLP 领域。例如，谷歌的 V-MoE 架构使用稀疏 MoE 框架进行计算机视觉任务，使用视觉变换器（Vision Transformers，ViT）进行图像分类，并实现专家架构。这允许我们像扩展文本模型一样扩展图像模型。



随着研究的进一步发展，MoE 越来越多应用到不同领域的复杂任务中。MoE 成为了现代 AI 架构中的重要基石，是深度学习中更高效、可扩展的解决方法。
详解 MoE 架构及原理
MoE 系统主要分为两个流程：



训练（Training）

推理（Inference）

训练
像其他机器学习模型一样，MoE 模型从数据中学习，但它们的训练过程是独特的。MoE 不是将整个模型作为一个单一实体进行训练，而是专注于分别训练其各个组成部分——专家和门控网络。通过这种方式，每个专家可以专门处理指定的任务，而门控网络学会有效地将输入路由到适当的专家。

专家训练
MoE 模型中的每个专家被视为一个单独的神经网络，并在数据或任务的子集上进行训练。每个专家的训练遵循标准的神经网络训练过程，最小化其特定数据子集的损失函数。



例如，在自然语言处理模型中，一个专家可能在正式文件数据集上进行训练，以专用于处理正式语言。相比之下，另一个可能在社交媒体对话上进行训练，以精通非正式交流表达。这种个性化训练使专家能够在其特定领域变得高度熟练。

门控网络训练
门控网络充当决策者，为给定输入选择最合适的专家。它与专家网络一起训练，但扮演不同的角色。门控网络的输入与整个模型输入的数据相同，可以包括文本、图像或任何基于模型任务的输入。门控网络的输出是一个概率分布，指示哪个或哪些专家最适合处理当前输入。



门控网络的训练通常是监督式的，在训练阶段提供标记数据。门控网络学会根据提供的标签对输入进行分类，并将其分配给正确的专家。在训练期间，门控网络被优化以准确选择专家，并提高 MoE 模型的整体性能。

联合训练
在联合训练阶段，整个 MoE 系统，包括专家模型和门控网络，都进行训练。这种策略确保门控网络和专家能够和谐工作。联合训练中的损失函数结合了各个专家和门控网络的损失，确保两个组成部分都有助于整体性能。



然后，组合的损失梯度通过门控网络和专家模型传播，促进更新，以提高 MoE 系统的性能。

推理阶段
在推理过程中，门控网络接收输入并选择最有可能提供正确输出的 top k 个专家。被选中的专家处理输入并生成它们的预测，然后将这些预测组合起来产生最终输出。与全连接模型相比，这种选择性激活专家的方式允许 MoE 以较低的计算成本进行预测。

稀疏激活的力量
MoE 的一个关键优势在于其使用稀疏激活，这源自于条件计算的概念。与传统的“密集”模型不同，所有参数不会对每个输入都激活，MoE 有选择地只激活必要的网络部分。这种方法提供了几个好处：



效率：MoE 模型可以通过只激活最相关的专家来处理大量数据，从而使用更少的计算资源。这对于像 NLP 中使用的大规模模型尤其重要，这些模型的处理时间和内存需求可能迅速变得过高。一个显著的例子是 Mistral 8x7B——一个高质量的稀疏混合专家模型（SMoE），它使用具有八个专家的 MoE 框架。每个专家有 110 亿参数和 550 亿共享注意力参数，每个模型总计 1660 亿参数。有趣的是，在推理过程中，每个 token 只使用两个专家，使 AI 处理过程更加高效和专注。

负载均衡：稀疏激活的一个重要考虑因素是确保所有专家都得到充分的训练。如果只有少数专家被门控网络激活，它们可能变得过度专业化，而其他专家则未被充分利用。为了防止这种不平衡，现代 MoE 使用了诸如 Noisy Top-k Gating 等技术，Shazeer等人（2017 年）添加了少量（可调的）噪声到专家选择过程中，确保所有专家之间的训练分布更加平衡。

MoE 对比传统模型的优势及挑战
优势
混合专家（MoE）架构相比传统深度学习模型提供了几个优势：



更高可扩展性：凭借稀疏激活，MoE 模型可以轻松扩展到数十亿甚至数万亿参数，从而减少对巨大计算能力的需求。

更灵活：MoE 的一个好处是可以在不重新训练整个系统的情况下向现有模型添加新专家。这种灵活性允许模型轻松适应新任务和领域。

更高效率：由于 MoE 仅对每个输入激活最相关的专家，它能够比传统模型更有效地处理多样化任务。这使得它更快、更准确，因为专家可以专注于他们最擅长的事情。

并行处理：专家可以独立工作，从而实现高效的并行处理。这种方法可以缩短训练和推理时间。

挑战
尽管 MoE 模型具有上述优势，但也存在某些挑战和局限性。



训练复杂性：训练 MoE 模型可能具有挑战性，特别是在管理门控网络和平衡各个专家的贡献方面。重要的是要确保门控网络学会有效地给专家分配适当的权重，这可以防止过度拟合或未充分利用特定专家。

通信成本：MoE 模型在训练和推理期间需要大量的基础设施资源，这是因为模型需要管理多个专家和门控机制。此外，当在大规模部署时，尤其是在各种设备或分布式系统之间部署，通信开销是一个主要挑战。协调和同步来自不同服务器上不同专家的输出可能导致延迟和计算负载的增加。

专家容量：为了防止特定专家过载并确保工作负载平衡，对每个专家可以同时处理的输入数量设置了阈值。一种常见的方法是使用 top-2 路由和 1.25 的容量因子，这意味着每个输入选择两个专家，每个专家处理其通常容量的 1.25 倍。这种策略还为每个核心分配一个专家，优化性能和资源管理。

透明度：不透明已经是 AI 中的一个显著问题，包括对于 LLM。MoE 模型可能会加剧这一问题，因为它们更加复杂。我们不仅要看一个模型如何做出决策，还必须弄清楚不同的专家和门控系统如何协同工作。这种额外的复杂性可能让我们更难以理解为何模型做出特定选择。

MoE 应用
MoE 已经被广泛应用于多种应用中。



自然语言处理：MoE 模型非常适合翻译、情感分析和问答等语言任务，因为它可以将每个任务分配给专家。例如，OpenAI 的 sophnet/Qwen3-30B-A3B-Thinking-2507 据称采用了具有 16 个专家的 MoE 架构（尽管 OpenAI 尚未官方确认其设计细节）。另一个例子是微软的翻译 API，Z-code。Z-code 中的 MoE 架构允许模型在保持相同计算能力的同时支持大规模的模型参数。

计算机视觉：谷歌的 V-MoEs 是一种基于视觉变换器（Vision Transformers, ViT）的稀疏架构，展示了 MoE 在计算机视觉任务中的有效性。MoE 模型可以通过将不同任务分配给专门的专家来帮助图像处理。例如，一个专家可能专注于某些类型的物体、特定的视觉特征或图像的其他部分。

多模态学习：MoE 可以将来自多个来源的数据，如文本、图像和音频，整合到一个模型中。这使得 MoE 非常适合多模态搜索或内容推荐等应用，这些应用需要整合不同模态的数据。

MoE 的未来方向
在未来几年中，MoE 的研究将集中在使模型更高效、更易于理解上。这包括改进专家如何协同工作，以及寻找更好的方法将任务分配给正确的专家。



进一步扩展：MoE 模型可以扩展到更大的规模，同时最小化计算成本。这包括优化训练和推理阶段以处理不断增加的专家和数据规模。当前正在探索如分布式计算等技术，以更高效地将任务分布在多台机器上，减少瓶颈并加快模型的运行速度。

创新的路由机制：另一个研究领域集中在开发更高效的路由策略上。虽然像 top-k 路由这样的现有方法被普遍使用，但高级技术如专家选择路由（Expert Choice Routing）可以改进任务分配给专家的过程。这可以实现更高效的负载平衡并改进任务和专家的匹配程度，确保模型在不同条件下都能以最佳状态运行。

现实世界的应用：MoE 模型在现实世界的应用中具有巨大潜力，如医疗保健和自动驾驶系统，因为它们能够处理复杂任务。在医疗保健领域，它们可以用于个性化治疗方案，而自动驾驶系统可以利用它们进行物体识别和决策。

总结
Mixture-of-Experts（MoE）是一种强大的模型架构，它通过将任务分配给专门的专家，使神经网络能够高效地扩展。具有稀疏激活的 MoE 模型能够以较低的计算成本处理大规模数据集和复杂任务。尽管 MoE 在训练复杂性和通信成本方面确实存在挑战，但它在可扩展性、灵活性和效率方面的优势使其成为现代 AI 应用中越来越受欢迎的选择。从自然语言处理到计算机视觉和多模态任务，MoE 模型通过让专家在不同领域处理专门的任务，提高了模型速度和准确性。


一、Transformer整体架构：从黑盒到组件拆解
1.1 模型黑盒视角
在机器翻译等典型任务中，Transformer可视为一个“输入-输出”黑盒：接收源语言文本（如法语句子），输出目标语言文本（如英语翻译）。以“Je suis étudiant”（我是学生）的翻译为例，黑盒的输入是3个法语单词的序列，输出是3个英语单词的序列，中间通过内部机制完成语义转换。



1.2 核心组件拆解
打开黑盒，Transformer的架构由三部分构成：编码器组件（Encoder Stack）、解码器组件（Decoder Stack）、连接层（Encoder-Decoder Attention），具体结构如下：

编码器组件：由6个结构相同但权重独立的编码器堆叠而成（“6”是论文中选择的超参数，实际可根据任务调整），核心作用是提取输入序列的全局语义特征。
解码器组件：与编码器对称，同样由6个解码器堆叠而成，核心作用是基于编码器的特征和已生成的目标序列，逐词生成输出。
连接层：位于解码器的自注意力层与前馈网络之间，使解码器能关注编码器输出中与当前生成词相关的语义信息（类似传统seq2seq模型的注意力机制[6]）。
1.3 编码器与解码器的内部结构
编码器子层
每个编码器包含两个核心子层，且每个子层后均设有残差连接（Residual Connection） 和层归一化（Layer Normalization）（后续章节详解）：

自注意力层：计算输入序列内部各元素的关联权重，捕捉局部与全局上下文。
前馈神经网络（Feed-Forward Network, FFN）：对自注意力层的输出进行非线性变换，且每个位置的变换独立（无序列依赖，支持并行计算）。

解码器子层
解码器在编码器结构的基础上增加了一个编码器-解码器注意力层，形成三层结构：

掩码自注意力层（Masked Self-Attention）：与编码器自注意力层类似，但通过“掩码”屏蔽未来位置的信息（避免生成时提前看到后续词），保证自回归生成逻辑。
编码器-解码器注意力层：以解码器输出为“查询（Query）”，编码器输出为“键（Key）”和“值（Value）”，建立输入与输出的语义关联。
前馈神经网络：与编码器的FFN结构完全一致，对注意力层输出进行非线性变换。

二、张量流动：从输入到特征的转换过程
Transformer处理数据的核心是张量（Tensor） 的传递与变换，我们以“Thinking machines are cool”（智能机器很酷）的翻译为例，拆解张量的流动逻辑。

2.1 词嵌入：将单词转换为向量
与所有NLP模型类似，Transformer首先通过词嵌入算法（Embedding Algorithm[7]） 将每个输入单词转换为固定维度的向量（论文中为512维）。例如，“Thinking”被映射为[0.2, 0.5, -0.1, …, 0.3]（共512个数值），该向量捕捉单词的基础语义信息。



注意：词嵌入仅发生在最底层编码器，上层编码器的输入是下层编码器的输出张量（同样为512维向量序列）。
序列长度：输入序列的长度由超参数决定（通常设为训练数据集中最长句子的长度，短句子需补零，长句子需截断）。
2.2 张量在编码器中的传递
每个单词的嵌入向量会依次流经编码器的两个子层，且在自注意力层中存在序列内依赖，在前馈网络中可完全并行计算（Transformer高效性的关键）。
以“Thinking”为例，其向量在编码器中的处理流程为：

嵌入向量 + 位置编码（后续章节详解）→ 输入自注意力层；
自注意力层计算该向量与其他所有单词向量的关联权重，输出融合全局上下文的向量；
向量流经前馈网络，通过非线性变换（如ReLU激活函数）增强表达能力；
输出向量传递到上层编码器，重复上述过程，最终形成输入序列的全局语义特征张量。
三、核心机制解析（一）：自注意力与多头注意力
自注意力机制是Transformer的“灵魂”，其本质是通过计算“注意力权重”，让模型在处理每个元素时“关注”序列中最相关的部分。

3.1 自注意力的直观理解
以句子“The animal didn’t cross the street because it was too tired”（这只动物没有过马路，因为它太累了）为例，人类能轻松判断“it”指代“the animal”，但传统模型难以捕捉这种长距离依赖。自注意力机制通过以下方式解决该问题：

当模型处理“it”时，会计算“it”与其他所有单词的“注意力得分”，得分越高表示关联性越强，最终“it”的向量会融合“the animal”的语义信息，实现精准指代。

3.2 自注意力的数学计算（向量视角）
自注意力的计算分为6个步骤，我们以单个单词向量（如“Thinking”的嵌入向量x₁）为例：

步骤1：生成Q、K、V向量
对每个输入向量x，通过三个独立的权重矩阵（W_Q、W_K、W_V，训练过程中学习）生成三个新向量：

查询向量（Query, Q）：表示当前单词“要找什么信息”；
键向量（Key, K）：表示其他单词“提供什么信息”；
值向量（Value, V）：表示其他单词“具体的信息内容”。
公式：Q = x × W_Q，K = x × W_K，V = x × W_V
维度：输入向量x为512维，W_Q/W_K/W_V为512×64维，因此Q/K/V为64维（维度降低可减少计算量）。


将所有加权后的V向量求和，得到当前单词的自注意力层输出向量Z：
Z = Σ(加权V)

3.3 自注意力的矩阵计算（工程实现）
实际训练中，为提升效率，自注意力通过矩阵运算批量处理所有单词，流程如下：

将所有输入单词的嵌入向量打包为矩阵X（维度：seq_len × 512，seq_len为序列长度）；
一次性生成Q、K、V矩阵：Q = X × W_Q，K = X × W_K，V = X × W_V（维度均为seq_len × 64）；
矩阵形式计算自注意力输出：
Z = Softmax( (Q × Kᵀ) / √d_k ) × V

3.4 多头注意力：捕捉多维度语义
论文提出多头注意力（Multi-Head Attention），通过并行运行多个自注意力头（论文中为8个），让模型从不同角度捕捉语义信息，具体流程如下：

为每个注意力头分配独立的W_Q、W_K、W_V矩阵，对输入X进行投影，生成8组Q、K、V矩阵；
每个注意力头独立计算自注意力输出Z₁~Z₈（维度均为seq_len × 64）；
将8组Z矩阵拼接（维度：seq_len × (64×8)=seq_len × 512），再通过权重矩阵W_O（512×512）投影，得到最终多头注意力输出。

如上图所示，当处理“it”时，不同注意力头分别聚焦于“animal”（指代关系）和“tired”（属性描述），最终融合多维度信息生成更全面的语义向量。

四、核心机制解析（二）：位置编码与残差连接
自注意力机制本身不具备“顺序感知能力”——若打乱输入序列的单词顺序，仅通过Q/K/V的点积计算，结果不会发生变化。此外，深层网络训练中易出现梯度消失问题，Transformer通过位置编码和残差连接分别解决这两个关键问题。

4.1 位置编码：为序列注入顺序信息
位置编码的核心思想是：为每个位置的单词嵌入向量添加一个“位置特征向量”，使模型能区分不同位置的单词。该向量需满足两个条件：

不同位置的编码向量具有唯一性；
编码向量的差值与单词在序列中的距离相关（如位置1与位置3的差值，应小于位置1与位置10的差值）。
位置编码的数学实现
论文采用正弦和余弦函数生成位置编码向量，公式如下（pos为单词在序列中的位置，i为向量维度的索引）：

当i为偶数时：PE(pos, i) = sin(pos / 10000^(2i/d_model))
当i为奇数时：PE(pos, i) = cos(pos / 10000^(2i/d_model))
其中d_model为嵌入向量的维度（论文中为512）。这种设计的优势在于：

正弦函数的周期性可自然体现位置的周期性（如句子的语法结构重复）；
可通过三角函数的和角公式，推导出任意位置的编码向量（支持长于训练数据的序列）。
在实际计算中，位置编码向量与词嵌入向量直接相加（维度均为512），形成包含“语义+位置”信息的输入向量，再传入自注意力层。

4.2 残差连接与层归一化：保障深层训练稳定
Transformer的编码器和解码器均为6层堆叠结构，深层网络易出现梯度消失（训练时参数更新缓慢）和表示退化（深层输出与浅层输出差异过小）问题。Transformer通过残差连接和层归一化的组合，有效解决这些问题。

残差连接（Residual Connection）
残差连接的核心是“跳过当前层”，将输入直接传递到输出端，与当前层的计算结果相加，公式如下：

输出 = 输入 + 层计算结果（如自注意力层输出）
以编码器的自注意力层为例，残差连接的流程为：

输入向量（词嵌入+位置编码）记为x；
自注意力层对x进行处理，输出向量记为Attention(x)；
残差连接输出：x + Attention(x)（需保证x与Attention(x)维度一致，论文中均为512维）。
残差连接的作用：

缓解梯度消失：梯度可通过“输入直接传递”的路径反向传播，避免因多层线性变换导致梯度衰减；
强化特征复用：保留浅层的基础特征，与深层的抽象特征融合，提升模型表达能力。
层归一化（Layer Normalization）
层归一化的作用是“标准化层输出”，减少内部协变量偏移（Internal Covariate Shift）——即训练过程中，层输入的分布随参数更新而变化，导致训练不稳定。其计算步骤如下：

计算层输出向量的均值μ和标准差σ；
对向量进行标准化：(向量 - μ) / (σ + ε)（ε为极小值，避免分母为0）；
通过可学习参数γ（缩放）和β（偏移）调整标准化结果：γ×标准化向量 + β。
在Transformer中，层归一化紧跟在残差连接之后，形成“残差连接+层归一化”的结构（论文中称为“Pre-LN”结构的早期形式），流程如下：

以编码器为例，完整的子层流程为：
输入 → 自注意力层 → 残差连接（输入 + 注意力输出） → 层归一化 → 前馈网络 → 残差连接（归一化输出 + FFN输出） → 层归一化 → 输出到上层编码器。

这种结构确保了每一层的输出分布稳定，同时为梯度传播提供“捷径”，使6层甚至更深的Transformer能稳定训练。

五、解码器工作流程：从特征到生成
解码器的核心任务是“自回归生成”——基于编码器的语义特征和已生成的目标序列，逐词输出最终结果。其工作流程分为编码阶段、解码阶段和掩码机制三部分。

5.1 编码阶段：为解码器提供语义基础
在解码开始前，编码器先完成对输入序列的处理：

输入序列（如法语“Je suis étudiant”）经过词嵌入和位置编码后，传入底层编码器；
经过6层编码器的“自注意力+FFN”处理，最终输出全局语义特征张量（维度：seq_len × 512）；
该张量被拆分为键矩阵K_enc和值矩阵V_enc，传递给所有解码器的“编码器-解码器注意力层”，为解码提供输入语义参考。

5.2 解码阶段：逐词生成目标序列
解码阶段采用“自回归”方式，每次生成一个单词，直到输出“句子结束符”（如），具体步骤如下：

初始输入：解码开始时，输入为“起始符”（如）的嵌入向量+位置编码；
第一层解码：
掩码自注意力层：处理“起始符”，屏蔽未来位置（此时无未来词，仅需标记当前位置）；
编码器-解码器注意力层：以掩码自注意力的输出为Q，编码器的K_enc和V_enc为K、V，建立与输入序列的语义关联；
前馈网络：对注意力输出进行非线性变换，输出到上层解码器；
多层传递：经过6层解码器处理后，输出一个512维向量；
生成单词：该向量经线性层（投影到词汇表维度）和Softmax（计算概率）后，选择概率最高的单词（如“I”）作为当前步输出；
迭代生成：将“ + I”作为下一轮输入，重复步骤2-4，依次生成“am”“a”“student”“”，最终完成翻译。
5.3 掩码机制：确保生成逻辑正确
解码器的“掩码自注意力层”需通过掩码（Mask） 屏蔽未来位置的信息，避免模型在生成第t个单词时“看到”第t+1个及以后的单词（违反自回归逻辑）。掩码的实现方式如下：

生成一个“下三角矩阵”（维度：seq_len × seq_len），下三角及对角线元素为0，上三角元素为-∞；
在计算自注意力得分后、Softmax之前，将该矩阵与得分矩阵相加，使未来位置的得分变为-∞；
Softmax后，未来位置的权重变为0，模型无法关注这些位置的信息。
Mamba 作为近期的深度学习新架构，被视为为超过transformer的序列建模构架，从文本到视觉处理，相关应用、研究、变体遍地开花。可以说，原本用Transformer做的工作都可以在mamba上重做一遍。
引言
尽管Transformer取得了令人印象深刻的成就，但仍然面临着固有的局限性，尤其是由于注意力计算的二次方计算复杂度，这导致了耗时的推理过程。

24年，一种名为Mamba的新型架构，从经典的状态空间模型（SSM）中汲取灵感，作为构建基础模型的有前途的替代方案出现，它在保持与Transformer相当的建模能力的同时，对于序列长度具有近线性的可扩展性。这激发了越来越多的研究积极探索Mamba在不同领域实现卓越性能的潜力。鉴于这种快速发展，迫切需要一个系统性的回顾，整合现有的Mamba赋能模型，为这种新兴的模型架构提供全面的理解。

1 Mamba 的概括
Mamba 是一种基于结构化状态空间序列模型（SSMs）的新兴架构，旨在高效捕捉序列数据中的复杂依赖性，成为 Transformer 的强大竞争对手。受经典状态空间模型启发，Mamba 融合了循环神经网络（RNN）和卷积神经网络（CNN）的特点，通过递归或卷积操作实现计算成本与序列长度的线性或近线性扩展，显著降低计算复杂度。

具体而言，Mamba 的核心优势包括：

选择机制：引入简单而有效的选择机制，通过输入参数化 SSM 参数，过滤无关信息，保留必要数据。
硬件感知算法：采用递归扫描而非卷积计算，优化硬件性能，在 A100 GPU 上实现高达 3 倍的加速。
建模能力：在保持与 Transformer 相当的建模能力的同时，具备近线性的可扩展性，适用于复杂和长序列数据。
这些特性使 Mamba 成为处理多领域任务的理想基础模型，已在计算机视觉、自然语言处理和医疗保健等领域展现出卓越性能。例如，Vim 模型在高分辨率图像特征提取中比 DeiT 快 2.8 倍，节省 86.8% 的 GPU 内存；而在语言建模任务中，改进的选择性 SSM 架构实现了 2-8 倍的加速。Mamba 的高效性和灵活性使其有望在多个研究和应用领域引发革命性变革。
图1. Mamba模型在不同下游任务中的应用示例。

由于Mamba强大的长序列建模能力及其高效率，大量的文献出现了，专注于在各种下游任务中使用和改进Mamba。鉴于与Mamba相关的研究的显著增长，进行一个全面的文献回顾，讨论未来研究的潜在方向是至关重要的。因此，在这项调查中，我们从几个角度对Mamba进行了全面的回顾，为新来者提供了Mamba内部工作原理的基本理解，同时帮助经验丰富的从业者了解其最新发展。具体来说，剩余的调查组织如下：第2节，我们回顾了各种代表性深度神经网络的背景知识，包括RNNs、Transformers和状态空间模型，而Mamba的细节在第3节中介绍。随后，我们在第4节总结了Mamba基础研究的最新进展，从块设计、扫描模式和内存管理的角度。然后，第5节介绍了将Mamba适应多样化数据的技术，包括序列和非序列数据。此外，第6节介绍了Mamba模型的代表性应用，而挑战和未来方向在第7节中呈现。最后，我们在第8节总结了整个调查。

Mamba 的架构设计


Mamba 架构的主要创新点包括：对输入信息有选择性处理、硬件感知的算法、更简单的架构。它是第一个真正实现匹配 Transformer 性能的线性时间序列模型，建立在更现代的适用于深度学习的结构化 SSM（S4, Structured SSM）基础上，与经典架构 RNN 有相似之处。

另外，基于 Mamba 架构还出现了一些新的魔改版本，应用于不同领域并带来了更多创新。例如，u-mamba 用于生物医学图像的分割任务，采用了混合 CNN-SSM 架构，能捕捉局部细粒度特征和长程依赖关系，具有自配置能力、线性扩展能力以及与其他技术集成的潜力；Weak-mamba-unet 是用于医学图像分割的弱监督学习框架，结合了 CNN、ViT 和 vmamba 的优势，采用多视角交叉监督学习方法；graph-mamba 是一种新型的图网络，将选择性状态空间模型与图网络集成，实现了输入相关的节点过滤和自适应上下文选择，在大型图上能减少高达74%的 GPU 内存消耗；swin-umamba 则提出了用于2D 医学图像分割的基于 mamba 的网络及其变体结构，变体具有更少的参数和更低的 FLOPs，适用于高效应用，并且有效整合了基于 ImageNet 的预训练。
Mamba 是一种新的选择性结构状态空间模型，它的一些最新创新点包括：
简化的 SSM 架构：Mamba 将结构化状态空间序列模型（S4）中使用的类似线性注意力的块和多层感知器（MLP）块进行集成，构建了 Mamba 块。总体架构由重复的 Mamba 块与标准规范化层和残差连接交织组成。它继承了状态空间模型序列长度的线性可伸缩性，同时实现了类似于 Transformer 的建模能力。
选择机制：通过将其参数作为输入的函数，提高基于上下文的推理能力，利用一种选择机制，使其能够更高效和有效地捕获相关信息，特别是在处理长序列时。这种选择机制可以基于输入内容有选择性地传播或遗忘信息，过滤掉与问题无关的信息，并且可以长期记住与问题相关的信息。
硬件感知算法：设计了融合了内核和重新计算的硬件感知算法，避免了中间状态的存储，减少内存需求，提高计算效率。例如采用“并行扫描算法”而非“卷积”来进行模型的循环计算，为了减少 GPU 内存层次结构中不同级别之间的 I/O 访问，它没有具体化扩展状态。
2 Mamba的基础知识
Mamba与循环框架的循环神经网络（RNNs）、并行计算和Transformer的注意力机制以及状态空间模型（SSMs）的线性属性密切相关。因此，本节旨在介绍这三种突出架构的概述。

2.1 循环神经网络（RNNs）
RNNs在处理序列数据方面表现出色，因为它们能够保留内部记忆[54]。这类网络在涉及分析和预测序列的众多任务中表现出显著的有效性，例如语音识别、机器翻译、自然语言处理和时间序列分析[69, 170]。为了掌握循环模型的基础，本节将提供标准RNN公式的简要概述。

具体来说，在每个离散时间步骤k，标准RNN特别处理一个向量 ∈ RD，连同前一步骤的隐藏状态e ℎ −1 ∈ R ，以产生一个输出向量 ∈ R 并更新隐藏状态到 ℎ ∈ RN。

隐藏状态充当网络的内存，并保留有关它所见过去输入的信息。这种动态内存允许RNN处理不同长度的序列。正式地，它可以写成






其中Wℎ ∈ R ×D是负责将模型输入处理成隐藏状态的权重矩阵，Wℎℎ ∈ R × 是隐藏状态之间的递归连接，W ℎ ∈ R ×N表示用于从隐藏状态生成输出的权重，, ℎ ∈ R 和 ∈ R 对应偏差，tanh表示引入非线性到RNN模型的双曲正切激活函数。换句话说，RNN是非线性递归模型，通过利用隐藏状态中存储的历史知识有效地捕获时间模式。

然而，RNNs有几个局限性。首先，RNNs在有效提取输入序列中的长距离动态方面能力有限。随着信息通过连续的时间步骤传播，网络中权重的重复乘法可能导致信息的稀释或丢失。因此，对于RNNs来说，在进行预测时保留和回忆早期时间步骤的信息变得具有挑战性。其次，RNNs以增量方式处理序列数据，限制了它们的计算效率，因为每个时间步骤都依赖于前一个。这使得并行计算对于它们来说很困难。此外，传统的RNNs缺乏内置的注意力机制，这允许网络捕获输入序列中的全局信息。这种注意力机制的缺失限制了网络选择性地建模数据的关键部分的能力。为了克服这些限制，Transformer和状态空间模型出现了，每种方法都从不同的角度解决了这些挑战。这些两种方法将在后续的小节中进一步阐述。

2.2 Transformers
Transformer[175]是深度学习领域的开创性模型，彻底改变了各种AI应用。它的引入标志着与传统序列到序列模型的显著偏离，通过采用自我注意力机制，促进了对模型输入中全局依赖性的捕获。例如，在自然语言处理中，这种自我注意力能力允许模型理解序列中不同位置之间的关系。






这样的程序然后通过Softmax函数传递，以标准化分数 ?并产生注意力权重，定义为：






除了执行单个注意力函数外，多头注意力被引入以增强模型捕获不同类型关系的能力，并为输入序列提供多种视角。在多头注意力中，输入序列并行通过多个自注意力模块进行处理。每个头独立操作，执行与标准自注意力机制完全相同的计算。然后，每个头的注意力权重被结合起来，创建值向量的加权和。这个聚合步骤允许模型利用来自多个头部的信息，并捕获输入序列中的多样化模式和关系。

数学上，多头注意力计算如下：






其中m是注意力头的数量，⊕是连接操作，WO是将多头注意力分数投影到最终值的线性变换。

2.3 状态空间模型
状态空间模型（SSMs）是一种传统的数学框架，用于描述系统随时间的动态行为[89]。近年来，SSMs在控制理论、机器人技术和经济学等多个领域中发现了广泛的应用[58 59]。在其核心，SSMs通过一组隐藏变量，即“状态”，来体现系统的行为，使其能够有效地捕获时间数据依赖性。与RNNs不同，SSMs是线性模型，具有关联属性。具体来说，在经典的状态空间模型中，制定了两个基本方程，即状态方程和观测方程，通过当前时间t的N维隐藏状态ℎ( ) ∈ R 来模拟输入 ( ) ∈ R和输出 ( ) ∈ R之间的关系。该过程可以写成






其中ℎ′( ) 是当前状态ℎ( )的导数，A ∈ R × 是描述状态如何随时间变化的状态转移矩阵，B ∈ R ×1是控制输入如何影响状态变化的输入矩阵，C ∈ R1× 表示基于当前状态生成输出的输出矩阵，D ∈ R表示决定输入如何直接影响输出的命令系数。一般来说，大多数SSMs在观测方程中省略了第二项，即，设置D ( ) = 0，这可以被认为是深度学习模型中的跳过连接。

2.3.1 离散化。为了符合机器学习设置对各种现实世界场景的要求，SSMs必须经历一个离散化过程，将连续参数转换为离散参数。离散化方法通常旨在将连续时间划分为 ?个具有相等积分区域的离散间隔。为了实现这一目标，作为最具代表性的解决方案之一，零阶保持（ZOH）[138, 225]成功地应用于SSMs，它假设函数值在间隔Δ = [ −1, ]内保持恒定。在ZOH离散化之后，SSM方程可以重写为






其中A = exp(ΔA)，B = (ΔA)−1(exp(ΔA) − I) · ΔB，k是离散时间步长。从这些公式中，很明显离散SSM具有类似于循环神经网络的结构，因此离散SSM可以完成与Transformer基础模型相比，具有更高效率的推理过程。

2.3.2 卷积计算。作为线性系统，离散SSM具有关联属性，因此可以无缝集成到卷积计算中。更具体地说，它可以独立地计算每个时间步的输出，如下所示：






通过创建一组卷积核s K = (CB, ..., CA B, ...)，递归计算可以转换为卷积形式：




其中x = [ 0, 1, ...]和y = [ 0, 1, ...] ∈ R L分别表示输入和输出序列，而 是序列长度。在这个情况下，输入矩阵B ∈ R × ，输出矩阵C ∈ R × ，和命令矩阵D ∈ R × ，而状态转移矩阵保持不变，即A ∈ R × 。

2.3.3 RNN、Transformer和SSM之间的关系。图2描述了循环神经网络（RNN）、Transformer和状态空间模型（SSM）的计算算法。
图2. 代表性模型架构的图示，即循环神经网络（RNN）、Transformer和状态空间模型（SSM）。(a) RNNs在非线性递归框架内运作，便于在自回归推理过程中快速输出。(b) Transformers在多个查询-键对上并行执行矩阵乘法，便于并行训练。(c) SSMs通过线性属性适应递归和卷积计算，融合了RNNs和Transformers的优势，允许SSMs进行递归推理和并行训练。尽管如此，传统的时间不变SSM在上下文感知建模方面不足，导致在特定任务中的性能下降。

一方面，传统的RNN在非线性递归框架内运作，每个计算仅依赖于前一个隐藏状态和当前输入。虽然这种格式允许RNN在自回归推理过程中快速生成输出，但它阻碍了它们充分利用GPU并行计算，导致模型训练速度变慢。另一方面，Transformer架构在多个查询-键对上并行执行矩阵乘法，可以有效地跨硬件资源分布，从而加快了基于注意力模型的训练。然而，当从基于Transformer的模型生成响应或预测时，推理过程可能会很耗时。例如，语言模型的自回归设计需要顺序生成输出序列中的每个标记，这要求在每一步重复计算注意力分数，导致推理时间变慢。如表1所示，与RNNs和Transformers不同，它们仅限于支持一种类型的计算，离散SSMs具有支持递归和卷积计算的灵活性，这得益于它们的线性属性。这种独特的能力允许SSMs不仅实现有效的推理，还实现并行训练。然而，应该注意的是，大多数传统的SSM是时间不变的，这意味着它们的A、B、C和Δ与模型输入x无关。这将限制上下文感知建模，导致SSMs在某些任务中的性能下降，如选择性复制[55]。

3 Mamba1
为了解决传统SSM在上下文感知能力方面的不足，Mamba被[55]提出作为一种潜在的替代方案，承诺成为一个通用的序列基础模型骨干。最近，Mamba-2[28]提出了结构化状态空间对偶性（SSD），建立了一个强大的理论框架，将结构化SSMs与各种形式的注意力联系起来，允许我们将最初为Transformer开发的算法和系统优化转移到SSMs。在这一节中，我们将简要而清晰地介绍Mamba和Mamba-2。

3.1 硬件感知算法的选择性状态空间模型
传统的SSM在建模文本和其他信息密集型数据方面表现出有限的有效性[55]，阻碍了它们在深度学习中的进展。在追求赋予SSMs与Transformer相当的建模能力的过程中，Gu和Dao[55]基于结构化状态空间模型提出了三种创新技术，即基于高阶多项式投影算子（HiPPO）的内存初始化、选择机制和硬件感知计算，如图3所示。这些技术旨在增强SSMs在长距离线性时间序列建模方面的能力。特别是，初始化策略建立了一个一致的隐藏状态矩阵，有效地促进了长距离记忆。然后，选择机制使SSM能够获得内容感知表示。最后，Mamba设计了两种硬件感知计算算法，即并行关联扫描和内存重计算，以提高训练效率。
3.2 HiPPO-based Memory Initialization。
建模和从序列数据中学习是当代机器学习的基础挑战，构成了各种任务的基础，包括语言建模、语音识别和视频处理。在建模复杂和长期时间依赖性方面，内存是一个基本组成部分，包括存储和整合来自先前时间步骤的信息[73]。与RNNs类似，在SSMs中保留和遗忘历史隐藏状态（即矩阵A）在实现令人满意的性能方面起着关键作用。在以前的结构化状态空间序列模型（SSMs）中，已经提出了特殊的初始化建议，特别是在数据可用性有限的情况下。这些特殊的初始化在各种情况下都证明是有益的，包括在复杂值模型的情况下。同样，Mamba主要关注隐藏状态矩阵A的初始化，以捕获复杂的时间依赖性。这是通过利用HiPPO理论[56]和创新的缩放Legendre度量（LegS）来实现的，确保仔细考虑完整的历史上下文而不是有限的滑动窗口。具体来说，HiPPO-LegS为所有历史数据点分配均匀的权重，可以表示为：
其中n是多项式的数目， k表示特定的离散时间步骤。基于HiPPO理论，Mamba引入了两种简单的初始化方法，即S4D-Lin和S4D-Real[57]，如下所示：

其中n是A的第n个元素，对于所有输入维度 = 1, 2, ..., D。通过这种初始化，模型可以学习长期依赖性，通过压缩和重建输入信息信号，经历较小的新步骤退化和较大的旧步骤退化。根据公式，HiPPO-LegS具有有利的理论属性：它在输入时间尺度上保持一致，并且提供快速计算[56]。此外，它具有有界的梯度和近似误差，有助于参数学习过程。

3.3 选择机制。
传统的状态空间模型由于时间不变性属性，无法根据特定模型输入（即内容感知建模能力）产生个性化输出。为了为SSM提供类似于注意力机制的能力，Mamba设计了一个时变选择机制，根据模型输入参数化权重矩阵。这种创新使SSM能够无限期地过滤掉无关信息，同时保留相关细节。正式地说，选择机制涉及设置间隔Δ，并将矩阵B、C作为输x ∈ R × ×D的函数，可以公式化为：
其中SB ∈ R × × , SC ∈ R × × ，和SΔ ∈ R × × 是选择性空间矩阵，它们是输入的函数，以实现内容感知建模。 , , , 和 N分别代表批量大小、输入长度、输入特征大小和隐藏通道数。值得注意的是，WB ∈ R × , WC ∈ R × , 和 WΔ ∈ R ×1是相应组成部分的选择权重（即线性参数化投影），而BroadCastD意味着将结果广播到所有维度 = 1, 2, .., 。随后，选择性SSMs通过使用常见的统计技术，零阶保持（ZOH）[138]，进行离散化，如下所示：


它根据输入x生成输出ut y ∈ R × × 。请注意，Mamba中的时变选择机制具有与Transformer中的注意力机制类似的结构，即两者都根据输入及其投影执行操作，这允许Mamba的SSM实现灵活的内容感知建模。然而，它失去了与卷积的等价性，这对其效率产生了负面影响。

3.4 硬件感知计算。
选择机制旨在超越线性时间不变模型的限制。尽管如此，它对有效训练提出了挑战：SSMs的卷积核变得依赖于输入，导致无法进行并行计算。为了解决这个问题，Mamba利用了两种计算技术，即并行关联扫描（也称为并行前缀和）[64]和内存重计算。首先，并行关联扫描利用线性关联计算的属性和现代加速器（GPU和TPU）的并行性，以内存高效的方式执行选择性SSMs的计算。更具体地说，平行关联扫描围绕构建给定输入的平衡二叉树，并扫掠它从叶子到根。换句话说，平行关联扫描首先通过从叶子到根的遍历（即Sweep-Up），在树的内部节点创建部分和。然后，它逆转遍历，从根开始向上扫掠树，使用部分和构建整个扫描（即Sweep-Down）。

另一方面，Mamba利用了传统的重计算方法，以减少训练选择性SSM层时的整体内存需求。具体来说，Mamba在前向传递的并行关联扫描中避免存储大小为 ( , , , ) 的中间状态，以防止内存扩展。相反，它在后向传递中重新计算这些中间状态以进行梯度计算。通过这样做，重计算避免了在GPU内存单元之间读取 ( ) 元素的必要性。除了优化扫描操作的内存需求外，Mamba-1还将重计算的使用扩展到整个SSM层的效率，包括投影、卷积和激活，这些通常需要大量的内存资源，但可以快速重新计算。

4 Mamba 2
4.1 Mamba-2：状态空间对偶性
Transformers在深度学习的成功中发挥了关键作用，为各种领域带来了革命性的变化。它们启发了各种技术的发展，如参数高效微调[95]、灾难性遗忘缓解[96]和模型量化[196]，旨在从不同角度提高模型性能。为了使状态空间模型能够访问并从最初为Transformer开发的宝贵技术中受益，Mamba-2[28]引入了一个全面的框架，称为结构化状态空间对偶性（SSD），它建立了SSMs和不同形式的注意力之间的理论联系。正式地，




其中M表示SSMs的矩阵形式，使用序列半可分表示，M = CT A : Bi。

值得注意的是，C 和 B 分别代表与输入令牌x 和xi相关的选择性空间状态矩阵。A : 表示与输入令牌从j到i的范围相关的选择性隐藏状态矩阵。本质上，SSD表明，Transformer使用的注意力机制和SSM中使用的线性时变系统都可以被视为半可分矩阵变换。此外，Dao和Gu[28]还证明了选择性SSM等同于使用半可分掩码矩阵实现的结构化线性注意力机制。

基于SSD，Mamba-2设计了一种更高效的硬件计算，通过块分解矩阵乘法算法。具体来说，通过将状态空间模型视为半可分矩阵，Mamba-2将计算分解为矩阵块，在这些块中，对角块代表内部块计算。相比之下，非对角块通过SSM的隐藏状态表示内部块计算。这种方法使Mamba-2能够实现比Mamba-1的并行关联扫描快2-8倍的训练过程，同时保持与Transformer的竞争性。

4.2 SSD（State Space Duality）框架
展示了状态空间模型与一类称为半可分矩阵的结构化矩阵族之间的等价性，揭示了状态空间模型的新属性和算法。
显著改进了线性注意力理论，通过张量收缩的语言对其循环形式提供了明确证明，并将其推广到新的结构化掩码注意力（SMA）家族。同时证明了任何具有快速循环形式的核注意方法都是 SSM，且 SSM 和 SMA 有很大的交集，彼此是对偶的，同时具有 SSM 式的线性形式和类似注意力的二次方形式。
提出了一种基于半可分离矩阵块分解的 SSD 算法，该算法利用了 SSM 线性递推和二次对偶形式，在所有主要效率轴上获得了最优的权衡。基于 SSD 的实现比 Mamba 的优化选择性扫描实现快 2 到 8 倍，同时允许使用更大的循环状态大小（是 Mamba 的 8 倍甚至更高，且几乎不影响速度），与优化过的 softmax 注意力实现（FlashAttention-2）具有高度竞争力，在序列长度 2k 时性能相当，在序列长度 16k 时速度快 6 倍。
4.3 Mamba 2 的架构设计
在架构设计方面，Mamba 2 对 Mamba 块进行了修改，引入分组值注意力（GVA，Grouped-Value Attention）头结构以实现张量并行，并将修改后的并行 Mamba 块与作为内部 SSM 层的 SSD 结合，构建了 Mamba-2 架构。该架构的主要改进是从顺序生成转变为并行生成 SSM 参数，更适用于张量并行等扩展方法。采用 Mamba-2 架构的模型在困惑度和实际运行时间上优于 Mamba 和 Transformer++，并在标准下游评估中匹配或超越了 Mamba 和开源 transformers 的表现。

Mamba2 有许多创新点，以下是一些主要的方面：

Mamba 2是 Mamba 的进一步发展，它提出了 SSD（State Space Duality）框架，并基于此设计了新的体系架构。

与 Mamba 相比，Mamba 2的核心层是对选择性 SSM 的改进，速度提高了2-8倍，同时在语言建模方面继续与 transformers 竞争。新算法使其能够利用更大的状态维度（从16提升至256），在需要更大状态容量的任务上有显著改进。

研究发现注意力和 SSM 是互补的，将4-6个注意力层与 Mamba-2层混合，其表现优于 Transformer++和纯 Mamba-2。

该研究还展示了状态空间模型与一类称为半可分矩阵的结构化矩阵族之间的等价性，揭示了状态空间模型的新属性和算法；显著改进了线性注意力理论，通过张量收缩的语言对其循环形式提供了明确证明，并推广到新的结构化掩码注意力（SMA）家族；证明了任何具有快速循环形式的核注意方法都是 SSM。

在算法层面，提出了基于半可分离矩阵块分解的 SSD 算法，利用 SSM 线性递推和二次对偶形式，在所有主要效率轴上获得了最优权衡。基于 SSD 的实现比 Mamba 的优化选择性扫描实现快2到8倍，同时允许使用更大的循环状态大小（是 Mamba 的8倍甚至更高，且几乎不影响速度）。

这些创新使得 Mamba 在语言、音频和基因组学等多个领域表现出色，在处理长序列时具有更高的效率和性能。但 Mamba 相关算法仍在不断发展和改进中，新的研究可能会带来更多的创新和优化。

4.4 Mamba-1和Mamba-2的对比
在这一小节中，我们总结了Mamba-1和Mamba-2的块设计。图4展示了这两种架构的比较。Mamba-1是从SSM中心视角出发，其中选择性SSM层负责将输入序列X映射到Y。在这个设计中，(A, B, C)的线性投影应用于初始线性投影之后，以创建X。然后，输入令牌和状态矩阵通过选择性SSM单元传递，使用并行关联扫描，以产生输出Y。之后，Mamba-1使用跳过连接来鼓励特征重用，并缓解在模型训练过程中经常发生的退化问题。最后，通过堆叠这种块与标准归一化和残差连接交错，构建了Mamba模型。

至于Mamba-2，它引入了SSD层，旨在从[X, A, B, C]到Y创建一个映射。这是通过同时处理[X, A, B, C]并以与标准注意力架构生成Q、K、V投影相同的方式在块的开始进行单个投影来实现的。换句话说，Mamba-2块通过删除顺序线性投影来简化Mamba-1块，这使得SSD结构的计算速度比Mamba-1中的并行选择性扫描快。此外，在跳过连接后添加了一个归一化层，旨在提高训练稳定性。

5 Mamba模型的进步
状态空间模型和Mamba最近被探索，并成为基础模型骨干的一个有前途的替代方案。如表2所示，大规模Mamba基础模型不仅在学术研究中蓬勃发展，而且在工业界也取得了显著进展，如Falcon Mamba 7B和Mistral 7B，通过在GPU上成功训练证明了它们的有效性。尽管如此，Mamba架构仍然面临挑战，如内存丢失、对多样化任务的泛化能力不足以及与基于Transformer的语言模型相比在捕获复杂模式方面的劣势。为了克服这些挑战，已经做出了大量努力来改进Mamba架构。现有的研究主要集中在修改块设计、扫描模式和内存管理方面。这一节将介绍这三个方面的几个重要技术，并在表3中提供相关研究的总结。
5.1 块设计
Mamba块的设计和结构对Mamba模型的整体性能有重要影响，使其成为一个新的研究焦点。如图5所示，基于构建新Mamba块的不同方法，现有研究可以分为三类：a)集成方法旨在将Mamba块与其他知名模型集成，以便在保持有效性和效率的同时实现平衡；b)替代方法尝试将Mamba块作为高级模型框架中主要层的替代品；c)修改方法专注于修改经典Mamba块内的组件。以下小节将详细介绍这些方法。
4.1.1 集成。鉴于Mamba在捕获长期动态方面的卓越能力，它已广泛与其他模型集成，利用其优势为特定场景提供强大的框架。集成特别包括先进的模型，如Transformers、图神经网络（GNNs）、循环神经网络（RNNs）、卷积神经网络（CNNs）和尖峰神经网络（SNNs）。下面描述了一些具体的例子。

• 基于Transformer的模型在许多任务中表现出色，但它们的二次方计算复杂度在推理过程中仍然阻碍了它们[58]。为了实现高效的生成，一些研究人员提出了将Mamba块与基于Transformer的模型结合起来。例如，Jamba[111]结合了Transformer和Mamba层的块，以应对长期内容的自然语言处理任务，利用了两种模型家族的优势。注意力-Mamba混合模型的性能优于单独的Transformer和Mamba模型，与普通Transformer模型相比，吞吐量更好。Mambaformer[201]利用混合框架预测多个时间序列，包括汇率、小时电力消耗和电力负荷，它在内部结合了Mamba块和Transformer层，分别用于长期和短期依赖性。由于Mamba和Transformer的集成，Mambaformer在长期-短期时间序列预测方面优于基于Transformer的预测器。

• GNN通过消息传递机制在捕获邻近关系方面展现出了有希望的潜力，其中信息通过连接图在堆叠层中传播。然而，这些模型面临一个重大限制，即过度平滑[20]，特别是当试图捕获高阶邻接信号时。为了解决这一挑战，Mamba已被用于图表示学习[103, 115, 179, 205]。例如，Graph Mamba[9]将图结构数据重新格式化为特定顺序的序列令牌，并利用Mamba块中的选择性SSM层构建了一个新的Graph Mamba Network (GMN)架构，它在图表示学习能力方面表现出色，特别是在需要高阶节点依赖性的数据处理方面。

• RNN基础模型在捕获时间动态方面取得了显著成果。然而，RNNs仍然面临重大挑战，包括耗时的递归训练和隐藏状态的内存容量限制。受到最近Mamba基础架构出现的启发，一些研究人员开发了Mamba块与RNNs的融合。例如，VMRNN[171]在时空预测方面取得了最先进的性能，与基于递归和无递归的方法相比，减少了浮点运算（FLOPs）。它通过引入一种新的递归单元来实现这一点，该单元将Mamba块与长短期记忆（LSTM）结合起来。

• CNN基础方法受到局部接受域的限制，导致在捕获全局和长期语义方面表现不佳[55]。众所周知，状态空间模型在学习长期模式方面具有优越能力，一些研究[107, 188, 206]探索了利用Mamba块增强CNN基础模型的潜力，特别是在计算机视觉领域。例如，MedMamba[216]和nnMamba[53]展示了如何通过集成视觉Mamba块来提高CNN在图像分析任务中的性能。

• SNN最近被提出作为一种有前景的网络架构，灵感来自大脑中生物神经元的行为：通过离散尖峰在神经元之间传递知识。SNN的一个关键优势在于其潜在的低功耗实现，因为它们可以利用神经活动的稀疏和事件驱动特性。受到SNN和SSM在低功耗实现方面的启发，以及SSM在长期学习能力方面的优越性，一些开创性的研究深入研究了将这两种方法结合起来。例如，SpikeMba[106]将它们结合起来处理对突出对象的信心偏差，并在视频序列中捕获持久依赖性。通过广泛的评估，作者声称将这两种模型结合起来提高了视频定位任务的有效性，精确地时刻检索和高光检测。

4.1.2 替代。受到选择性SSM在高效计算和长序列学习方面的卓越能力的启发，采用Mamba模块替代经典建模框架中的关键组件，如U-Net[151]和扩散模型[70]，已经引起了很多关注。通过引入选择性SSM层，这些方法实现了长距离学习和高效计算，以满足特定任务的需求。下面，我们展示了使用Mamba模块替代的一些实例，特别是高级框架，如U-Net和扩散模型。

• U-Net。许多努力[110, 163, 180, 181]已经做出，以将U-Net与Mamba在捕获复杂和广泛语义方面的能力结合起来，以推进计算机视觉任务中的模型性能。例如，Mamba UNet[180]专门使用视觉Mamba块构建了一个类似U-Net的模型（即，一个编码器-解码器模型，融合了跳跃连接），用于医学图像分割。他们的评估表明，Mamba-UNet超过了几种U-Net变体，这可以归因于Mamba块在处理长距离补丁序列方面的有效性和效率。

• 扩散模型。一些努力[46, 48, 136]已经尝试构建一种新型的扩散模型，扩散状态空间模型（DiS），它用状态空间骨干替代了典型的骨干（例如，CNNs、注意力、U-Nets）。鉴于Mamba块在适应长距离依赖性方面的显著效率和有效性，DiS被区别于使用扩散模型生成更长序列[46]。例如，Oshima等人[136]提出了一个基于Mamba的扩散模型，显著降低了长视频序列的内存消耗，同时与基于Transformer的模型相比，在性能指标上仍然保持了竞争力。此外，MD-Dose[48]和P-Mamba[211]在扩散模型的反向过程中构建了使用Mamba块的噪声预测器，最终为医学图像处理生成了特定目标。

• 其他。除了U-Net和扩散模型，还有一些替代品。例如，Res-VMamba[19]在残差学习框架中采用了视觉Mamba块，用于食品类别分类。此外，SPMamba[101]采用了TF-GridNet[190]，这是最近开发的一种时频模型，作为其基础架构，然后在Transformer组件之后使用双向Mamba块。这种调整使模型能够高效地包含更广泛的上下文信息，用于语音分离任务。

4.1.3 修改。除了直接使用Mamba块的集成和替代方法外，还进行了一些努力来修改Mamba块，目的是在不同场景中增强其性能。例如，Jamba[111]借鉴了专家混合（MoE）[45, 82]的概念，使他们的混合（Transformer-Mamba）仅解码器模型能够以更少的计算进行预训练，并允许灵活的目标特定配置。值得注意的是，Jamba模型（56B可用参数，12B活动参数，4GB KV缓存）需要比代表性的基于Transformer的语言模型LLaMA-2-7B（6.7B可用参数，12B活动参数，128GB KV缓存）小32倍的KV缓存，同时提供了更多的可用和活动参数。这使得Jamba能够在单个A100 GPU（80GB）上吞吐140K的上下文长度，这是LLaMA-2-70B支持的长度的七倍。除了MoE，一些研究提出了将SSM层修改为K路结构，涉及使用并行SSM单元处理模型输入，允许从多个角度捕获信息和知识。例如，Sigma[178]开发了一种新的基于Mamba的视觉编码器，它通过使用并行SSM层来处理多模态输入。UltraLight VM-UNet[194]提出了一种具有并行SSM单元的视觉Mamba层，该层在不同通道中处理深度特征。总之，通过实施这些修改（即，K路，MoE），这些基于Mamba的模型获得了增强的学习能力，特别是在处理多模态输入和快速适应多尺度任务方面。此外，一项开创性研究，Mamba®，提出了一种新的方法，建议在将输入通过SSM层之前，将寄存器均匀地整合到视觉输入令牌中。这种修改旨在增强图像补丁序列方向的表示，从而使Mamba块的单向推理范式适用于视觉任务。尽管取得了这些成功，但探索修改Mamba块仍然是一个有前景但尚未充分探索的领域。

5.2 扫描模式
并行关联扫描操作是Mamba模型中的一个关键组件，旨在解决选择机制引起的计算问题，加速训练过程，并减少内存需求。它通过利用时间变化SSMs的线性属性来设计硬件级别的内核融合和重计算。然而，Mamba的单向序列建模范式阻碍了对各种数据（如图像和视频）的全面学习过程。为了缓解这个问题，一些研究专注于设计有效的扫描方法，以增强模型性能并促进Mamba模型的训练过程。如图6所示，现有研究集中在开发扫描模式技术，可以分为两类：1) Flatten Scan方法从平铺的角度处理模型输入；2) Stereo Scan方法跨维度、通道或尺度扫描模型输入。
4.2.1 扁平扫描。扁平扫描是指将模型输入展平为令牌序列，并相应地从不同方向进行扫描。这种类型的扫描通常用于一维（例如，时间序列）和二维（例如，图像）数据。在本节中，我们进一步将其分为四类，即双向扫描、扫描扫描、连续扫描和高效扫描。

• 双向扫描。借鉴双向递归神经网络（Bi-RNNs）[155]的概念，Visual Mamba[236]为视觉数据引入了一种扫描方法，称为双向扫描（Bi-Scan），涉及使用同时向前和向后的SSM处理输入令牌，从而增强了模型的空间感知处理能力。最近，一些研究已经利用Bi-Scan方法来促进其基于Mamba的模型的学习能力[105]。例如，DPMamba[87]和SPMamba[101]都利用了一对双路径（向前和向后）的选择性SSM来模拟语音信号，实现了语音分离的双向知识处理。这些显著的成功可以归因于Bi-Scan的有效性及其易于部署。

• 扫描扫描。如图6所示，扫描扫描技术按特定方向处理模型输入，类似于清洁工仔细清扫地板[189, 216]。例如，Cross-Scan[121]涉及将输入图像划分为补丁，然后将其沿四个不同路径展平，这被视为两个双向扫描的融合。通过采用这些互补的遍历路径，Cross Scan使每个图像补丁能够高效地整合来自不同方向的邻居信息，从而促进了信息丰富、接受域的建立。Omni-Scan[163, 229]结合了从多个方向对图像信息流的建模，例如2（向前和向后）×4（左-右、上-下、右上-左下、左上-右下）。这种策略增强了在各个方向上对上下文信息的全局建模能力，使提取全面的全局空间特征成为可能。

• 连续扫描。为了确保输入序列的连续性，连续扫描技术扫描相邻的列或行之间的令牌[66]，如图6所示。例如，为了更好地处理2D空间输入，PlainMamba[203]引入了一种连续扫描方法，称为连续扫描，它扫描列（或行）之间的相邻令牌，而不是在Cross Scan中前往相反的令牌。此外，Hilbert Scan[66]基于Hilbert矩阵沿着曲折的路径行进。根据他们的评估结果，可以推断出，增强输入令牌的语义连续性在各种视觉识别任务中为基于Mamba的模型带来了优越的性能。

• 高效扫描。与上述专注于实现更全面的输入建模的扫描方法不同，高效扫描方法旨在加速训练和推理过程。通常，高效扫描将给定输入分成几个部分，并并行处理它们，从而减少计算需求。例如，Efficient-2D Scan[139]通过跳过补丁来处理图像，从而减少了四倍的计算需求，同时保留了全局特征图。此外，Gao等人[50]在他们的Mamba框架中引入了一种有效的双向子空间扫描方案。该方案旨在为4D光场超分辨率任务高效捕获长期空间-角度对应关系。具体来说，它将补丁序列分解为两部分，并通过两个双向扫描方案进行处理。通过这样做，扫描方法降低了输入长度，并解决了长期记忆问题，而不会牺牲完整的4D全局信息。

4.2.2 立体扫描。通过从额外的角度对模型输入进行建模，立体扫描方法在扫描过程中捕获更广泛知识的能力方面优于扁平扫描方法。这种增强的能力允许更全面地理解模型输入。具体来说，这些方法可以分为三个主要类别：层次扫描、时空扫描和混合扫描。层次扫描从不同层次处理输入，而时空扫描从时间和空间的角度考虑输入模式。此外，混合扫描结合了多种扫描方法，以利用不同扫描技术的益处。

• 层次扫描方法涉及使用不同内核大小的扫描来从全局到局部或从宏观到微观捕获语义知识[26, 63, 162, 181]。例如，Mamba-in-Mamba层次编码器[24]用于红外小目标检测，结合了内部和外部选择性SSM块。内部的一个专门用于捕获视觉补丁之间的相互作用以提取局部模式。相反，外部块旨在描述视觉句子之间的关系以捕获全局特征。HiSS[12]将输入序列划分为块，并连续地对块特征进行层次建模以进行连续序列预测。块首先由低级SSM单元处理，然后由高级SSM块将处理后的特征映射到输出序列。

• 时空扫描。由于现实世界中动态系统的普遍性，对时空扫描方法的兴趣日益增加，以提高Mamba块的性能[207, 209]。例如，VideoMamba[102]将原始的2D扫描扩展为两个3D扫描：空间优先扫描和时间优先扫描。结合这两种扫描方法，VideoMamba在处理长、高分辨率视频方面表现出色。此外，ChangeMamba[21]集成了三种时空扫描机制（顺序建模、交叉建模和并行建模），以实现多时态特征之间的上下文信息交互，用于遥感变化检测。

• 混合扫描。为了全面地建模特征，许多努力专注于结合不同扫描方法的优势[29, 32, 53, 163, 231]，即混合扫描。例如，Mambamixer[10]展示了Scan of Scan，它动态地使用一组图像扫描方法，即Cross-Scan、Zigzag Scan和Local Scan，来遍历图像补丁。Mambamixer还引入了双选择机制，以在令牌和通道之间混合信息。通过这样做，他们展示了与其他视觉模型相媲美的性能。

Pan-Mamba[68]引入了两种基于Mamba架构的扫描方法：通道交换扫描和跨模态扫描。通过结合这两种扫描方法，Pan-Mamba增强了其在图像锐化中的高效跨模态信息交换和融合能力。

5.3 内存管理

像RNNs一样，状态空间模型中的隐藏状态内存有效地存储了先前步骤的信息，因此在SSM的整体功能中起着至关重要的作用。虽然Mamba引入了基于HiPPO的方法进行内存初始化[55]，但在SSM单元的内存管理方面仍然存在挑战，包括在层之间传输隐藏信息以及实现无损内存压缩。为此，一些开创性的研究提出了不同的解决方案，包括内存初始化、压缩和连接。例如，Ezoe和Sato[35]尝试通过使用平衡截断方法在模型重新训练期间改进选择性SSM的初始化过程。此外，DGMamba[123]引入了一种隐藏状态抑制方法，以增强状态空间模型中隐藏状态的领域泛化能力。这种方法旨在减轻这些隐藏状态的负面影响，从而缩小不同领域之间隐藏状态的差距。同样，DenseMamba[67]提出了一种密集连接方法，以增强SSMs中层与层之间隐藏信息的传播。这种策略旨在通过选择性地将较浅层的隐藏状态整合到更深层中，减轻内存退化并保留详细的信息以用于输出生成。

6 Mamba 的应用
Mamba 架构作为选择性状态空间模型（SSM）的扩展，具备处理序列和非序列数据的能力。本章探讨 Mamba 在不同任务中的应用，包括序列数据、非序列数据以及多模态数据，并总结其在实际场景中的显著应用。

6.1 序列数据
序列数据是指以特定顺序组织的数据，Mamba 凭借其高效建模长期依赖性的能力，在多种序列任务中表现出色。

6.1.1 自然语言
Mamba 在自然语言处理（NLP）领域展现出强大的潜力，成为大型语言模型的有力替代品。例如：

MambaByte：利用 Mamba 捕获长期依赖性，避免子词标记化的归纳偏差，在长期语言建模任务中超越了最先进的 Transformer。
Jamba 和 BlackMamba：结合专家混合（MoE）技术，通过线性复杂度生成与 MoE 的快速推理能力，提升了语言处理性能。
6.1.2 视频
视频理解和生成的核心在于学习时空表示，Mamba 能有效区分短期动作和解释长视频内容。例如：

VideoMamba：通过 3D 卷积将输入视频投影到时空补丁中，使用双向 Mamba 块编码向量表示，适用于视频理解和生成任务。
Vivim：引入时态 Mamba 块，压缩时空表示，用于医学视频分割。
6.1.3 时间序列
Mamba 在时间序列分析中表现出卓越的效率，适用于股市分析、交通建模和天气预报等任务。例如：

TimeMachine：利用 Mamba 捕获多变量时间序列中的持久模式，确保线性复杂度计算和最小内存占用。
Mambaformer：结合选择性 SSM 和注意力层，用于长期和短期预测。
6.1.4 语音
Mamba 在语音分离和增强任务中展现了显著优势，降低了计算复杂度。例如：

SPMamba 和 DPMamba：利用双向 Mamba 模块捕获更广泛的上下文信息，提升语音分离性能。
TRAMBA：结合 Transformer 和 Mamba，显著降低内存消耗，优化移动平台上的语音质量。
6.1.5 运动
Mamba 在人体运动建模中表现优异，能够捕获复杂的时空模式。例如：

Motion Mamba：通过层次化 SSM 层捕获时间模式，确保运动一致性。
MambaMOS：设计专门的运动感知模型，实现高质量、长序列运动生成。
6.2 非序列数据
非序列数据不遵循特定顺序，Mamba 在图像、图结构数据和点云等领域展现出了出色的成功。

6.2.1 图像
Mamba 在计算机视觉任务中表现出色，显著降低了计算时间和内存占用。例如：

Vision Mamba：结合双向 SSM 和位置嵌入，实现全局视觉语义建模，比 Vision Transformers 更高效。
VMamba：引入 2D Selective Scan (SS2D)，使 Mamba 能有效处理视觉数据。
6.2.2 图结构数据
Mamba 在图建模中展示了强大的长期建模能力和高效率。例如：

Graph-Mamba：结合图展平机制和 Mamba 提供的选择机制，促进输入依赖的上下文过滤。
GMN：基于选择性 SSMs 的图神经网络格式，超越 GNNs 和基于 Transformer 的模型。
6.2.3 点云
Mamba 在点云分析中显著降低了计算成本，适用于机器人技术和自动驾驶。例如：

PointMamba：采用分层扫描策略，利用普通 Mamba 提取序列化点令牌特征。
Point Cloud Mamba：显著减少内存使用量，性能优于基于 Transformer 的同类产品。
6.3 多模态数据
Mamba 在多模态任务中整合语言和图像等数据，提供了有价值且互补的信息。例如：

VL-Mamba：探索视觉-语言任务的潜力，通过连接器模块对齐视觉补丁与语言令牌。
文本控制的运动 Mamba：根据文本查询动态捕获全局时间信息，增强人体运动理解。
Fusion-Mamba 和 Sigma：融合来自不同模态的互补信息，分别改进目标检测和语义分割。
第7章 应用
Mamba 在多个领域中展现出广泛的应用价值，包括自然语言处理、计算机视觉、语音分析、药物发现、推荐系统以及机器人技术和自主系统。

7.1 自然语言处理
Mamba 在问答系统和文本摘要任务中表现出色，解决了传统模型的计算效率问题。

7.1.1 问答系统
Mamba-Chat：通过状态空间表示维护对话理解，确保上下文感知。
Jamba 和 DenseMamba：结合 MoE 和隐藏状态整合技术，提升问答任务性能。
7.1.2 文本摘要
LOCOST：基于状态空间模型，处理长文档摘要，显著降低内存和推理开销。
SAMBA：结合滑动窗口注意力和选择性序列压缩，提升吞吐量和性能。
7.2 计算机视觉
Mamba 在疾病诊断和运动识别与生成任务中展现了优越性能。

7.2.1 疾病诊断
U-Mamba 和 SegMamba：结合 CNN 和 SSM，提升医学图像分割性能。
CMViM 和 ProMamba：通过跨模态对比学习和提示技术，增强疾病诊断和息肉分割能力。
7.2.2 运动识别与生成
HARMamba 和 Simba：利用双向 SSM 架构，提升实时人体运动识别和骨架动作识别性能。
Motion Mamba 和 InfiniMotion：生成连续、长时间的人体运动，降低计算资源需求。
7.3 语音分析
Mamba 在语音分离、标记和增强任务中表现出显著优势。

7.3.1 语音分离和标记
DPMamba 和 SPMamba：捕获动态时间依赖性，提升语音分离性能。
DASS 和 MAMCA：结合知识蒸馏和选择性 SSM，优化音频标记和调制分类任务。
7.3.2 语音增强
TRAMBA 和 oSpatialNet-Mamba：通过混合架构和长期多通道增强，显著降低内存消耗并提升语音质量。
7.4 药物发现
Mamba 在蛋白质设计、分子设计和基因组分析中降低了复杂性，提升了效率。

PTM-Mamba 和 ProtMamba：高效处理长序列，为蛋白质设计提供关键工具。
Saturn 和 Caduceus：优化分子设计和基因组建模，提升预测准确性和训练上下文长度。
7.5 推荐系统
Mamba 在个性化推荐任务中展现了高效性和扩展潜力。

Mamba4Rec 和 RecMamba：通过选择性 SSM 提升模型性能，显著降低训练时间和内存成本。
EchoMamba4Rec 和 Mamba4KT：捕获用户交互复杂模式，提升推荐精度和知识追踪研究效率。
7.6 机器人技术和自主系统
Mamba 在机器人技术和自主系统中展现了优越性能和扩展潜力。

RoboMamba 和 MaIL：通过端到端训练和模仿学习，提升视觉常识和机器人推理能力，同时确保高效微调和推理。
7 挑战与机遇

前面的部分全面回顾了Mamba的最新先进技术和多样化应用。然而，Mamba的研究仍处于起步阶段，存在相当大的挑战和机遇。

7.1 基于Mamba的基础模型

通过将模型规模扩大到数十亿级别，并在大规模混合源语料库上进行训练，基础模型（FMs）展现出了令人印象深刻的零样本学习能力，使FMs能够在广泛的通用任务中表现出色[13]。作为代表性的例子，最近几年见证了基于Transformer的大型语言模型的蓬勃发展，特别是ChatGPT，激发了对各个领域基础模型的探索热情。尽管Transformer是成功的主要驱动力，但它们面临着紧迫的计算和内存效率问题[172]，这随着基于注意力模型的规模呈指数级增长的训练内存和劳动密集型的自回归解码而增长。作为Transformer的有前途的替代品，Mamba[28, 55]最近出现了。Mamba提供了Transformer的内容感知学习能力，同时将计算与输入长度线性扩展，使其在捕获长期依赖性和提高训练和推理效率方面有效。鉴于这些优势，开发特定领域的基于Mamba的基础模型具有巨大潜力，这为解决基于Transformer的模型所面临的问题提供了机会。

7.2 硬件感知计算

基础模型以其庞大的规模和密集的矩阵运算（如矩阵乘法和卷积）而闻名，需要使用GPU和TPU等尖端硬件进行高吞吐量的训练和推理。这些先进的硬件使研究人员能够使用更大的数据集并在各个领域取得最先进的性能。然而，现有的基础模型还没有充分利用硬件的计算能力，导致模型效率有限[172]。作为提高计算效率的有前途的替代品，Mamba-1[55]和Mamba-2[28]提出了硬件感知计算算法，即并行关联扫描和块分解矩阵乘法。这些算法考虑了GPU和TPU的固有特性，包括设备间消息传输等因素，为解决计算效率问题提供了新的视角。受此启发，探索新的硬件高效算法，如FlashButterfly[47]，以优化硬件利用，为SSMs以及其他架构如Transformers和RNNs提供了节省资源和加速计算的有前途的途径。

7.3 值得信赖的Mamba模型

SSM的发展预计将为电子商务、医疗保健和教育等多个行业带来显著的好处。与此同时，像许多现有的架构一样，Mamba模型可能是数据依赖的，可能对用户和社会构成严重威胁[130]。这些威胁源于几个因素，如不稳定的决策制定、隐私问题等。因此，确保Mamba模型的可信任性在四个关键维度上至关重要[116]：安全性&鲁棒性、公平性、可解释性和隐私。

7.3.1 安全性&鲁棒性。大型基础模型已被证明对输入的小扰动高度敏感，这可能危及这些模型在安全关键应用中的安全性和鲁棒性[44, 135, 191]。同时，基于Mamba的模型也不免于这些漏洞[128]。在追求成为Transformer的可靠替代品的过程中，有必要研究和增强基于Mamba模型的安全性和鲁棒性。具体来说，模型输出应对输入的小扰动保持鲁棒。一个可能的解决方案可能涉及在将提示输入到基于Mamba的模型之前自动预处理它们。此外，作为一种代表性技术，对抗性机器训练[78]可以用来增强基于Mamba模型的安全性和鲁棒性。



7.3.2 公平性。大型基础模型在广泛的数据集上进行训练，往往会无意中暴露出训练语料库中存在的偏见和刻板印象[126]，这可能在生成的输出中表现出来。例如，在LLMs领域，偏见可能导致受到用户档案属性如性别和年龄影响的歧视性回应，加强刻板印象并不公平地对待特定用户群体[86]。虽然最近已经做出了努力来解决LLMs中的公平性问题，但在Mamba模型的非歧视性和公平性方面的研究仍然存在差距。因此，需要进一步的探索和研究来弥合这一差距。

7.3.3 可解释性。深度学习模型经常因其“黑盒”特性而受到批评，深度学习模型的可解释性已经成为研究社区中的一个热门话题，它表示理解和解释模型生成的决策或预测的能力[34]。通过解释模型预测，用户可以基于模型的输出做出更明智的决策。为此，已经提出了几种技术来为基于注意力机制的神经架构提供合理的内在解释[74]。此外，研究人员还研究了基于Transformer的语言模型生成自然语言描述以解释其答案的能力[214]。尽管越来越多的研究试图充分利用Mamba，但对Mamba模型的功能进行理解的研究仍处于早期阶段，需要进一步的调查。

7.3.4 隐私。保护隐私在用户和基于Mamba的模型之间建立信任至关重要。当用户相信他们的隐私得到尊重时，他们更有可能与AI系统互动，分享相关信息，并在不担心数据被滥用的情况下寻求帮助。因此，这种信任对于Mamba模型的广泛采用和接受至关重要。一个有效的策略是交叉验证Mamba模型的输出并筛选敏感内容[93]。此外，联邦学习有望在Mamba模型的训练过程中加强隐私保护，其中模型在许多分散的边缘设备或服务器上进行训练，这些设备或服务器上存放着本地数据样本，而不进行数据交换。这种方法有助于保持数据的本地化和隐私。此外，将隐私意识的正则化技术（如差分隐私约束）整合到训练中，有望防止过度拟合敏感数据。

7.4 将Transformer的新兴技术应用于Mamba

Transformer作为主导的骨干，已经引领AI社区开发了许多旨在提高基于注意力模型性能的独特工具。幸运的是，通过SSMs和注意力的连接，Mamba-2[28]引入的SSD框架允许我们为Transformer和Mamba开发共享的技术和库。鉴于此，一个重要的未来方向出现了，即探索如何将为基于Transformer的模型设计的新兴技术有效地应用于基于Mamba的模型。

7.4.1 参数高效微调。大型基础模型将其参数规模扩大到数十亿，已经在多个领域取得了突破性进展。然而，它们的庞大规模和计算需求在为特定下游任务定制时提出了重大挑战。为此，已经提出了几种参数高效微调（PEFT）技术，包括LoRA[72]和Adapter家族[49, 91]，这些技术涉及在微调期间最小化参数调整或对计算资源的需求。从最近在利用PEFT为基于Transformer层构建的大型语言模型方面取得的成就中汲取灵感，将PEFT应用于Mamba模型已经出现了一个有趣的话题，目标是扩大Mamba在下游任务中的应用范围。例如，LoRA（低秩适应）的部署预计将促进SSD模型的快速微调，从而实现Mamba在各个领域的广泛应用。然而，为Mamba基础模型实施这些PEFT技术的具体细节尚未确定，需要进一步的调查。

7.4.2 缓解灾难性遗忘。灾难性遗忘，也称为灾难性干扰，是指在机器学习模型中观察到的现象，当它们在新任务上进行训练时，会在以前学习的任务上经历显著的性能下降[92]。这个问题对于基础模型来说是一个挑战，因为它们需要保留预训练任务的知识，并在不同的下游领域展示一致的性能。作为基础模型的一个有前途的架构，Mamba需要进行彻底的调查以解决灾难性遗忘问题。最近的研究表明，通过奖励最大化和分布匹配策略[96, 97]来解决这一挑战。此外，还开发了持续学习方法来缓解基于Transformer的语言模型中的灾难性遗忘[90, 187]。这些技术也可以通过连接SSMs和注意力应用于Mamba模型，但尚未探索。

7.4.3 检索增强生成（RAG）。作为AI中最复杂的技术之一，RAG可以提供可靠和最新的外部知识，为多种任务提供重要的实用性[30, 99]。大型语言模型最近展示了突破性的语言理解和生成能力，尽管它们在内部知识过时和幻觉方面的固有局限性。鉴于RAG在提供当前和有价值的补充信息方面的强有力能力，检索增强的LLM已经出现，利用外部知识数据库来增强LLMs的生成质量[22]。同样，RAG可以与Mamba语言模型集成，协助它们产生高质量的输出，这是一个有前途的未来研究方向。

小结
Mamba作为一种新兴的深度学习架构，在多个领域，如语言生成、图像分类、推荐和药物发现等方面展现出了显著的成功，这得益于其强大的建模能力和计算效率。最近，越来越多的努力被投入到开发具有更强大表示学习能力和更低计算复杂度的基于Mamba的深度学习模型中。鉴于Mamba的快速发展，迫切需要一个系统的概述。

Mamba 架构凭借其高效的长期依赖性建模能力和灵活的扩展性，在序列、非序列和多模态数据任务中展现了广泛应用前景。无论是自然语言处理、计算机视觉还是药物发现等领域，Mamba 都为现有方法提供了有力补充和替代方案。

最近，一篇匿名投稿的论文「MAMBA-3: IMPROVED SEQUENCE MODELING USING STATE SPACE PRINCIPLES」在AI圈引起了不小的波澜。作为备受关注的Mamba架构的最新迭代，Mamba-3不仅在性能上全面超越了它的前辈Mamba-2，更是在多个维度上展示了其作为Transformer架构有力竞争者的巨大潜力。

这篇论文目前是匿名状态，处于双盲评审中，我们还无法得知其背后的英雄团队。但从其扎实的研究和惊艳的结果来看，这无疑是一项重磅工作。


论文标题: MAMBA-3: IMPROVED SEQUENCE MODELING USING STATE SPACE PRINCIPLES
论文地址: https://openreview.net/pdf?id=HwCvaJOiCj
研究背景：超越Transformer的探索
自从Transformer问世以来，它就以其强大的性能统治了几乎所有的序列建模任务。但它并非完美，其核心的自注意力机制（Self-Attention）所带来的二次方计算复杂度和线性增长的内存占用（KV Cache），使得它在处理超长序列时变得异常昂贵和低效。

为了解决这个问题，研究者们提出了许多“亚二次方”模型，其中，状态空间模型（State Space Models, SSMs）如Mamba，以其线性计算复杂度和恒定的内存占用，成为了最有前途的挑战者之一。像Mamba-2这样的模型，虽然在推理效率上取得了显著进步，但在模型质量和某些关键能力（如状态追踪）上做出了一些妥协。

而Mamba-3的出现，似乎就是要打破这种“鱼与熊掌不可兼得”的局面。它从一个“推理优先”的视角出发，旨在打造一个在质量、能力和效率上三者兼得的完美模型。

Mamba-3的核心革新
Mamba-3在前代Mamba-2的基础上，引入了三项源于经典状态空间理论的深刻改进，使其在表达能力、状态追踪和硬件效率上都实现了质的飞跃。

更精确的离散化：梯形递归
这是Mamba-3最核心的创新。SSM的本质是将连续时间系统离散化。Mamba-2采用的是较为简单的欧拉法（Euler's method），这是一种一阶近似，虽然简单，但牺牲了精度。
Mamba-3则升级为广义梯形法则（generalized trapezoidal rule）。如上图所示，梯形法则通过对区间两端点进行加权平均来近似积分，是一种二阶方法，能够更精确地捕捉连续动态，从而生成一个更具表达能力的循环过程。论文的消融实验也证明，这一改进与新的偏置（BC bias）相结合，甚至使得之前模型中必备的短卷积（short causal convolution）都变得可选了，足见其强大。
更丰富的状态追踪：复数状态空间
为了追求极致的简洁和训练速度，Mamba-2将状态转移矩阵简化为了一个实数标量，但这削弱了模型追踪“旋转”动态的能力，导致其在一些需要精确状态追踪的任务（如奇偶校验、模运算）上表现不佳。

Mamba-3重新引入了复数状态空间（Complex-valued SSMs）。复数能够自然地表达旋转，从而恢复了这种关键能力。巧妙的是，研究者们发现这种复数更新等价于在模型的输入和输出上应用一种数据依赖的旋转位置编码（RoPE），使得模型可以在不引入过多计算开销的情况下，高效地实现复数动态。

实验结果令人印象深刻：在奇偶校验（Parity）和模运算（Modular Arithmetic）等形式语言任务上，Mamba-3几乎完美地解决了这些任务，而Mamba-2则完全失败，表现和随机猜测无异。

更高的硬件效率：MIMO范式
为了让解码过程中的计算单元（如GPU）“忙起来”，Mamba-3引入了多输入多输出（Multiple-Input Multiple-Output, MIMO）的SSM范式。
简单来说，传统的SISO（单输入单输出）模型在更新状态时使用的是外积（outer product），计算量/内存访问比（即算术强度）较低，导致硬件闲置。Mamba-3的MIMO变体则改为使用矩阵乘法（matrix product）进行状态更新，显著提高了算术强度。这意味着在解码的每一步，MIMO Mamba-3都能进行更多的有效计算，从而更好地利用硬件，提高整体的推理效率，而这一切并不需要增加状态大小。

惊艳的实验结果
Mamba-3在一系列语言建模和合成任务上都取得了SOTA（State-of-the-Art）或极具竞争力的结果。

语言建模新标杆
在标准的下游语言建模评测中，研究者们训练了从180M到1.5B不同参数规模的模型。结果显示，在每一个规模上，Mamba-3都稳定地优于Mamba-2、Gated DeltaNet和Transformer等一众强力基线。
更有趣的是，MIMO版本的Mamba-3在参数量匹配的情况下，平均准确率比SISO版本还要高出超过1个百分点，证明了MIMO在提升模型质量上的价值。

Mamba-3通过回归状态空间模型的基本原理，引入了梯形递归、复数状态和MIMO范式这三大法宝，成功地在模型质量、核心能力和推理效率这三个关键维度上实现了全面突破。它不仅为序列建模领域提供了一个更强、更快的SOTA模型，也为我们揭示了超越Transformer架构的更多可能性。

Mamba-3的出现，可能会进一步加速业界对SSM架构的探索和采纳。虽然在某些检索任务上与Transformer仍有差距，但其展现出的巨大潜力和快速的迭代步伐，让人不禁对其未来充满期待。

IT之家 10 月 16 日消息，今天下午，通义千问 Qwen 通过公众号宣布：Qwen Chat Memory 正式上线，标志着 Qwen 拥有“长记忆”，能理解用户的上下文、保留重要信息、回忆过往对话。具体来看，其能够存下与用户相关的记忆，并在对话中主动关联上下文。
1.1模型架构设计
通义千问（Qwen）1 系列模型采用经典的 Transformer 解码器架构，是标准的自回归大语言模型框架。在此基础上，Qwen 引入了一些改进和定制设计：

Transformer Decoder 架构：Qwen 是纯解码器的大模型（即 GPT 类架构），堆叠多层自注意力和前馈网络组成。这种架构善于处理生成类任务。

分组查询注意力（Grouped Query Attention, GQA）：模型的多头注意力采用分组查询机制，共享一部分 Key/Value 头，从而降低大模型推理的显存和计算开销，提高推理效率。例如，Qwen1.5-110B 模型使用了 GQA，使得在不损失性能的前提下大幅优化了推理速度。

位置编码方式： Qwen 使用旋转位置编码 RoPE（Rotary Position Embedding）来引入位置信息。RoPE 在诸多大模型中已被验证有效（如 PaLM、LLaMA 等)，可以使模型具备相对位置感知能力。特别地，Qwen 在实现中使用了 FP32 精度 来计算 RoPE 的频率矩阵，以确保在长上下文情况下的数值稳定性和精度。这为后续扩展上下文长度打下基础。

权重解耦（Untied Embeddings）：与部分 Transformer 实现将词嵌入层和输出投影层权重绑定不同，Qwen 选择了解耦的词表嵌入设计，即输入嵌入矩阵和输出投影矩阵分开，不共享参数。实验发现这可以提升模型效果，但代价是略增内存消耗。

去 Bias 处理：受 PaLM 等模型经验影响，Qwen 去除了大部分线性层中的偏置项，以简化模型和提高训练稳定性。但有所不同的是，在注意力层的 Q、K、V 投影中保留了偏置。研究表明，在 QKV 添加偏置有助于增强模型长上下文外推能力（即在上下文长度超出训练范围时保持稳定的注意力分布）。

预归一化与 RMSNorm：Qwen 采用 Pre-Norm 架构，即在每个子层（注意力或前馈）之前执行归一化。归一化层则使用 RMSNorm 而非传统 LayerNorm。RMSNorm通过仅使用均方根统计量归一化向量，提高了运行效率，并且经验证性能与 LayerNorm 相当。Pre-Norm 的使用结合 RMSNorm，有利于提升深层网络的训练稳定性。

激活函数与前馈结构：Qwen 的前馈网络激活函数采用 SwiGLU。SwiGLU 是 Swish 与门控线性单元 (GLU) 的结合，近期在 PaLM 等模型中被证明效果优于传统的 GeLU 等激活函数。同时，Qwen 将前馈层隐藏单元维度从传统的4倍隐藏规模缩减为约3倍，以配合GLU的门控机制减少参数量，但性能无明显损失。

上下文长度扩展机制：针对长上下文需求，Qwen 的架构特别融入了长序列支持机制。虽然预训练时上下文长度通常为2048（部分新版小模型已扩展至8192），但 Qwen 在推理阶段通过无需重新训练的技巧实现了长上下文扩展。具体来说，Qwen结合了 NTK 插值方法和窗口化注意力策略：

利用 NTK-aware缩放插值 技术，对位置编码进行缩放插值，以减缓长距离位置上高频信息随长度增加而衰减的问题。动态的 NTK 插值会根据不同段落（chunk）自适应调整缩放比例，避免上下文特别长时性能急剧下降。

引入LogN Scaling 以及 分层窗口注意力策略，模拟长序列Transformer。如在底层层采用较小窗口限制注意力范围，在高层则逐渐增大窗口直至全局注意力。通过对低层和高层使用不同的注意力窗口，模型在不增加计算量的情况下掌握长程依赖，同时确保训练稳定。

😎得益于上述设计，Qwen1.5 全系列模型均支持最长 32768 个 token 的上下文（32K tokens，相比常规模型的2K或4K长度有数量级提升），在处理长文档时具备显著优势。此外，还有专门的超长版本 Qwen-Long，可将上下文扩展到百万级别，通过检索式分块实现更长上下文处理（此为产品化的拓展）。总的来说，Qwen 架构在保持 Transformer 主干的同时，通过RoPE位置编码、预归一化、RMSNorm、SwiGLU激活等当前先进配置，并辅以针对长上下文和高效推理的创新（如GQA和NTK插值），打造了性能与效率兼顾的模型架构。

1.2 模型规模与参数配置
阿里云针对不同应用场景，发布了 Qwen1 系列的大模型家族，涵盖多种参数规模：



Qwen1.0 系列（初代发布）包括约 7B（70亿）参数和 14B（140亿）参数两个主力模型，以及一个小模型 1.8B（18亿）。其中 Qwen-7B 和 Qwen-14B 是基础模型版本，分别还有对齐人类指令的聊天版本 Qwen-7B-Chat 和 Qwen-14B-Chat。1.8B 模型则作为轻量级模型，同样提供 Base 和 Chat 版本供社区试验使用。这些模型的层数和隐藏维度随参数规模增加：例如 Qwen-7B 大约使用了 32 层 Transformer、每层隐藏尺寸4096、注意力头数32；Qwen-14B 使用40层、隐藏尺寸5120、注意力头数40；而 Qwen-1.8B 使用24层、隐藏2048、注意力头数16。所有模型的注意力头维度均为128，使架构在不同规模上保持一致的相对维度配置。
Qwen1.5 系列（进阶改进版）在1.0基础上进一步丰富了模型规模，从 0.5B（5亿） 超小参数模型到 72B（720亿） 超大模型，共覆盖 0.5B、1.8B、4B、7B、14B、72B 六种规模。每种规模皆提供基础 Base 模型和对话 Chat 模型，以满足不同算力和应用需求。尤其值得一提的是，Qwen1.5 新增了0.5B微型模型，方便边缘设备或移动端部署；同时上新了72B巨模型，在复杂任务上获得更强性能。Qwen-72B 是该系列中预训练规模最大的模型之一，其参数量达到 LLaMA2-70B 级别，但通过更多训练数据和优化实现了更高的基准表现。在 Qwen1.5 后续更新中（2024年初），阿里还开源了首个千亿级模型 Qwen1.5-110B，参数规模达到 1100 亿，这是 Qwen 系列首度迈过千亿门槛。110B 模型在零样本/N-shot任务和对话能力上均进一步提升，在部分评测中可比肩甚至超越同时期Meta的70B级模型。
各规模模型的具体参数配置（层数、隐藏维度、注意力头等）随公开技术报告释出【上图】。例如，Qwen-7B 为32层Transformer、隐藏维度4096，Qwen-14B为40层5120维，Qwen-72B则更深更宽（模型报告称其也采用了基于128维/头的配置）。所有模型均采用了相同的架构优化（RoPE、RMSNorm等）。需要注意的是，随着参数规模增长，Qwen 模型的上下文长度支持也有所扩展：早期7B/14B模型训练时上下文长2048，后续新版通过额外训练和插值技术扩展到8192；而Qwen1.5系列全规模均宣布支持到32K长度（通过前述架构扩展方案实现）。这种一致的长文本支持在大小模型上都具备，体现了架构设计的可扩展性。

总结来说，Qwen1 系列提供了从5亿到千亿级的完整参数谱系，在模型深度和宽度上与主流开源模型接轨，并通过系列化的 Base/Chat 版本方便用户根据算力和应用选择合适的模型。这种多尺寸布局满足了不同场景对模型能力和资源的平衡需求。

1.3训练数据
海量高质量语料是通义千问系列性能突出的重要基础。根据阿里云官方报告，Qwen 模型使用了高达 3 万亿（3 trillion）tokens 的超大规模训练语料库。这一数据规模在开源模型中处于领先地位（例如 LLaMA2-70B 使用约2万亿tokens），为模型的知识储备和多样性提供了保证。

数据来源与构成方面，Qwen 的预训练语料非常多元，覆盖了广泛的领域和语言类型：

通用网络文本：大量爬取的互联网文本语料，经过严格的质量过滤。这包括百科、新闻、论坛问答、社交媒体等各类中文和英文网页内容，保证模型对开放域知识的覆盖。
专业书籍文档：一批专业领域的电子书、文档资料，用于增强模型在法律、医学、金融、科技等专门领域的知识和语调。高质量的书本内容提供了比网络文本更权威详实的信息来源。
代码数据：Qwen 语料中特别加入了 代码 数据，包括多种编程语言的源代码仓库、编程问答等。这使模型具备一定的编程理解和生成能力，为后来推出的代码专版模型（Code-Qwen）打下基础。训练中还对数字和代码片段做了特殊处理，例如将长数字拆分为单个数字Token等。
数学与公式：数据中包含数学领域的文本，如Math教程、公式证明、竞赛题解等，以及模型生成数学推理需要的格式数据。这使得模型在数学推算、公式格式化方面有更好的表现，相对于纯通用语料训练的模型是一大优势。
多语言语料：通义千问强调中英双语并重，尤其注重中文能力，同时扩展支持多达 27 种语言。除中英文外，训练数据中涵盖了法语、西班牙语、德语、俄语、日语、韩语、越南语、阿拉伯语、泰语、土耳其语、意大利语等主要语种。为了兼顾多语言，Qwen 团队通过大量实验优化了不同语种数据在总语料中的比例。中文和英文仍占主要份额（据称“当前以中文和英文为主”），但也确保模型对其他语种有基本的理解和生成能力，不至于出现词表未涵盖或编码效率低的问题。事实上，Qwen 使用了一个超大词表来支持多语言（下文详述），保证了非中英语言的词汇不会被过度切分，从而提高多语言的表示效率。
数据清洗与预处理：阿里云团队对语料进行了严格的质量控制。例如过滤低质量、重复、违禁内容；对文本进行分句、去除乱码和HTML标签等清洗；对中英文进行平衡采样等。同时，他们通过大量对比实验优化了训练语料的分布。这意味着在3万亿Token的规模下，不同来源、不同语言的数据占比是精心调优的，既让模型广泛涉猎知识，又避免某类数据过多导致的偏差或过拟合。这种数据配比的优化在技术报告中有所提及，说明了数据工程在大模型训练中的重要性。
分词与词表：为了高效利用上述多语种数据，Qwen 定制了自己的 Tokenizer。其词表大小约为 152K（十五万）词元。相比大多数开源模型常用的3万～5万词表（往往只针对中英），Qwen的词表要大得多。这一设计有以下考虑：

对于中文，Qwen词表中不仅包括常用汉字，还收录了一些高频词汇（短词）。传统BPE对中文常以字为基本单元，而15万词表允许模型直接使用一些中文常用词作为Token，从而减少中文文本长度，提高生成效率。
对于英文及其他拉丁语系语言，词表容量充足意味着可以覆盖更多的完整单词或短语，而不必切分为过多子词。这样处理英文时的token效率与英文专用模型相当。
对于多语种，152K的词表可以囊括许多小语种的基础词汇和字符集（如西文变音字符、阿拉伯字母等），显著降低它们的词分割粒度。实践表明，Qwen 在多种语言上的压缩率（平均每个字/词对应的token数量）都很高，即 同样一句话，Qwen 编码所需的 token 数往往更少。这不仅提升了训练和推理效率，也意味着模型具备更强的多语言扩展性。
总体而言，Qwen 的训练数据既“广”且“精”——拥有海量规模涵盖百科到代码的广泛知识，又通过精细清理和词表优化保证了数据质量和多语言支持。这为模型提供了坚实的语料基础，使其在各类下游任务中都能有出色表现。据介绍，Qwen-7B等模型在常识问答、专业知识、代码理解、数学推理等基准上，相比同规模开源模型显著超越。可以说，大规模高质量的训练语料是 Qwen 系列能够“底盘扎实”的关键原因。

1.4训练方法与策略
预训练阶段：Qwen 模型的预训练采用传统的自回归语言模型目标（Auto-regressive LM）。也就是给定前文预测下一个token的标准方法，与 GPT 等模型一致。训练过程中并没有使用额外的自监督任务（如填空、排序等），而是专注于next-token预测这一单一目标，从而让模型充分学习大规模语料的统计规律和语义关联。

在优化算法上，Qwen 使用 AdamW 优化器 (β1=0.9, β2=0.95) 并配合 余弦退火学习率调度。训练采用混合精度（BFloat16）以平衡效率和稳定性。此外，通过Flash Attention等高效算子加速注意力计算，并利用深度并行技术（如张量模型并行、流水线并行）在多卡集群上训练超大模型。这使得像72B、110B这样规模的模型在阿里云强大的计算集群支持下完成了预训练。训练batch累积达到数百万tokens（报告中提到4M tokens每步的全局Batch），这意味着在2048序列长度下，每步并行处理约2000条样本，充分利用了算力资源。

有监督微调 (SFT)：在得到预训练的Base模型后，阿里云团队进行了监督微调，以使模型能够更好地遵循指令、进行对话。这一步通常是构造一批高质量的指令-回答示例数据进行微调。据报道，阿里收集并标注了多种风格、多轮次的对话数据，以及角色扮演、工具使用等场景的数据。特别地，考虑到中英文及多模态需求，SFT 数据涵盖了中英双语的指令，还有助于模型理解图片/代码等说明（比如 Qwen-VL、Code-Qwen 等衍生模型会用到的指令）。由于公众开源的中文多轮对话数据相对有限，阿里团队应当投入了人工标注来补足这一部分。此外，他们非常重视安全相关的指令微调：针对可能的有害内容（暴力、色情、歧视等），专门标注了这类请求及拒绝回答的范例数据，以训练模型在遇到不当请求时能礼貌拒绝或给出安全提示。通过SFT，得到的 Qwen-Chat 模型在遵从用户指令、控制回答风格和内容安全上比基座模型有大幅提升。

对齐与强化学习 (RLHF)：Qwen-Chat 模型进一步采用了 人类反馈强化学习 来优化回答的质量和对齐人意程度。具体流程与 OpenAI 的 InstructGPT / ChatGPT 类似，包括训练一个奖励模型来评分模型输出，然后使用 PPO（近端策略优化） 算法微调语言模型策略。技术报告中显示，他们训练了一个 Qwen-RM（Reward Model）用于评判对话回答的好坏，并通过与人类偏好数据对比训练，使RM能近似模拟多数人偏好。接着，以预训练模型为初始策略、SFT模型输出为参考，通过 PPO 优化语言模型，使其生成的回答能够获得更高的奖励模型评分，即更符合人类偏好。人类评估结果表明，引入 RLHF 后的 Qwen-Chat 相比仅SFT的版本输出质量明显提升，在对话自然度、有用性方面更接近人类期望。阿里的实验显示，Qwen-14B-Chat (RLHF) 在复杂对话中的表现仅略逊于 sophnet/Qwen3-30B-A3B-Thinking-2507，优于没有RLHF的模型许多。

值得一提的是，在 Qwen1.5 系列中，**团队还探索了新的对齐算法DPO（直接偏好优化）。**DPO是一种不通过显式奖励模型、直接优化策略以匹配人类偏好的方法，可视为RLHF的变体。官方介绍中提到 Qwen1.5 采用了 DPO 和 PPO 相结合的技术来增强模型对人类偏好的对齐度。这意味着除了常规的PPO训练，他们可能尝试用DPO损失直接优化SFT模型，使其输出排序更符合人类排名。这在学术上是一个新兴方向，阿里将其引入实际大模型训练，体现了前沿性。通过综合这些对齐手段，Qwen-Chat 模型在 遵循指令、内容安全 和 用户偏好 等方面达到业界先进水平。它不仅能准确回答问题，还学会了在不当请求时拒答，在需要一步步推理或工具查询时按合理步骤执行，这些都是人类偏好对齐训练带来的能力。

蒸馏与小模型训练：对于Qwen系列的小参数模型（如0.5B、1.8B），虽然主要也是通过预训练获得，但推测阿里可能采用了一些知识蒸馏或迁移学习策略来提升其效果。公开信息中未明确提及蒸馏过程，但社区有相关探索。例如，有技术爱好者使用更大的Qwen模型生成数据，再微调小模型，以提高小模型的性能。官方则更多是通过同样的大规模数据直接训练小模型，并适当增加训练轮次来弥补容量不足。从结果看，Qwen-1.8B 在多个任务上已经超越同期同规模模型，甚至某些指标上接近于更大模型。这说明阿里在小模型训练上也下了功夫，可能通过更长训练、调节正则等方法，以及得益于大词表带来的效率，使得小模型也拥有不俗实力。

总之，Qwen1 系列在训练方法上遵循“大规模预训练+对齐微调”的范式，充分利用 SFT 和 RLHF 来提升模型实用性和安全性。同时，灵活运用了前沿的优化策略（如 DPO）和工程技巧（如 FlashAttention、混合精度）来在实际算力下训练超大模型。多管齐下的训练策略造就了 Qwen 模型在性能和对齐上的全面领先。

1.5 推理与部署优化技术
阿里云在 Qwen 模型的推理部署上亦提供了多项优化手段，旨在降低使用门槛、提高运行效率：

高效分词器设计：前文提到，Qwen 采用了约152k规模的大词表。这在推理阶段的直接效果是单词平均 token 数更少，等于减少了需要处理的序列长度。例如，同样一段中文或法文文本，Qwen 切分出的 Token 数比使用常规词表的模型少很多。因此，在生成同样内容时，Qwen 实际处理的步数更少，推理速度相对更快。此外，大词表也降低了罕见词被拆分的几率，提高生成的连贯性。这些都属于“以空间换时间”的优化——虽然词表大幅增加了embedding层参数和显存占用，但换来了推理效率和多语言性能的提升。
缓存与长上下文推理：Qwen 在解码生成时充分利用了 KV Cache（键值缓存）技术。也就是对于每一步生成，将Transformer每层的 Key/Value 隐状态缓存下来，下一步生成时重复利用，而不必每次重新计算前序token的注意力。这是Transformer加速的常规手段，Qwen 的实现中对此进行了优化，使其在长上下文（如上万token）情况下仍能高效读取缓存、计算增量推理。而且，正如官方指出的，Qwen 还支持KV缓存的量化存储。也就是说，将缓存的张量用更低精度（如 int8）表示，以减少超长对话时内存占用和带宽开销。这对32K这样长度的推理非常有帮助。通过缓存量化，长上下文推理既保证速度又缓解了显存压力。
模型量化支持：为了方便不同设备部署，Qwen 官方提供了多种模型量化版本。包括 8比特 (Int8) 量化和 4比特 (Int4) 量化的权重文件。量化模型极大降低了显存占用：据介绍，Qwen-1.8B 的 Int4 模型推理显存需求不到2GB，生成2048 token的过程中峰值也仅约3GB。如此一来，即使消费级显卡乃至部分CPU设备也能运行小模型。即便是中大型的Qwen-7B、14B，使用8bit量化后在单卡16GB显存上即可跑通7B推理，在多卡上也能负担14B。这为社区用户大幅降低了使用门槛。值得强调的是，Qwen 提供的量化采用了先进的 GPTQ 算法等，能在尽可能降低精度损失的同时将权重压缩。实际基准测试表明，Qwen-7B 4bit量化模型在大多数任务上与全精度模型只有微小差距，却将推理速度提高了近一倍。这种性能/效率比令人印象深刻。
并行和分布式支持：对于超大模型（如72B、110B），Qwen 提供了完备的并行推理支持。在多GPU环境下，可以通过 张量并行（Tensor Parallelism）将模型权重按张量维度切分到多卡计算；结合 流水线并行 将不同层分配到不同卡顺序执行，从而实现多卡协同推理。这套并行机制源自Megatron-LM等开源框架，阿里云在其代码中集成了这些能力，使得部署 70B+ 模型成为可能。此外，Qwen 兼容一些高性能推理引擎，例如 vLLM 和 FasterTransformer 等。官方文档中有使用 vLLM 在 GPU 集群上部署 Qwen-Chat 服务的示例。通过这些引擎，可以利用批量调度和高效内存管理，进一步提升吞吐和降低延迟。在异构硬件方面，Qwen 还支持阿里自研的 昇腾910 AI芯片 以及 海光DCU 加速器的推理。这意味着除了CUDA GPU，用户也可在这些国产AI硬件上运行 Qwen 模型，体现了框架的灵活性。
高效服务与调用：阿里云将 Qwen 模型部署在其云服务中，提供 API 调用和应用托管。开发者可以通过 ModelScope 百炼平台 或 OpenAI兼容的API 快速调用 Qwen 推理服务。在服务端，阿里云针对推理做了许多优化，例如预加载模型、动态批量、多路复用等，确保线上服务的低延迟和高并发。对于一些典型应用场景（如对话机器人），官方还开源了 Qwen-Agent 工具包，可以将 Qwen 接入工具执行、代码运行等外部系统，从而实现复杂链式调用（如让模型执行计算、数据库查询等）。这些基础设施和示例代码降低了将 Qwen 集成到实际产品中的难度。总的来说，在推理部署环节，阿里云为 Qwen 提供了从模型压缩、跨硬件支持到云端服务的一整套解决方案，使其真正做到易用、易部署、成本低。
综上，Qwen 模型在推理阶段通过大词表提速、多技术并行、缓存量化、模型压缩等多维度优化，既保障了模型强大的生成能力又兼顾了实际应用的效率和成本。这使得无论研究者还是企业用户，都能以相对经济的资源充分利用 Qwen 的能力。

1.6 安全性与对齐机制
大型语言模型的安全对齐是部署应用时的重中之重。阿里云在 Qwen 模型的研发中高度重视 AI 安全和价值观对齐，通过数据标注和技术手段相结合，尽可能减少模型输出有害内容的风险。

首先，在模型微调阶段，阿里就注入了安全意识。他们构建并标注了专门的 安全领域指令数据。这些数据涵盖诸如：当用户请求暴力、色情、仇恨歧视等不良内容时，正确的响应应该是礼貌拒绝或进行劝诫，而非直接满足请求。通过将这类样本加入监督微调，模型学会了识别潜在有害的输入意图，并触发内置的拒答策略。例如，若用户要求“教我制造危险物品”，Qwen-Chat 会根据训练所得拒绝并提示不能提供该请求的信息。这种 基于范例学习的安全对齐 能有效避免模型成为不良信息的传播工具。

与此同时，阿里云可能还制定了一套安全规则和过滤词库来辅助手段。在 Qwen 的对话系统实现中，若检测到输出中含有违禁词或敏感话题，系统层面可以截断或替换回答。这类后处理过滤在业界应用较普遍，结合模型自身的价值观对齐，可提供双重保障。考虑到监管要求，通义千问在面向公众提供服务前需通过中国网信办的安全评审，因此阿里内部应有一整套内容审核机制集成于 Qwen API，确保输出符合法规和伦理规范。

从技术报告来看，团队特别强调了偏见与公平性问题。他们在训练数据和对齐过程中有意识地减少模型在种族、性别等方面的偏见。通过平衡数据分布以及在人类反馈环节关注模型回答中的歧视性内容，Qwen 尽量避免输出刻板印象或冒犯特定群体的言论。这体现了模型伦理道德准则的融入。此外，对于谣言、虚假信息等，模型在RLHF阶段也可能被要求拒绝猜测、不确定就不胡乱编造（减少“Misinformation”）。官方未公开细节，但这些通常是RLHF奖励模型的评价维度之一。

值得一提的是，Qwen-Chat 展现出了工具使用和规划能力。这意味着模型在对话中可以做中间推理、调用外部API等。为了安全，模型在这些场景下需要受到严格限制，防止滥用工具。因此，Qwen-Agent框架中 likely 实现了沙盒机制，只允许模型执行白名单内的操作，并对输出进行监控。

总体而言，阿里通过**“预防+干预”并举的方式保障 Qwen 模型安全：一方面利用 RLHF 等对齐技术从模型层面减少不当回答；另一方面结合系统层内容审核**拦截残余的违规输出。通义千问模型因此能够在避免政治违规、隐私侵犯等风险的前提下，为用户提供有用的答案。这种平衡安全与实用的设计，使 Qwen-Chat 在国内外评测中被认为是 对齐良好、响应稳健 的大模型之一。

当然，再完善的对齐也不可能百分百杜绝问题。所以阿里云在开源许可和使用协议中也注明了不得将 Qwen 用于非法用途等条款，提醒用户负责任地使用模型。在实际应用中，持续的人工监测和反馈迭代也很必要。阿里方面也在不断更新 Qwen 的安全策略（例如增加新违禁话题、优化拒答话术等）。随 Qwen2.5、Qwen3 等版本迭代，安全对齐能力预计会进一步增强。安全是 AI 应用的底线，通义千问系列在这方面的投入为业界树立了一个正面典范。


