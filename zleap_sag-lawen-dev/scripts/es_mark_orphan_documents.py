"""
ES å­¤ç«‹æ–‡æ¡£æ ‡è®°è„šæœ¬

æ‰«æä¸‰ä¸ª ES ç´¢å¼•ï¼Œæ‰¾å‡º source_config_id ä¸å­˜åœ¨äº MySQL ä¸­çš„æ–‡æ¡£ï¼Œ
è¾“å‡ºä¸º CSV å¹¶æ ‡è®°è½¯åˆ é™¤ã€‚

å®‰å…¨æªæ–½ï¼š
- æ‰“å° ES å’Œ MySQL åœ°å€ä¾›ç¡®è®¤
- è¾“å‡º CSV ä¾›å®¡è®¡
- éšæœºæŠ½æ ·éªŒè¯
- æ‰§è¡Œå‰äºŒæ¬¡ç¡®è®¤
"""

import asyncio
import csv
import random
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, Set

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from sqlalchemy import select

from dataflow.core.config import get_settings
from dataflow.core.storage.elasticsearch import ElasticsearchClient, ESConfig
from dataflow.db import SourceConfig, get_session_factory
from dataflow.db.base import close_database
from dataflow.utils import get_logger

logger = get_logger("scripts.es_mark_orphan_documents")

# éœ€è¦æ‰«æçš„ç´¢å¼•åˆ—è¡¨
TARGET_INDICES = [
    "entity_vectors",
    "event_vectors",
    "source_chunks",
]

# æ‰¹é‡å¤„ç†å¤§å°
BATCH_SIZE = 1000


# è¾“å‡ºè¾…åŠ©å‡½æ•°
def print_header(text: str) -> None:
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "=" * 70)
    print(f"  {text}")
    print("=" * 70)


def print_success(text: str) -> None:
    """æ‰“å°æˆåŠŸä¿¡æ¯"""
    print(f"  âœ“ {text}")


def print_info(text: str) -> None:
    """æ‰“å°æ™®é€šä¿¡æ¯"""
    print(f"  â€¢ {text}")


def print_warning(text: str) -> None:
    """æ‰“å°è­¦å‘Šä¿¡æ¯"""
    print(f"  âš ï¸  {text}")


def print_error(text: str) -> None:
    """æ‰“å°é”™è¯¯ä¿¡æ¯"""
    print(f"  âœ— {text}")


def print_danger(text: str) -> None:
    """æ‰“å°å±é™©è­¦å‘Š"""
    print(f"  ğŸš¨ {text}")


async def get_valid_source_config_ids() -> Set[str]:
    """
    ä» MySQL è·å–æ‰€æœ‰æœ‰æ•ˆçš„ source_config_id

    Returns:
        Set[str]: æœ‰æ•ˆçš„ source_config_id é›†åˆ
    """
    print_info("æ­£åœ¨ä» MySQL è·å–æœ‰æ•ˆçš„ source_config_id...")

    factory = get_session_factory()
    async with factory() as session:
        result = await session.execute(select(SourceConfig.id))
        ids = {row[0] for row in result.fetchall()}

    print_success(f"è·å–åˆ° {len(ids)} ä¸ªæœ‰æ•ˆçš„ source_config_id")
    return ids


async def scan_index_for_orphans(
    es_client: ElasticsearchClient,
    index_name: str,
    valid_ids: Set[str]
) -> Dict[str, int]:
    """
    æ‰«æå•ä¸ªç´¢å¼•ï¼Œæ‰¾å‡ºå­¤ç«‹çš„ source_config_id

    ä½¿ç”¨ ES composite aggregation æŒ‰ source_config_id åˆ†ç»„ç»Ÿè®¡ï¼Œ
    é¿å…éå†æ¯æ¡æ–‡æ¡£ï¼Œå¤§å¹…æå‡æ€§èƒ½ã€‚

    Args:
        es_client: ES å®¢æˆ·ç«¯
        index_name: ç´¢å¼•åç§°
        valid_ids: æœ‰æ•ˆçš„ source_config_id é›†åˆ

    Returns:
        Dict[str, int]: å­¤ç«‹çš„ source_config_id -> æ–‡æ¡£æ•°é‡
    """
    print_info(f"æ­£åœ¨æ‰«æç´¢å¼•: {index_name}...")

    orphan_counts: Dict[str, int] = {}
    total_source_configs = 0
    orphan_source_configs = 0
    total_orphan_docs = 0

    try:
        # ä½¿ç”¨ composite aggregation åˆ†é¡µè·å–æ‰€æœ‰ source_config_id åŠå…¶æ–‡æ¡£æ•°
        # composite aggregation æ”¯æŒåˆ†é¡µï¼Œå¯å¤„ç†é«˜åŸºæ•°å­—æ®µ
        after_key = None

        while True:
            agg_query = {
                "composite": {
                    "size": BATCH_SIZE,
                    "sources": [
                        {"source_config_id": {"terms": {"field": "source_config_id"}}}
                    ]
                }
            }

            # æ·»åŠ åˆ†é¡µ after_key
            if after_key:
                agg_query["composite"]["after"] = after_key

            response = await es_client.client.search(
                index=index_name,
                size=0,  # ä¸éœ€è¦è¿”å›æ–‡æ¡£ï¼Œåªè¦èšåˆç»“æœ
                aggs={"by_source_config": agg_query}
            )

            buckets = response["aggregations"]["by_source_config"]["buckets"]

            if not buckets:
                break

            for bucket in buckets:
                source_config_id = bucket["key"]["source_config_id"]
                doc_count = bucket["doc_count"]
                total_source_configs += 1

                # åœ¨ Python ç«¯è¿‡æ»¤å‡ºå­¤ç«‹çš„ source_config_id
                if source_config_id not in valid_ids:
                    orphan_counts[source_config_id] = doc_count
                    orphan_source_configs += 1
                    total_orphan_docs += doc_count

            # è·å–ä¸‹ä¸€é¡µçš„ after_key
            after_key = response["aggregations"]["by_source_config"].get("after_key")
            if not after_key:
                break

    except Exception as e:
        logger.error(f"æ‰«æç´¢å¼• {index_name} å¤±è´¥: {e}", exc_info=True)
        raise

    print_info(f"  {index_name}: æ€» source_config_id {total_source_configs}, "
               f"å­¤ç«‹ {orphan_source_configs} ä¸ª (å…± {total_orphan_docs} æ¡æ–‡æ¡£)")

    return orphan_counts


async def scan_all_indices(
    es_client: ElasticsearchClient,
    valid_ids: Set[str]
) -> Dict[str, Dict[str, int]]:
    """
    å¹¶è¡Œæ‰«ææ‰€æœ‰ç›®æ ‡ç´¢å¼•

    Returns:
        Dict[str, Dict[str, int]]: ç´¢å¼•å -> (source_config_id -> æ–‡æ¡£æ•°é‡)
    """
    print_header("æ‰«æ ES ç´¢å¼•")

    # å…ˆæ£€æŸ¥å“ªäº›ç´¢å¼•å­˜åœ¨
    existing_indices = []
    for index_name in TARGET_INDICES:
        exists = await es_client.index_exists(index_name)
        if not exists:
            print_warning(f"{index_name}: ç´¢å¼•ä¸å­˜åœ¨ï¼Œè·³è¿‡")
        else:
            existing_indices.append(index_name)

    if not existing_indices:
        return {}

    # å¹¶è¡Œæ‰«ææ‰€æœ‰å­˜åœ¨çš„ç´¢å¼•
    tasks = [
        scan_index_for_orphans(es_client, index_name, valid_ids)
        for index_name in existing_indices
    ]
    scan_results = await asyncio.gather(*tasks)

    return dict(zip(existing_indices, scan_results))


def save_to_csv(scan_results: Dict[str, Dict[str, int]], output_dir: Path) -> str:
    """
    å°†æ‰«æç»“æœä¿å­˜åˆ° CSV æ–‡ä»¶

    Args:
        scan_results: æ‰«æç»“æœ
        output_dir: è¾“å‡ºç›®å½•

    Returns:
        str: CSV æ–‡ä»¶è·¯å¾„
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_path = output_dir / f"orphan_source_configs_{timestamp}.csv"

    # å†™å…¥ CSV
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["index_name", "source_config_id", "document_count"])

        for index_name, orphan_counts in scan_results.items():
            for source_config_id, count in orphan_counts.items():
                writer.writerow([index_name, source_config_id, count])

    return str(csv_path)


def get_random_samples(scan_results: Dict[str, Dict[str, int]], count: int = 5) -> list:
    """
    ä»æ‰«æç»“æœä¸­éšæœºæŠ½å–æ ·æœ¬

    Returns:
        list: [(index_name, source_config_id, doc_count), ...]
    """
    all_orphans = []
    for index_name, orphan_counts in scan_results.items():
        for source_config_id, doc_count in orphan_counts.items():
            all_orphans.append((index_name, source_config_id, doc_count))

    if len(all_orphans) <= count:
        return all_orphans

    return random.sample(all_orphans, count)


def calculate_total_stats(scan_results: Dict[str, Dict[str, int]]) -> dict:
    """
    è®¡ç®—æ€»ä½“ç»Ÿè®¡ä¿¡æ¯

    Returns:
        dict: {total_docs, total_orphan_ids, by_index: {index: {docs, ids}}}
    """
    total_docs = 0
    total_orphan_ids = set()
    by_index = {}

    for index_name, orphan_counts in scan_results.items():
        index_docs = sum(orphan_counts.values())
        index_ids = len(orphan_counts)

        total_docs += index_docs
        total_orphan_ids.update(orphan_counts.keys())

        by_index[index_name] = {
            "docs": index_docs,
            "ids": index_ids
        }

    return {
        "total_docs": total_docs,
        "total_orphan_ids": len(total_orphan_ids),
        "by_index": by_index
    }


async def mark_documents_as_deleted(
    es_client: ElasticsearchClient,
    scan_results: Dict[str, Dict[str, int]]
) -> Dict[str, dict]:
    """
    å°†å­¤ç«‹æ–‡æ¡£æ ‡è®°ä¸ºè½¯åˆ é™¤

    Returns:
        Dict[str, dict]: ç´¢å¼•å -> æ›´æ–°ç»“æœ
    """
    print_header("æ ‡è®°è½¯åˆ é™¤")

    results = {}

    for index_name, orphan_counts in scan_results.items():
        if not orphan_counts:
            print_info(f"{index_name}: æ— éœ€æ›´æ–°")
            results[index_name] = {"updated": 0, "failed": 0}
            continue

        orphan_ids = list(orphan_counts.keys())
        print_info(f"{index_name}: æ­£åœ¨æ›´æ–° {len(orphan_ids)} ä¸ª source_config_id çš„æ–‡æ¡£...")

        try:
            # ä½¿ç”¨ update_by_query æ‰¹é‡æ›´æ–°
            response = await es_client.client.update_by_query(
                index=index_name,
                query={
                    "terms": {
                        "source_config_id": orphan_ids
                    }
                },
                script={
                    "source": "ctx._source.is_delete = true",
                    "lang": "painless"
                },
                conflicts="proceed",
                refresh=True
            )

            updated = response.get("updated", 0)
            failures = len(response.get("failures", []))

            results[index_name] = {
                "updated": updated,
                "failed": failures
            }

            if failures > 0:
                print_warning(f"{index_name}: æ›´æ–° {updated} æ¡ï¼Œå¤±è´¥ {failures} æ¡")
            else:
                print_success(f"{index_name}: æ›´æ–° {updated} æ¡")

        except Exception as e:
            print_error(f"{index_name}: æ›´æ–°å¤±è´¥ - {e}")
            logger.error(f"æ›´æ–°ç´¢å¼• {index_name} å¤±è´¥: {e}", exc_info=True)
            results[index_name] = {"updated": 0, "failed": -1, "error": str(e)}

    return results


def confirm_action(prompt: str) -> bool:
    """
    è¯·æ±‚ç”¨æˆ·ç¡®è®¤

    Returns:
        bool: ç”¨æˆ·æ˜¯å¦ç¡®è®¤
    """
    print()
    user_input = input(f"  {prompt} (è¾“å…¥ 'yes' ç¡®è®¤): ").strip().lower()
    return user_input == "yes"


async def main() -> None:
    """ä¸»å‡½æ•°"""
    es_client = None
    settings = get_settings()

    try:
        print_header("ES å­¤ç«‹æ–‡æ¡£æ ‡è®°å·¥å…·")
        logger.info("=" * 60)
        logger.info("ES å­¤ç«‹æ–‡æ¡£æ ‡è®°å·¥å…·")
        logger.info("=" * 60)

        # ==================== é˜¶æ®µ 1: ç¯å¢ƒç¡®è®¤ ====================
        print_header("ç¯å¢ƒä¿¡æ¯ç¡®è®¤")

        es_config = ESConfig(
            hosts=f"{settings.es_host}:{settings.es_port}",
            username=settings.es_username,
            password=settings.es_password,
            scheme=settings.es_scheme,
        )
        es_host = f"{settings.es_host}:{settings.es_port}"
        mysql_host = f"{settings.mysql_host}:{settings.mysql_port}/{settings.mysql_database}"

        print_danger("è¿™æ˜¯ä¸€ä¸ªå±é™©æ“ä½œï¼Œå°†ä¿®æ”¹ ES æ–‡æ¡£çš„è½¯åˆ é™¤æ ‡è®°ï¼")
        print()
        print_info(f"Elasticsearch åœ°å€: {es_host}")
        print_info(f"MySQL åœ°å€: {mysql_host}")
        print()
        print_warning("è¯·ä»”ç»†ç¡®è®¤ä»¥ä¸Šåœ°å€æ˜¯å¦æ­£ç¡®ï¼")

        if not confirm_action("ç¡®è®¤ä»¥ä¸Šä¿¡æ¯æ­£ç¡®ï¼Œå¼€å§‹æ‰«æ"):
            print_info("æ“ä½œå·²å–æ¶ˆ")
            return

        # ==================== é˜¶æ®µ 2: è¿æ¥æ•°æ®åº“ ====================
        print_header("è¿æ¥æ•°æ®åº“")

        # è¿æ¥ ES
        print_info("æ­£åœ¨è¿æ¥ Elasticsearch...")
        es_client = ElasticsearchClient(config=es_config)

        if not await es_client.check_connection():
            print_error("Elasticsearch è¿æ¥å¤±è´¥")
            raise Exception("ES è¿æ¥å¤±è´¥")

        print_success("Elasticsearch è¿æ¥æˆåŠŸ")

        # è·å– MySQL ä¸­çš„æœ‰æ•ˆ ID
        valid_ids = await get_valid_source_config_ids()

        # ==================== é˜¶æ®µ 3: æ‰«æç´¢å¼• ====================
        scan_results = await scan_all_indices(es_client, valid_ids)

        # ==================== é˜¶æ®µ 4: è¾“å‡º CSV ====================
        print_header("è¾“å‡º CSV")

        output_dir = Path(__file__).parent / "output"
        csv_path = save_to_csv(scan_results, output_dir)

        print_success(f"CSV å·²ä¿å­˜åˆ°: {csv_path}")

        # ==================== é˜¶æ®µ 5: ç»Ÿè®¡å’Œç¡®è®¤ ====================
        stats = calculate_total_stats(scan_results)

        if stats["total_docs"] == 0:
            print_header("æ‰«æç»“æœ")
            print_success("æœªå‘ç°å­¤ç«‹æ–‡æ¡£ï¼Œæ— éœ€å¤„ç†")
            return

        print_header("æ‰«æç»“æœç»Ÿè®¡")
        print_info(f"æ€»è®¡éœ€è¦æ ‡è®°çš„æ–‡æ¡£: {stats['total_docs']} æ¡")
        print_info(f"æ¶‰åŠçš„å­¤ç«‹ source_config_id: {stats['total_orphan_ids']} ä¸ª")
        print()

        for index_name, index_stats in stats["by_index"].items():
            print_info(f"  {index_name}: {index_stats['docs']} æ¡æ–‡æ¡£, "
                      f"{index_stats['ids']} ä¸ª source_config_id")

        # éšæœºæŠ½æ ·éªŒè¯
        print_header("éšæœºæŠ½æ ·éªŒè¯")
        print_warning("è¯·æ‰‹åŠ¨éªŒè¯ä»¥ä¸‹ source_config_id æ˜¯å¦ç¡®å®ä¸å­˜åœ¨äº MySQL ä¸­ï¼š")
        print()

        samples = get_random_samples(scan_results, 5)
        for i, (index_name, source_config_id, doc_count) in enumerate(samples, 1):
            print_info(f"  {i}. [{index_name}] source_config_id: {source_config_id} "
                      f"(æ–‡æ¡£æ•°: {doc_count})")

        print()
        print_danger("è­¦å‘Šï¼šä»¥ä¸‹æ“ä½œå°†ä¿®æ”¹ ES æ–‡æ¡£ï¼")
        print()
        print_info(f"Elasticsearch åœ°å€: {es_host}")
        print_info(f"MySQL åœ°å€: {mysql_host}")
        print()
        print_warning(f"å³å°†æ ‡è®° {stats['total_docs']} æ¡æ–‡æ¡£ä¸ºè½¯åˆ é™¤çŠ¶æ€")

        if not confirm_action("ç¡®è®¤æ‰§è¡Œè½¯åˆ é™¤æ ‡è®°"):
            print_info("æ“ä½œå·²å–æ¶ˆ")
            return

        # ==================== é˜¶æ®µ 6: æ‰§è¡Œè½¯åˆ é™¤æ ‡è®° ====================
        update_results = await mark_documents_as_deleted(es_client, scan_results)

        # ==================== é˜¶æ®µ 7: æ€»ç»“ ====================
        print_header("æ“ä½œæ€»ç»“")

        total_updated = sum(r.get("updated", 0) for r in update_results.values())
        total_failed = sum(r.get("failed", 0) for r in update_results.values() if r.get("failed", 0) > 0)

        print_success(f"æˆåŠŸæ ‡è®°: {total_updated} æ¡æ–‡æ¡£")

        if total_failed > 0:
            print_warning(f"å¤±è´¥: {total_failed} æ¡")

        print_info(f"CSV æ–‡ä»¶: {csv_path}")
        print_success("è½¯åˆ é™¤æ ‡è®°å®Œæˆï¼")

        logger.info("=" * 60)
        logger.info(f"âœ“ ES å­¤ç«‹æ–‡æ¡£æ ‡è®°å®Œæˆï¼æ ‡è®° {total_updated} æ¡æ–‡æ¡£")
        logger.info("=" * 60)

        print("=" * 70 + "\n")

    except Exception as e:
        print_error(f"æ“ä½œå¤±è´¥: {e}")
        logger.error(f"ES å­¤ç«‹æ–‡æ¡£æ ‡è®°å¤±è´¥: {e}", exc_info=True)
        sys.exit(1)

    finally:
        # å…³é—­è¿æ¥
        if es_client:
            await es_client.close()
        await close_database()


if __name__ == "__main__":
    asyncio.run(main())
